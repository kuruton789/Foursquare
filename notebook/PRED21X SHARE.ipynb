{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28076,"status":"ok","timestamp":1656837381694,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"B2jmKwpq4rsr","outputId":"3db7b5e6-9b98-4b2a-b2fb-fa9aaf07404e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":552,"status":"ok","timestamp":1656837382243,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"mj_QT2qG5Axl","outputId":"f5d3f44d-4ced-4fba-b3dd-a9fcb2aef75c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Kaggle/Foursquare'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","path = '/content/drive/MyDrive/Kaggle/Foursquare'\n","os.chdir(path) # カレントディレクトリを指定\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221507,"status":"ok","timestamp":1656837603747,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"LBo5DYyg3sZF","outputId":"915441d2-fda7-4112-acb7-ebf2b1ea2f59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pykakasi\n","  Downloading pykakasi-2.2.1-py3-none-any.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 4.1 MB/s \n","\u001b[?25hCollecting deprecated\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Collecting jaconv\n","  Downloading jaconv-0.3.tar.gz (15 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pykakasi) (4.11.4)\n","Requirement already satisfied: wrapt\u003c2,\u003e=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated-\u003epykakasi) (1.14.1)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003epykakasi) (4.1.1)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003epykakasi) (3.8.0)\n","Building wheels for collected packages: jaconv\n","  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jaconv: filename=jaconv-0.3-py3-none-any.whl size=15564 sha256=d796da75e5b1a12552058af71d494d7f279c5d1df39ab2c37bccb1bdbd5ebf56\n","  Stored in directory: /root/.cache/pip/wheels/8f/4f/c2/a2a3b14d0e94f855f4aa8887bf0267bee9ecfb8e62a9ee2d92\n","Successfully built jaconv\n","Installing collected packages: jaconv, deprecated, pykakasi\n","Successfully installed deprecated-1.2.13 jaconv-0.3 pykakasi-2.2.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-text\n","  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","\u001b[K     |████████████████████████████████| 4.6 MB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-hub\u003e=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n","Collecting tensorflow\u003c2.10,\u003e=2.9.0\n","  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[K     |████████████████████████████████| 511.7 MB 4.5 kB/s \n","\u001b[?25hRequirement already satisfied: wrapt\u003e=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.14.1)\n","Requirement already satisfied: numpy\u003e=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.21.6)\n","Collecting keras\u003c2.10.0,\u003e=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 40.9 MB/s \n","\u001b[?25hCollecting tensorflow-estimator\u003c2.10.0,\u003e=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[K     |████████████████████████████████| 438 kB 54.6 MB/s \n","\u001b[?25hCollecting flatbuffers\u003c2,\u003e=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: astunparse\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.6.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem\u003e=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (0.26.0)\n","Requirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (3.3.0)\n","Requirement already satisfied: six\u003e=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.15.0)\n","Requirement already satisfied: keras-preprocessing\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.1.2)\n","Requirement already satisfied: grpcio\u003c2.0,\u003e=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.46.3)\n","Requirement already satisfied: google-pasta\u003e=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (0.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (57.4.0)\n","Collecting gast\u003c=0.4.0,\u003e=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (21.3)\n","Collecting tensorboard\u003c2.10,\u003e=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: h5py\u003e=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (3.1.0)\n","Requirement already satisfied: absl-py\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.1.0)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.1.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (4.1.1)\n","Requirement already satisfied: libclang\u003e=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (14.0.1)\n","Requirement already satisfied: protobuf\u003c3.20,\u003e=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (3.17.3)\n","Requirement already satisfied: wheel\u003c1.0,\u003e=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse\u003e=1.6.0-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py\u003e=2.9.0-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.5.2)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (3.3.7)\n","Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (0.4.6)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.35.0)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (2.23.0)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.0.1)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (0.2.8)\n","Requirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (4.2.4)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (4.8)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.3.1)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (4.11.4)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (3.8.0)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (0.4.8)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (3.0.4)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (3.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003etensorflow\u003c2.10,\u003e=2.9.0-\u003etensorflow-text) (3.0.9)\n","Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow, tensorflow-text\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0\n","    Uninstalling flatbuffers-2.0:\n","      Successfully uninstalled flatbuffers-2.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n","Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[K     |████████████████████████████████| 68 kB 1.7 MB/s \n","\u001b[?25hCollecting pybind11\u003e=2.2\n","  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n","Requirement already satisfied: setuptools\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3143744 sha256=51ceceddc9268002657598a5768a33a487e823d5f7e91243bc326741bebe0989\n","  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./nadare/common_utils/mojimoji-0.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Installing collected packages: mojimoji\n","Successfully installed mojimoji-0.0.12\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Unidecode\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 4.1 MB/s \n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.3.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting SudachiPy\n","  Downloading SudachiPy-0.6.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: SudachiPy\n","Successfully installed SudachiPy-0.6.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sudachidict_full\n","  Downloading SudachiDict-full-20220519.tar.gz (9.1 kB)\n","Requirement already satisfied: SudachiPy\u003c0.7,\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from sudachidict_full) (0.6.5)\n","Building wheels for collected packages: sudachidict-full\n","  Building wheel for sudachidict-full (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sudachidict-full: filename=SudachiDict_full-20220519-py3-none-any.whl size=126860362 sha256=0fb3f2ec1dfdd87d40eb6d17e796810e45629e178cd3cdec5abc9e57fc94a8c2\n","  Stored in directory: /root/.cache/pip/wheels/d3/45/0e/7d302062690f0d59ef491f4d1b2ea3e8beb96e86813038ae7a\n","Successfully built sudachidict-full\n","Installing collected packages: sudachidict-full\n","Successfully installed sudachidict-full-20220519\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pythainlp\n","  Downloading pythainlp-3.0.8-py3-none-any.whl (11.5 MB)\n","\u001b[K     |████████████████████████████████| 11.5 MB 3.9 MB/s \n","\u001b[?25hCollecting tinydb\u003e=3.0\n","  Downloading tinydb-4.7.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: requests\u003e=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.22.0-\u003epythainlp) (2022.6.15)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.22.0-\u003epythainlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.22.0-\u003epythainlp) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.22.0-\u003epythainlp) (2.10)\n","Requirement already satisfied: typing-extensions\u003c5.0.0,\u003e=3.10.0 in /usr/local/lib/python3.7/dist-packages (from tinydb\u003e=3.0-\u003epythainlp) (4.1.1)\n","Installing collected packages: tinydb, pythainlp\n","Successfully installed pythainlp-3.0.8 tinydb-4.7.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emoji\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 4.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=3f8ca74430c18f6a3de99ca0e67ca1b5f4e029eb9b6932dbd72683fc58bbb9f0\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.7.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-ranking\n","  Downloading tensorflow_ranking-0.5.0-py2.py3-none-any.whl (141 kB)\n","\u001b[K     |████████████████████████████████| 141 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: six\u003e=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.15.0)\n","Requirement already satisfied: absl-py\u003e=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.1.0)\n","Collecting tensorflow-serving-api\u003c3.0.0,\u003e=2.0.0\n","  Downloading tensorflow_serving_api-2.9.0-py2.py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy\u003e=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.21.6)\n","Requirement already satisfied: protobuf\u003e=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (3.17.3)\n","Requirement already satisfied: grpcio\u003c2,\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.46.3)\n","Requirement already satisfied: tensorflow\u003c3,\u003e=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (2.9.1)\n","Requirement already satisfied: wrapt\u003e=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.14.1)\n","Requirement already satisfied: keras-preprocessing\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (57.4.0)\n","Requirement already satisfied: h5py\u003e=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (3.1.0)\n","Requirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (3.3.0)\n","Requirement already satisfied: google-pasta\u003e=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (0.2.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (4.1.1)\n","Requirement already satisfied: flatbuffers\u003c2,\u003e=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.12)\n","Requirement already satisfied: libclang\u003e=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (14.0.1)\n","Requirement already satisfied: astunparse\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.6.3)\n","Requirement already satisfied: tensorflow-estimator\u003c2.10.0,\u003e=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (2.9.0)\n","Requirement already satisfied: tensorboard\u003c2.10,\u003e=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (2.9.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (21.3)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.1.0)\n","Requirement already satisfied: gast\u003c=0.4.0,\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (0.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem\u003e=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (0.26.0)\n","Requirement already satisfied: keras\u003c2.10.0,\u003e=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (2.9.0)\n","Requirement already satisfied: wheel\u003c1.0,\u003e=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse\u003e=1.6.0-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py\u003e=2.9.0-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.5.2)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (0.4.6)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (3.3.7)\n","Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (0.6.1)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.0.1)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.8.1)\n","Requirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (4.2.4)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (4.8)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (0.2.8)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.3.1)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (4.11.4)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (3.8.0)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (0.4.8)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (2.10)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003c2.10,\u003e=2.9-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (3.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003etensorflow\u003c3,\u003e=2.9.0-\u003etensorflow-serving-api\u003c3.0.0,\u003e=2.0.0-\u003etensorflow-ranking) (3.0.9)\n","Installing collected packages: tensorflow-serving-api, tensorflow-ranking\n","Successfully installed tensorflow-ranking-0.5.0 tensorflow-serving-api-2.9.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Levenshtein\n","  Downloading Levenshtein-0.18.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (258 kB)\n","\u001b[K     |████████████████████████████████| 258 kB 4.0 MB/s \n","\u001b[?25hCollecting rapidfuzz\u003c3.0.0,\u003e=2.0.1\n","  Downloading rapidfuzz-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 48.3 MB/s \n","\u001b[?25hCollecting jarowinkler\u003c1.1.0,\u003e=1.0.3\n","  Downloading jarowinkler-1.0.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (102 kB)\n","\u001b[K     |████████████████████████████████| 102 kB 66.3 MB/s \n","\u001b[?25hInstalling collected packages: jarowinkler, rapidfuzz, Levenshtein\n","Successfully installed Levenshtein-0.18.1 jarowinkler-1.0.5 rapidfuzz-2.1.1\n"]}],"source":["!pip install pykakasi\n","!pip install tensorflow-text\n","!pip install fasttext\n","!pip install ./nadare/common_utils/mojimoji-0.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps\n","!pip install Unidecode\n","!pip install SudachiPy\n","!pip install sudachidict_full\n","!pip install pythainlp\n","!pip install emoji\n","!pip install tensorflow-ranking\n","!pip install Levenshtein"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3312,"status":"ok","timestamp":1656837607049,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"0_Ek_7_TQX_Z","outputId":"841f2564-291a-4162-c09c-bd38451f7f46"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mojimoji in /usr/local/lib/python3.7/dist-packages (0.0.12)\n"]}],"source":["!pip install mojimoji"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ShUl2ZC3sTE"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_text as tf_text\n","from fasttext import load_model\n","\n","from tqdm.notebook import tqdm\n","import re\n","import gc\n","\n","import mojimoji\n","import string\n","import unidecode\n","from sudachipy import Dictionary, SplitMode\n","import pykakasi\n","from unidecode import unidecode\n","import pythainlp\n","\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAgdOV2d1cVv"},"outputs":[],"source":["INPUT_PATH = './input'\n","nadare_util_dir = './nadare/common_utils/'\n","nadare_feature_dir = './nadare/FEAT20/'\n","\n","DEBUG = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YWe3YeO71fTr"},"outputs":[],"source":["test_df = pd.read_csv(os.path.join(INPUT_PATH, \"train.csv\"), encoding=\"utf-8\")\n","\n","if DEBUG:\n","  test_df = test_df.iloc[:10000]"]},{"cell_type":"markdown","metadata":{"id":"odJBzw8f7pn3"},"source":["# ID to Index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jribjcm5eBN"},"outputs":[],"source":["test_id_map = {v: i for i, v in enumerate(test_df[\"id\"].values)}\n","test_df[\"ix\"] = test_df[\"id\"].map(test_id_map)\n","test_df[\"categories\"] = test_df[\"categories\"].fillna(\"\")\n","test_df[\"pid\"] = -1\n","test_df[\"group\"] = -1"]},{"cell_type":"markdown","metadata":{"id":"St2jrZg-7inK"},"source":["# 言語判定"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37221,"status":"ok","timestamp":1656837660243,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"fX5WIXmI59WP","outputId":"3e398ada-ff95-490f-8dc3-7cc00fa04d11"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"]}],"source":["from fasttext import load_model\n","ft_model = load_model(nadare_util_dir + \"lid.176.bin\")\n","def predict_language(text):\n","    label, prob = ft_model.predict(text, 1)\n","    return list(zip([l.replace(\"__label__\", \"\") for l in label], prob))[0][0]\n","\n","test_df[\"name_language\"] = np.vectorize(predict_language)(test_df[\"name\"].fillna(\"\"))\n","test_df[\"address_language\"] = np.vectorize(predict_language)(test_df[\"address\"].fillna(\"\"))\n","\n","def language_pattern(country, language):\n","    if country == \"JP\":\n","        return 0\n","    elif country == \"TH\":\n","        return 1\n","    elif country == \"CN\":\n","        return 2\n","    elif language == \"ja\":\n","        return 0\n","    elif language == \"th\":\n","        return 1\n","    elif language == \"zh\":\n","        return 2\n","    else:\n","        return 3\n","test_df[\"name_handle_pattern\"] = np.vectorize(language_pattern)(test_df[\"country\"], test_df[\"name_language\"])     \n","test_df[\"address_handle_pattern\"] = np.vectorize(language_pattern)(test_df[\"country\"], test_df[\"address_language\"])\n","del ft_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3291,"status":"ok","timestamp":1656837663532,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"pJufGci46AgA","outputId":"12889d7d-f58e-43ae-eeb8-920a6700b500"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Parameter dict_type of Dictionary() is deprecated, use dict instead\n"]}],"source":["import mojimoji\n","import string\n","import unidecode\n","from sudachipy import Dictionary, SplitMode\n","import pykakasi\n","from unidecode import unidecode\n","import pythainlp\n","\n","\n","import emoji\n","emoji_set = set()\n","for k, v in emoji.UNICODE_EMOJI.items():\n","    emoji_set.update(v.keys())\n","punctuation_transer = str.maketrans(string.punctuation, \" \"*len(string.punctuation))\n","\n","kakasi = pykakasi.kakasi()\n","kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\n","kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\n","kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\n","conversion = kakasi.getConverter()\n","tokenizer = Dictionary(dict_type=\"full\").create()\n","tokenize_mode = SplitMode.A\n","\n","def decode_list(x):\n","    if type(x) is list:\n","        return list(map(decode_list, x))\n","    return x.decode(\"UTF-8\")\n","\n","def decode_utf8_tensor(x):\n","    return list(map(decode_list, x.to_list()))\n","\n","zh_segmenter = tf_text.HubModuleTokenizer(nadare_util_dir + \"zh_segmentation\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plK0t6QYvz4t"},"outputs":[],"source":["def remove_punctuation(x):\n","    return x.translate(punctuation_transer)\n","\n","def remove_emoji(x):\n","    return ''.join([c for c in x if c not in emoji_set])\n","\n","def zenhan_normalize(x):\n","    return mojimoji.han_to_zen(mojimoji.zen_to_han(x, kana=False), digit=False, ascii=False)\n","\n","japanese_except_words = set([\" \"])\n","def handle_japanese(x):\n","    x = zenhan_normalize(x)\n","    morphemes = tokenizer.tokenize(x, tokenize_mode)\n","    reading_forms = \" \".join([m.reading_form() for m in morphemes if (m.normalized_form() != \" \" and m.reading_form() != 'キゴウ')])\n","    romanize_forms = conversion.do(reading_forms)\n","    return romanize_forms\n","\n","def handle_thai(x):\n","    tokenized_word = pythainlp.tokenize.word_tokenize(x, engine=\"newmm\")\n","    romanized_words = []\n","    for word in tokenized_word:\n","        try:\n","            romanized_words.append(pythainlp.romanize(word, engine='royin'))\n","        except:\n","            pass\n","    return \" \".join(romanized_words)\n","\n","def handle_chinese(x):\n","    result = \" \".join([\"\".join(unidecode(decode_utf8_tensor(zh_segmenter.tokenize([word]))[0][0]).split()).lower()\n","                       for word in decode_utf8_tensor(zh_segmenter.tokenize([x]))[0]])\n","    return result\n","    \n","def language_handle(handle_pattern, text):\n","    if handle_pattern == 0:\n","        text = handle_japanese(text)\n","    elif handle_pattern == 1:\n","        text = handle_thai(text)\n","    elif handle_pattern == 2:\n","        text = handle_chinese(text)\n","    return text\n","\n","def sub(x):\n","    x = re.sub(r'[^a-zA-Z0-9\u0026 ]', ' ', x)\n","    x = re.sub(r' +', ' ', x)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"zV2ZYW9w-tAx"},"source":["# JP or TH or CN変換"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":794475,"status":"ok","timestamp":1656838457995,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"zPj44CG_7ck9","outputId":"f3267e9f-f0e6-4d25-c64b-7438efdd1b28"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9bb0be75911444679bb8b58c850f9cdb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/150443 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  from ipykernel import kernelapp as app\n"]}],"source":["test_df[\"sp_name_raw\"] = test_df[\"name\"].fillna(\" \")\n","name_handle_index = np.where((test_df[\"name_handle_pattern\"] \u003c= 2) \u0026 (test_df[\"sp_name_raw\"] != \" \"))[0]\n","\n","name_handled = []\n","for pattern, name in tqdm(zip(test_df[\"name_handle_pattern\"].values[name_handle_index],\n","                              test_df[\"sp_name_raw\"].values[name_handle_index]), total=len(name_handle_index)):\n","    name_handled.append(language_handle(pattern, name))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":21546,"status":"ok","timestamp":1656839560922,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"UZvm75Ga9Evd"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07ac0f78e4404a2c9c730f379e2b5e6f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/90478 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  from ipykernel import kernelapp as app\n"]}],"source":["test_df[\"sp_address_raw\"] = test_df[\"address\"].fillna(\" \")\n","address_handle_index = np.where((test_df[\"address_handle_pattern\"] \u003c= 2) \u0026 (test_df[\"sp_address_raw\"] != \" \"))[0]\n","\n","address_handled = []\n","for pattern, address in tqdm(zip(test_df[\"address_handle_pattern\"].values[address_handle_index],\n","                              test_df[\"sp_address_raw\"].values[address_handle_index]), total=len(address_handle_index)):\n","    address_handled.append(language_handle(pattern, address))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1656839562320,"user":{"displayName":"クルトン","userId":"17231302919530674825"},"user_tz":-540},"id":"H_C63VDz9P-X"},"outputs":[{"data":{"text/plain":["94"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del zh_segmenter, kakasi, conversion, tokenizer\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RxrT4ziN-0nu"},"outputs":[],"source":["test_df.loc[name_handle_index, \"sp_name_raw\"] = name_handled\n","test_df.loc[address_handle_index, \"sp_address_raw\"] = address_handled\n","cols = ['sp_name_raw', \"sp_address_raw\"]\n","for col in cols:\n","    test_df[col] = test_df[col].fillna('').apply(unidecode)\n","    test_df[col] = test_df[col].str.lower()\n","    test_df[col] = test_df[col].apply(remove_emoji)\n","    test_df[col] = test_df[col].apply(sub)"]},{"cell_type":"markdown","metadata":{"id":"Wmy59wjuADO_"},"source":["# 不明なアドレスの穴埋め"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"daj8tdL9_MWx"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","test_df[\"sp_name\"] = test_df[\"sp_name_raw\"]\n","\n","address_index = np.where(test_df[\"sp_address_raw\"] != \" \")[0]\n","knn = KNeighborsRegressor(n_neighbors=min(3, (~test_df[\"address\"].isna()).sum()), \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(test_df[['latitude','longitude']].values)[address_index], address_index)\n","address_neiighbor = address_index[knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']].values))[1]]\n","test_df[\"sp_address\"] = np.vectorize(lambda x: \" \".join(x))(pd.Series(test_df[\"sp_address_raw\"].values[address_neiighbor].tolist()))\n","test_df[\"sp_address\"] = test_df[\"sp_address\"].apply(sub)"]},{"cell_type":"markdown","metadata":{"id":"prAUfPeEBoQo"},"source":["# TSP Features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nTR419YD_W8s"},"outputs":[],"source":["import os\n","\n","country_index_df = pd.read_csv(nadare_feature_dir + 'country_tsp_index.csv')\n","state_index_df = pd.read_csv(nadare_feature_dir + 'state_tsp_index.csv')\n","city_index_df = pd.read_csv(nadare_feature_dir + 'city_tsp_index.csv')\n","geo_city_index_df = pd.read_csv(nadare_feature_dir + 'geo_city_tsp_index.csv')\n","category_index_df = pd.read_csv(nadare_feature_dir + 'category_tsp_index.csv').fillna(\"nan\")\n","\n","country_index_map = {country: i for country, i in country_index_df[[\"country\", \"country_index\"]].values}\n","state_index_map = {state: i for state, i in state_index_df[[\"state\", \"state_index\"]].values}\n","city_index_map = {city: i for city, i in city_index_df[[\"city\", \"city_index\"]].values}\n","geo_city_index_map = {geo_city: i for geo_city, i in geo_city_index_df[[\"geo_city\", \"geo_city_index\"]].values}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GnH9YNSnwIi1"},"outputs":[],"source":["category_ix_dict = {k: v for k, v in category_index_df[[\"category\", \"category_ix\"]].values}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YOnqzUMcBrT2"},"outputs":[],"source":["def get_value_with_default(dict_, value, default=0):\n","    return np.vectorize(lambda x: dict_.get(x, default))(value)\n","\n","def get_category_ix(df, category_ix_dict):\n","    cat_pairs = []\n","    all_category = set()\n","    for row in df[\"categories\"].drop_duplicates().dropna().values:\n","        for cat in row.split(\", \"):\n","            if len(cat):\n","                cat_pairs.append([row, cat])\n","                all_category.add(cat)\n","\n","    category_df = pd.DataFrame(cat_pairs, columns=[\"categories\", \"category\"])\n","    categories_df = df[[\"ix\", \"categories\"]].merge(category_df, on=\"categories\", how=\"left\").sort_values(by=\"ix\")\n","\n","    categories_df[\"category_ix\"] = get_value_with_default(category_ix_dict, categories_df[\"category\"].values, -1)\n","    categories_df = categories_df[categories_df[\"category_ix\"] \u003e= 0]\n","    categories_ix = tf.RaggedTensor.from_value_rowids(values=tf.constant(categories_df[\"category_ix\"].values, \"int32\"),\n","                                                              value_rowids=tf.constant(categories_df[\"ix\"].values, \"int32\"),\n","                                                              nrows=len(df))\n","    return categories_ix\n","category_ix_dict = {k: v for k, v in category_index_df[[\"category\", \"category_ix\"]].values}\n","test_categories_ix = get_category_ix(test_df, category_ix_dict)"]},{"cell_type":"markdown","metadata":{"id":"vsC7hC15DP1X"},"source":["# country, city, stateの推定"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gmrOtLeaBvMx"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","geocode_df = pd.read_csv(nadare_util_dir + \"geocode.csv\", header=None)\n","geocode_df.columns = [\"latitude\", \"longitude\", \"country\", \"geo_city\"]\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(geocode_df[['latitude','longitude']]), geocode_df.index)\n","geo_index = knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']]))[1].T[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"80AdhmdEzSBm"},"outputs":[],"source":["test_df[\"geo_country\"] = geocode_df[\"country\"].values[geo_index]\n","test_df[\"geo_city\"] = geocode_df[\"geo_city\"].values[geo_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EcoPvOL_CPbS"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(geo_city_index_df[['latitude','longitude']]), geo_city_index_df[\"geo_city_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']]))[1].T[0]\n","test_df[\"pseudo_geo_city\"] = np.where(test_df[\"geo_city\"].isin(geo_city_index_df[\"geo_city\"].values), test_df[\"geo_city\"].values, geo_city_index_df[\"geo_city\"][pseudo_index].values)\n","test_df[\"pseudo_geo_city_ix\"] = test_df[\"pseudo_geo_city\"].map(geo_city_index_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FeA3fybDDWY5"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(city_index_df[['latitude','longitude']]), city_index_df[\"city_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']]))[1].T[0]\n","test_df[\"pseudo_city\"] = np.where(test_df[\"city\"].isin(city_index_df[\"city\"].values), test_df[\"city\"].values, city_index_df[\"city\"][pseudo_index].values)\n","test_df[\"pseudo_city_ix\"] = test_df[\"pseudo_city\"].map(city_index_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZfSpEGBDhNg"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(state_index_df[['latitude','longitude']]), state_index_df[\"state_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']]))[1].T[0]\n","test_df[\"pseudo_state\"] = np.where(test_df[\"state\"].isin(state_index_df[\"state\"].values), test_df[\"state\"].values, state_index_df[\"state\"][pseudo_index].values)\n","test_df[\"pseudo_state_ix\"] = test_df[\"pseudo_state\"].map(state_index_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIEPW1rzDt0F"},"outputs":[],"source":["test_df[\"country_ix\"] = get_value_with_default(country_index_map, test_df[\"country\"], np.where(country_index_df[\"country\"] == \"NAN\")[0][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWfnTJ_4Aywt"},"outputs":[],"source":["ix_params_dict = {\"country\": {\"dimention\": 64, \"num_category\": len(country_index_df)+1},\n","                  \"state\": {\"dimention\": 64, \"num_category\": len(state_index_df)},\n","                  \"city\": {\"dimention\": 64, \"num_category\": len(city_index_df)},\n","                  \"geo_city\": {\"dimention\": 64, \"num_category\": len(city_index_df)}\n","                  }\n","test_ix_values = {\"country\": tf.convert_to_tensor(test_df[\"country_ix\"].values.astype(np.int32)),\n","                   \"state\": tf.convert_to_tensor(test_df[\"pseudo_state_ix\"].values.astype(np.int32)),\n","                   \"city\": tf.convert_to_tensor(test_df[\"pseudo_city_ix\"].values.astype(np.int32)),\n","                   \"geo_city\": tf.convert_to_tensor(test_df[\"pseudo_geo_city_ix\"].values.astype(np.int32)),\n","                   }\n","\n","class IxEmbeddingLayer(tf.keras.layers.Layer):\n","    def __init__(self, ix_params_dict, num_layer=1, out_dim=128):\n","        super(IxEmbeddingLayer, self).__init__()\n","        self.ix_params_dict = ix_params_dict\n","        self.num_layer = num_layer\n","        self.out_dim = out_dim\n","        self.embedding_layers = {k: tf.keras.layers.Embedding(v[\"num_category\"], v[\"dimention\"]) for k, v in ix_params_dict.items()}\n","        self.denses = [tf.keras.layers.Dense(out_dim, activation=\"gelu\")]\n","        self.out = tf.keras.layers.Dense(out_dim)\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","    \n","    def call(self, ix_values):\n","        results = []\n","        for k, v in ix_values.items():\n","            results.append(self.embedding_layers[k](v))\n","        X = tf.concat(results, axis=-1)\n","        \n","        for i in range(self.num_layer):\n","            X = self.drop_out(self.denses[i](X))\n","        return self.out(X)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hAdip1wAAl8C"},"outputs":[],"source":["# test_df.to_csv('./output/test_df_intermed.csv', index = False)"]},{"cell_type":"markdown","metadata":{"id":"afjA2nV1ICXm"},"source":["# Embedding Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esOD1CytD4Vl"},"outputs":[],"source":["def approx_ranks(logits):\n","    r\"\"\"Computes approximate ranks given a list of logits.\n","    Given a list of logits, the rank of an item in the list is one plus the total\n","    number of items with a larger logit. In other words,\n","    rank_i = 1 + \\sum_{j \\neq i} I_{s_j \u003e s_i},\n","    where \"I\" is the indicator function. The indicator function can be\n","    approximated by a generalized sigmoid:\n","    I_{s_j \u003c s_i} \\approx 1/(1 + exp(-(s_j - s_i)/temperature)).\n","    This function approximates the rank of an item using this sigmoid\n","    approximation to the indicator function. This technique is at the core\n","    of \"A general approximation framework for direct optimization of\n","    information retrieval measures\" by Qin et al.\n","    Args:\n","    logits: A `Tensor` with shape [batch_size, list_size]. Each value is the\n","      ranking score of the corresponding item.\n","    Returns:\n","    A `Tensor` of ranks with the same shape as logits.\n","    \"\"\"\n","    list_size = tf.shape(input=logits)[1]\n","    x = tf.tile(tf.expand_dims(logits, 2), [1, 1, list_size])\n","    y = tf.tile(tf.expand_dims(logits, 1), [1, list_size, 1])\n","    pairs = tf.sigmoid(y - x)\n","    return tf.reduce_sum(input_tensor=pairs, axis=-1) + .5\n","    \n","    \n","class RankMSE(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(RankMSE, self).__init__()\n","    \n","    def call(self, y_true, y_pred):        \n","        weight = 1 / y_true\n","        rmse = tf.reduce_sum(tf.math.sqrt(tf.math.square(y_true - y_pred))*weight, axis=-1) / tf.reduce_sum(weight, axis=1)\n","        \n","        return tf.reduce_mean(rmse)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qy_yE9djELUp"},"outputs":[],"source":["class MixerModel(tf.keras.Model):\n","    \n","    def __init__(self, name_model, address_model, ix_emb_layer, cat_emb_layer, num_layer=2, num_dim=128):\n","        super(MixerModel, self).__init__()\n","        self.num_layer = num_layer\n","        self.num_dim = num_dim\n","        self.name_model = name_model\n","        self.address_model = address_model\n","        self.ix_emb_layer = ix_emb_layer\n","        self.cat_emb_layer = cat_emb_layer\n","        \n","        self.tokenize_norm = [tf.keras.layers.LayerNormalization() for i in range(4)]\n","    \n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        \n","        self.pointwise_filter_denses = [tf.keras.layers.Dense(num_dim, activation=\"gelu\") for _ in range(self.num_layer)]\n","        self.pointwise_out_denses = [tf.keras.layers.Dense(num_dim) for _ in range(self.num_layer)]\n","        self.depthwise_filter_denses = [tf.keras.layers.Dense(4, activation=\"gelu\") for _ in range(self.num_layer)]\n","        self.depthwise_out_denses = [tf.keras.layers.Dense(4) for _ in range(self.num_layer)]\n","        self.norm_layers = [tf.keras.layers.Dense(4) for _ in range(self.num_layer)]\n","        \n","    def check_tensor(self, x, ix):\n","        if isinstance(x, tf.RaggedTensor):\n","            x = x.to_tensor()\n","        return x\n","        \n","    def call(self, info, training=False):\n","        ix, name, address, position, ix_values, categories, size = info\n","        X_name = self.tokenize_norm[0](tf.reduce_sum(self.name_model(name), axis=-2))\n","        X_address = self.tokenize_norm[1](tf.reduce_sum(self.address_model(address), axis=-2))\n","        X_ix = self.tokenize_norm[2](self.drop_out(self.ix_emb_layer(ix_values)))\n","        X_categories = self.tokenize_norm[3](tf.reduce_sum(self.drop_out(self.cat_emb_layer(categories)), axis=-2))\n","        \n","        X = tf.stack([X_name, X_address, X_ix, X_categories], axis=-2)\n","        \n","        for i in range(self.num_layer):\n","            X_ = tf.nn.l2_normalize(X, axis=-1)\n","            X_T = tf.linalg.matrix_transpose(X_)\n","            X_T = self.depthwise_out_denses[i](self.depthwise_filter_denses[i](X_T))\n","            X = X + tf.linalg.matrix_transpose(X_T)\n","            X_ = tf.nn.l2_normalize(X, axis=-1)\n","            X_ = self.pointwise_out_denses[i](self.pointwise_filter_denses[i](X_))\n","            X = X + X_\n","        return tf.reduce_mean(X, axis=-2)\n","\n","def simcse_task(mixer_model, loss_func, neighbor_info):\n","    X_a = tf.nn.l2_normalize(mixer_model(neighbor_info, training=True), axis=-1)\n","    X_b = tf.nn.l2_normalize(mixer_model(neighbor_info, training=True), axis=-1)\n","    shape = tf.shape(X_a)\n","    X_a = tf.reshape(X_a, [-1, neighbor_info[-1], shape[-1]])\n","    X_b = tf.reshape(X_b, [-1, neighbor_info[-1], shape[-1]])\n","    shape = tf.shape(X_a)\n","    \n","    cossim_c = tf.reshape(tf.einsum(\"nad,nbd-\u003enab\", X_a, X_b), [shape[0]*shape[1], shape[1]])\n","    cossim_r = tf.reshape(tf.einsum(\"nad,nbd-\u003enab\", tf.transpose(X_a, [1, 0, 2]), tf.transpose(X_b, [1, 0, 2])), [shape[0]*shape[1], shape[0]])\n"," \n","    label_c = tf.reshape(tf.repeat(tf.expand_dims(tf.eye(tf.shape(X_a)[1]), axis=0), tf.shape(X_a)[0], axis=0), tf.shape(cossim_c))\n","    label_r = tf.reshape(tf.repeat(tf.expand_dims(tf.eye(tf.shape(X_a)[0]), axis=1), tf.shape(X_a)[1], axis=0), tf.shape(cossim_r))\n","    \n","    scale_c = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label_c)[1], \"float32\") - 1.)\n","    scale_r = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label_r)[1], \"float32\") - 1.)\n","    loss = loss_func(label_c, cossim_c * scale_c) + loss_func(label_r, cossim_r * scale_r)\n","    return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zU6Ehm8PESKn"},"outputs":[],"source":["class LocationModel(tf.keras.Model):\n","    def __init__(self, address_spe, ix_emb_layer, hidden_dims=128, num_rank=256, alpha=1.):\n","        super(LocationModel, self).__init__()\n","        self.address_spe = address_spe\n","        self.ix_emb_layer = ix_emb_layer\n","        self.num_rank = num_rank\n","        self.dense = tf.keras.layers.Dense(hidden_dims, use_bias=False)\n","        self.address_norm_n = tf.keras.layers.LayerNormalization()\n","        self.address_norm_q = tf.keras.layers.LayerNormalization()\n","        self.alpha = alpha\n","        self.loss_func = RankMSE()\n","        \n","    def haversine(self, X, Y):\n","        delta = Y - X\n","        x_lats = tf.gather(X, 0, axis=-1)\n","        y_lats = tf.gather(Y, 0, axis=-1)\n","        dlat = tf.gather(delta, 0, axis=-1)\n","        dlon = tf.gather(delta, 1, axis=-1)\n","\n","        a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","        c = tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a))\n","        return c\n","    \n","    def call(self, query_info, neighbor_info):\n","        # ix, name, address, position, ix_values, categories  \n","        _, _, query_address, query_position, query_ix_values, _ = query_info\n","        X_address = self.address_norm_q(tf.reduce_sum(self.address_spe(query_address), axis=1))\n","        X_ix = self.ix_emb_layer(query_ix_values)\n","        X_query = tf.nn.l2_normalize(self.dense(tf.concat([X_address, X_ix], axis=-1)), axis=-1)\n","        query_distance = self.haversine(tf.expand_dims(query_position, axis=0), tf.expand_dims(query_position, axis=1))\n","\n","        _, _, neighbor_address, neighbor_position, neighbor_ix_values, _ = neighbor_info\n","        X_address = self.address_norm_n(tf.reduce_sum(self.address_spe(neighbor_address), axis=2).to_tensor())\n","        X_ix = self.ix_emb_layer(neighbor_ix_values)\n","        X_neighbor = tf.nn.l2_normalize(self.dense(tf.concat([X_address, X_ix], axis=-1)), axis=-1)\n","        neighbor_distance = self.haversine(tf.expand_dims(query_position, axis=1), neighbor_position)\n","        \n","        distance = tf.concat([query_distance, neighbor_distance], axis=-1)\n","        X_target = tf.concat([tf.repeat(tf.expand_dims(X_query, axis=1), tf.shape(X_query)[0], axis=1), X_neighbor], axis=1)\n","        cossim = tf.einsum(\"nd,nmd-\u003enm\", X_query, X_target)\n","        \n","        y_true = tf.argsort(tf.argsort(tf.random.uniform(tf.shape(distance)), axis=1), axis=1) + 1\n","        calc_position = tf.where(y_true \u003c= self.num_rank)\n","        y_true = tf.cast(tf.reshape(tf.gather_nd(y_true, calc_position), [-1, self.num_rank]), \"float32\")\n","        cossim = tf.reshape(tf.gather_nd(cossim, calc_position), [-1, self.num_rank])\n","        y_pred = approx_ranks(cossim * self.alpha)\n","        loss = self.loss_func(y_true, y_pred)\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKIkDxWpEXu3"},"outputs":[],"source":["class CategoryEmbeddingLayer(tf.keras.layers.Layer):\n","    \n","    def __init__(self, num_categories, dim_categories, init_embedding=None):\n","        super(CategoryEmbeddingLayer, self).__init__()\n","        self.num_categories = num_categories\n","        self.dim_categories = dim_categories\n","        self.category_dense = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=(num_categories, dim_categories)), trainable=True, name=self.name + \"/category_embedding\")\n","        if not init_embedding is None:\n","            self.category_dense.assign(init_embedding)\n","            \n","    def call(self, C):\n","        X = tf.gather(self.category_dense, C)\n","        return X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHYcgr1NEaXE"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow_text import SentencepieceTokenizer\n","\n","class SupervisedContrastiveLoss(tf.keras.layers.Layer):\n","    def __init__(self, from_logits=False):\n","        super(SupervisedContrastiveLoss, self).__init__()\n","        if not from_logits:\n","            NotImplementedError\n","        \n","    @tf.function\n","    def call(self, true, pred, sample_weight=None):\n","        if sample_weight is None:\n","            sample_weight = tf.ones(tf.shape(pred), dtype=tf.float32)\n","        epred = tf.exp(pred) * sample_weight\n","        scale = tf.reduce_sum(epred, axis=1, keepdims=True)\n","        loss = tf.reduce_sum((tf.math.log(scale - epred * true) - true*pred) * true * sample_weight) / tf.reduce_sum(true*sample_weight)\n","        return loss\n","    \n","class SentencePieceEmbeddingLayer(tf.keras.layers.Layer):\n","    \n","    def __init__(self, vocab_size, out_dim, sp_model_path, init_embedding=None):\n","        super(SentencePieceEmbeddingLayer, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.out_dim = out_dim\n","        model = open(sp_model_path, \"rb\").read()\n","        self.tokenizer = SentencepieceTokenizer(model)\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, out_dim)\n","        if not init_embedding is None:\n","            self.embedding(0)\n","            self.embedding.trainable_variables[0].assign(init_embedding)\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","    \n","    def call(self, X):\n","        token = self.tokenizer.tokenize(X)\n","        X = self.drop_out(self.embedding(token))\n","        return X\n","    \n","class DpConvWrapper(tf.keras.layers.Layer):\n","    \n","    def __init__(self, sp_layer, num_layer=2):\n","        super(DpConvWrapper, self).__init__()\n","        self.sp_layer = sp_layer\n","        self.num_layer = num_layer\n","        self.out_dim = self.sp_layer.out_dim\n","        self.norms = [tf.keras.layers.LayerNormalization() for i in range(num_layer)]\n","        self.dep_convs = [tf.keras.layers.DepthwiseConv1D(kernel_size=3, strides=1, padding=\"same\", activation=\"gelu\") for _ in range(num_layer)]\n","        self.denses = [tf.keras.layers.Dense(sp_layer.out_dim) for _ in range(num_layer)]\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        \n","        \n","    def call(self, X):\n","        token = name_dpc.sp_layer.tokenizer.tokenize(X) + 1\n","        start = tf.cast(tf.concat([tf.zeros([1], dtype=\"int64\"), tf.cumsum(token.row_lengths())], axis=0), \"int32\")\n","        row_ids = tf.cast(token.value_rowids(), \"int32\")\n","        col_ids = tf.range(tf.shape(token.value_rowids())[0], dtype=\"int32\") - tf.gather(start, token.value_rowids())\n","        X = self.drop_out(name_dpc.sp_layer.embedding(token.to_tensor(default_value=2)))\n","        rank = tf.rank(X)\n","        shape = tf.shape(X)\n","        \n","        for i in range(name_dpc.num_layer):\n","            X_ = name_dpc.norms[i](X)\n","            X_ = name_dpc.dep_convs[i](X_)\n","            X_ = name_dpc.denses[i](X_)\n","            X += X_\n","            \n","        X = tf.RaggedTensor.from_value_rowids(tf.gather_nd(X, tf.stack([row_ids, col_ids], axis=1)),\n","                                              token.value_rowids(),\n","                                              nrows=tf.cast(shape[0], \"int64\"))\n","        return X        \n","\n","    \n","class SkipgramModel(tf.keras.Model):\n","    \n","    def __init__(self, name_spe):\n","        super(SkipgramModel, self).__init__()\n","        self.name_spe = name_spe\n","        \n","    @tf.function(experimental_relax_shapes=True)\n","    def call(self, name, n_name):\n","        X = self.name_spe(name)\n","        X_sum_neighbor = tf.reduce_sum(self.name_spe(n_name), axis=1).to_tensor()\n","        name_skip_loss = self.skipgram_task(X, X_sum_neighbor)\n","        return name_skip_loss\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def skipgram_task(self, X, X_sum_neighbor):\n","        X_sum = tf.reduce_sum(X, axis=1)\n","        X_sum_other = tf.gather(X_sum, X.value_rowids()) - X.values\n","\n","        \n","        X_sum_norm = tf.nn.l2_normalize(X_sum, axis=1)\n","        X_sum_other_norm = tf.nn.l2_normalize(X_sum_other, axis=1)\n","        X_sum_neighbor_norm = tf.slice(tf.gather(tf.nn.l2_normalize(X_sum_neighbor, axis=-1), X.value_rowids()), [0, 1, 0], [-1, -1, -1])\n","        X_value_norm = tf.nn.l2_normalize(X.values, axis=1)\n","        \n","\n","        correct_cossim = tf.expand_dims(tf.einsum(\"Vd,Vd-\u003eV\", X_value_norm, X_sum_other_norm), axis=1)\n","        wrong_cossim = tf.einsum(\"Vd,Nd-\u003eVN\", X_value_norm, X_sum_norm)\n","        neighbor_cossim = tf.einsum(\"Vd,VNd-\u003eVN\", X_value_norm, X_sum_neighbor_norm)\n","        cossim = tf.concat([correct_cossim, wrong_cossim, neighbor_cossim], axis=1)\n","\n","        rowwise_mask = tf.expand_dims(tf.gather(X.row_lengths(), X.value_rowids()) \u003e 1, axis=1)\n","        sameid_mask = tf.concat([tf.expand_dims(tf.cast(X.value_rowids(), \"int32\"), axis=1) != tf.expand_dims(tf.range(tf.shape(X.row_lengths())[0] + 1)-1, axis=0),\n","                                 tf.ones(tf.gather(tf.shape(X_sum_neighbor_norm), [0, 1], axis=0), dtype=\"bool\")], axis=1)\n","\n","        mask = tf.math.logical_and(rowwise_mask, sameid_mask)\n","        label = tf.concat([tf.ones(tf.shape(correct_cossim)), tf.zeros(tf.shape(wrong_cossim)), tf.zeros(tf.shape(neighbor_cossim))], axis=1)\n","        \n","        # Fixed AdaCos paramet2r\n","        scale = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label)[1], \"float32\") - 1.)\n","\n","        label = tf.where(mask, label, tf.zeros(tf.shape(mask)))\n","        pred = tf.where(mask, scale * cossim, scale * -1.)\n","        loss = tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(label, pred, from_logits=True))\n","        return loss   \n","    \n","class ClassifyTrainModel(tf.keras.Model):\n","    \n","    def __init__(self, name_model, address_model, ix_emb_layer, cat_emb_layer):\n","        super(ClassifyTrainModel, self).__init__()\n","        self.name_model = name_model\n","        self.address_model = address_model\n","        self.ix_emb_layer = ix_emb_layer\n","        self.cat_emb_layer = cat_emb_layer\n","        self.dense = tf.keras.layers.Dense(cat_emb_layer.dim_categories, use_bias=True)\n","        self.scale = tf.sqrt(2.) * tf.math.log(tf.cast(cat_emb_layer.num_categories, \"float32\") - 1.)\n","        self.loss_function = SupervisedContrastiveLoss(from_logits=True)\n","        \n","        self.layer_norms = [tf.keras.layers.LayerNormalization() for i in range(3)]\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        self.nan_mask = tf.concat([tf.zeros([1, 1]), tf.ones([1, cat_emb_layer.num_categories-1])], axis=1)\n","        \n","    def transform(self, name, address, ix_values):\n","        X_name = self.layer_norms[0](tf.reduce_sum(self.drop_out(self.name_model(name)), axis=1))\n","        X_address = self.layer_norms[1](tf.reduce_sum(self.drop_out(self.address_model(address)), axis=1))\n","        X_ix = self.layer_norms[2](self.ix_emb_layer(ix_values))\n","        X = self.dense(tf.concat([X_name, X_address, X_ix], axis=-1))\n","        return X\n","        \n","    def call(self, name, address, ix_values, categories):\n","        X = self.transform(name, address, ix_values)\n","        classify_loss = self.classify_task(X, categories)\n","        return classify_loss\n","    \n","    def classify_task(self, X, C):\n","        cossim = tf.einsum(\"ND,CD-\u003eNC\", tf.nn.l2_normalize(X, axis=1), tf.nn.l2_normalize(self.cat_emb_layer.category_dense, axis=1))\n","        \n","        true = tf.scatter_nd(tf.stack([C.value_rowids(), C.values], axis=1),\n","                              tf.ones(tf.shape(C.values)),\n","                              [tf.shape(X)[0], self.cat_emb_layer.num_categories])\n","        true = true * self.nan_mask\n","        calc_loc = tf.where(tf.reduce_sum(true, axis=1) \u003e 0.)\n","        pred = self.scale * cossim\n","        loss = self.loss_function(tf.gather_nd(true, calc_loc), tf.gather_nd(pred, calc_loc))\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"za76ES2SEiY3"},"outputs":[],"source":["def haversine(X, Y):\n","    delta = Y - X\n","    x_lats = tf.gather(X, 0, axis=-1)\n","    y_lats = tf.gather(Y, 0, axis=-1)\n","    dlat = tf.gather(delta, 0, axis=-1)\n","    dlon = tf.gather(delta, 1, axis=-1)\n","\n","    a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","    c = 2 * tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a))\n","    return c\n","\n","\n","class TFMiniBatchHKmeans(tf.keras.Model):\n","    def __init__(self, n_cluster=64, learning_rate=0.1):\n","        super(TFMiniBatchHKmeans, self).__init__()\n","        self.n_cluster = n_cluster\n","        self.learning_rate = learning_rate\n","        self.centroids = tf.Variable(tf.zeros((n_cluster, 2), dtype=\"float32\"), trainable=False)\n","    \n","    def init_centroid(self, X):\n","        new_centroids = []\n","        choosed = tf.zeros(len(X), dtype=\"bool\")\n","\n","        ix = tf.cast(tf.random.uniform([1], maxval=len(X))//1, \"int32\")[0]\n","        choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","        new_centroids.append(tf.gather(X, ix))\n","        dist = tf.ones([tf.shape(X)[0]]) * tf.float32.max / 10\n","\n","        for i in tqdm(range(self.n_cluster-1)):\n","            centroid = tf.gather(X, ix)\n","    \n","            dist = tf.where(choosed, tf.keras.backend.epsilon() , tf.minimum(dist, tf.math.square(hkmeans.haversine(centroid, X))))\n","            prob = dist / tf.reduce_sum(dist, axis=0, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            ix = tf.cast(tf.argmax(gumbel), \"int32\")\n","            choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","            new_centroids.append(tf.gather(X, ix))\n","        self.centroids.assign(tf.stack(new_centroids, axis=0))\n","        \n","    def partial_fit(self, X):\n","        score = self.haversine(tf.expand_dims(X, axis=1), tf.expand_dims(self.centroids, axis=0))\n","        maxix = tf.argmin(score, axis=1)\n","        new_centroids = tf.math.unsorted_segment_mean(X, maxix, num_segments=self.n_cluster)\n","        num_member = tf.math.unsorted_segment_sum(tf.ones((len(X), 1)), maxix, num_segments=self.n_cluster)\n","        new_centroids = tf.where(num_member \u003e 0, new_centroids, self.centroids)\n","        new_centroids = new_centroids * self.learning_rate + self.centroids * (1 - self.learning_rate)\n","        dist = tf.reduce_mean(tf.reduce_min(score, axis=1))\n","        self.centroids.assign(new_centroids)\n","        return dist\n","    \n","    def haversine(self, X, Y):\n","        delta = Y - X\n","        x_lats = tf.gather(X, 0, axis=-1)\n","        y_lats = tf.gather(Y, 0, axis=-1)\n","        dlat = tf.gather(delta, 0, axis=-1)\n","        dlon = tf.gather(delta, 1, axis=-1)\n","\n","        a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","        c = tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a))\n","        return c\n","    \n","    def transform(self, X, return_score=False):\n","        score = tf.math.log(self.haversine(tf.expand_dims(X, axis=1), tf.expand_dims(self.centroids, axis=0)) + tf.keras.backend.epsilon())\n","        res = tf.argmin(score, axis=-1)\n","        if return_score:\n","            dist = tf.reduce_min(score, axis=-1)\n","            return res, dist\n","        else:\n","            return res\n","\n","class TFMiniBatchSKmeans(tf.keras.Model):\n","    def __init__(self, n_cluster=64, n_dim=100, alpha=3/2, learning_rate=0.1):\n","        super(TFMiniBatchSKmeans, self).__init__()\n","        self.n_dim = n_dim\n","        self.n_cluster = n_cluster\n","        self.alpha = alpha\n","        self.learning_rate = learning_rate\n","\n","        self.centroids = tf.Variable(tf.zeros((n_cluster, n_dim), dtype=\"float32\"), trainable=False)\n","    \n","    def init_centroid(self, X):\n","        new_centroids = []\n","        X = tf.nn.l2_normalize(X, axis=-1)\n","        choosed = tf.zeros(len(X), dtype=\"bool\")\n","\n","        ix = tf.cast(tf.random.uniform([1], maxval=len(X))//1, \"int32\")[0]\n","        choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","        new_centroids.append(tf.gather(X, ix))\n","        \n","        dist = tf.ones([tf.shape(X)[0]]) * tf.float32.max / 10\n","\n","        for i in tqdm(range(self.n_cluster-1)):\n","            centroid = tf.gather(X, ix)\n","            cos = tf.einsum(\"d,nd-\u003en\", centroid, X)\n","            dist = tf.where(choosed, tf.keras.backend.epsilon() , tf.maximum(tf.keras.backend.epsilon(), tf.minimum(dist, 1. - cos)))         \n","            prob = dist / tf.reduce_sum(dist, axis=0, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            ix = tf.cast(tf.argmax(gumbel), \"int32\")\n","            choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","            new_centroids.append(X[ix])\n","        self.centroids.assign(tf.stack(new_centroids, axis=0))\n","        \n","    @tf.function\n","    def partial_fit(self, X):\n","        score = tf.einsum(\"nd,kd-\u003enk\", X, self.centroids)\n","        maxix = tf.argmax(score, axis=1)\n","        new_centroids = tf.math.unsorted_segment_mean(X, maxix, num_segments=self.n_cluster)\n","        num_member = tf.math.unsorted_segment_sum(tf.ones((len(X), 1)), maxix, num_segments=self.n_cluster)\n","        new_centroids = tf.where(num_member \u003e 0, new_centroids, self.centroids)\n","        new_centroids = tf.nn.l2_normalize(new_centroids, axis=1)\n","        new_centroids = new_centroids * self.learning_rate + self.centroids * (1 - self.learning_rate)\n","        new_centroids = tf.nn.l2_normalize(new_centroids, axis=1)\n","        dist = tf.reduce_mean(self.alpha - tf.reduce_max(score, axis=1))\n","        self.centroids.assign(new_centroids)\n","        return dist\n","    \n","    def transform(self, X, return_score=False):\n","        score = tf.einsum(\"...d,kd-\u003e...k\", X, self.centroids)\n","        res = tf.argmax(score, axis=-1)\n","        if return_score:\n","            cossim = tf.reduce_max(score, axis=-1)\n","            return res, cossim\n","        else:\n","            return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsEYV1WwEkbC"},"outputs":[],"source":["class DataContainer():\n","    def __init__(self, df, ix_values, category_ix, positive_ix, neighbor_ix, neighbor_dist):\n","        self.name = tf.constant(df[\"sp_name\"].values)\n","        self.address = tf.constant(df[\"sp_address\"].values)\n","        self.position = tf.expand_dims(tf.constant(np.deg2rad(df[[\"latitude\", \"longitude\"]].astype(np.float32).values), dtype=\"float32\"), axis=0)\n","        self.pid = tf.constant(df[\"pid\"].astype(np.int32).values, dtype=\"int32\")\n","        self.group = tf.constant(df[\"group\"].astype(np.int32).values, dtype=\"int32\")\n","        self.ix_values = ix_values\n","        self.category_ix = category_ix\n","        self.positive_ix = positive_ix\n","        self.neighbor_ix = neighbor_ix\n","        self.neighbor_dist = neighbor_dist\n","        \n","    def get_position(self, ix):\n","        return tf.gather(tf.gather(self.position, ix, axis=1), 0)\n","\n","    def call(self, ix, size=1):\n","        name = tf.gather(self.name, ix)\n","        address = tf.gather(self.address, ix)\n","        position = self.get_position(ix)\n","        categories = tf.gather(self.category_ix, ix)\n","        ix_values = {k: tf.gather(self.ix_values[k], ix) for k in self.ix_values.keys()}\n","        return ix, name, address, position, ix_values, categories, size\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def log_haversine(self, X, Y):\n","        delta = Y - X\n","        x_lats = tf.gather(X, 0, axis=-1)\n","        y_lats = tf.gather(Y, 0, axis=-1)\n","        dlat = tf.gather(delta, 0, axis=-1)\n","        dlon = tf.gather(delta, 1, axis=-1)\n","\n","        a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","        c = tf.math.log(2 * tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a)) + tf.keras.backend.epsilon())\n","        return c\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def negative_sample_by_score_top_k(self, ix, score, except_loc=None, k=128, distcut=4):\n","        pos_ix = tf.gather(self.positive_ix, tf.gather(self.pid, ix))\n","        score = tf.tensor_scatter_nd_update(score,\n","                                            tf.stack([tf.cast(pos_ix.value_rowids(), \"int32\"), pos_ix.values], axis=1),\n","                                            tf.ones([tf.shape(pos_ix.values)[0]])*tf.float32.min/10.)\n","        if not except_loc is None:\n","            score = tf.tensor_scatter_nd_update(score,\n","                                                except_loc,\n","                                                tf.ones([tf.shape(except_loc)[0]])*tf.float32.min/10.)            \n","        score, neighbor = tf.math.top_k(score, k=k)\n","        _, order = tf.math.top_k(tf.random.uniform([tf.shape(ix)[0], k]), k=k)\n","        score = tf.gather(score, order, batch_dims=1)\n","        neighbor = tf.gather(neighbor, order, batch_dims=1)\n","        \n","        score = tf.reverse_sequence(score, \n","                                    tf.ones([tf.shape(dist)[0]], dtype=\"int32\") * tf.shape(dist)[1],\n","                                    seq_axis=1)\n","        neighbor = tf.reverse_sequence(neighbor,\n","                                  tf.ones([tf.shape(neighbor)[0]], dtype=\"int32\") * tf.shape(neighbor)[1],\n","                                  seq_axis=1)\n","        return [score, neighbor]\n","    \n","    def get_positive_info(self, ix):\n","        pos_ix = tf.gather(self.positive_ix, tf.gather(self.pid, ix))\n","        Y = self.get_position(pos_ix)\n","        X = self.get_position(tf.gather(ix, pos_ix.value_rowids()))\n","        pos_dist = tf.RaggedTensor.from_value_rowids(self.log_haversine(X, Y.values), Y.value_rowids())\n","        pos_ix = pos_ix.to_tensor(default_value=-1)\n","        pos_ix = tf.where(tf.expand_dims(ix, axis=1) != pos_ix, pos_ix, -1)\n","        pos_dist = pos_dist.to_tensor(default_value=0.)\n","        return pos_dist, pos_ix\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def random_walk_sampling(self, query_ix, step, top_k):\n","        query_ix = tf.cast(query_ix, \"int32\")\n","        searched = tf.expand_dims(query_ix, axis=1)\n","        current = tf.identity(query_ix)\n","\n","        for i in range(step):\n","            n_ix = tf.gather(self.neighbor_ix, current)\n","            n_dist = tf.gather(self.neighbor_dist, current)\n","            n_dist = tf.where(tf.reduce_any(tf.expand_dims(n_ix, axis=2) == tf.expand_dims(searched, axis=1), axis=2), tf.keras.backend.epsilon(), n_dist)\n","\n","            prob = n_dist / tf.reduce_sum(n_dist, axis=1, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            nexts_ = tf.gather(n_ix, tf.math.top_k(gumbel, k=top_k)[1], batch_dims=1)\n","            searched = tf.concat([searched, nexts_], axis=1)\n","            current = tf.gather(nexts_, 0, axis=1)\n","        return tf.reshape(searched, [-1]), step*top_k + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XTRwvjOqqhYS"},"outputs":[],"source":["DIMSIZE = 128\n","WALK_STEP = 128\n","WALK_TOPK = 2\n","num_categories = max(category_ix_dict.values()) + 1\n","\n","name_spe = SentencePieceEmbeddingLayer(32000, DIMSIZE, nadare_feature_dir + \"sp_name.model\", None)\n","address_spe = SentencePieceEmbeddingLayer(32000, DIMSIZE, nadare_feature_dir + \"sp_address.model\", None)\n","\n","ix_emb_layer = IxEmbeddingLayer(ix_params_dict, num_layer=1, out_dim=128)\n","cat_emb_layer = CategoryEmbeddingLayer(num_categories, DIMSIZE)\n","\n","#locrank_model = LocationModel(address_dpc, ix_emb_layer, num_rank=128)\n","classify_model = ClassifyTrainModel(name_spe, address_spe, ix_emb_layer, cat_emb_layer)\n","mixer_model = MixerModel(name_spe, address_spe, ix_emb_layer, cat_emb_layer, num_layer=2, num_dim=128)\n","\n","mixer_loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","\n","\n","name_spe.embedding(0)\n","address_spe.embedding(0)\n","for k, v in ix_emb_layer.embedding_layers.items():\n","    v(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"livi7Cz8FHrG"},"outputs":[],"source":["classify_model.load_weights(nadare_feature_dir + \"classify_model\")\n","mixer_model.load_weights(nadare_feature_dir + \"mixer_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFWySnoLFQjN"},"outputs":[],"source":["container_cols = [\"sp_name\", \"sp_address\", \"latitude\", \"longitude\", \"pid\", \"group\"] \n","test_container =  DataContainer(test_df[container_cols],\n","                                   test_ix_values,\n","                                    test_categories_ix,\n","                                    None,\n","                                None,\n","                                None)\n","test_ixs = tf.range(len(test_container.name), dtype=\"int32\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtYUOPQ9FTJ1"},"outputs":[],"source":["normalized_category_dense = tf.nn.l2_normalize(cat_emb_layer.category_dense, axis=1)\n","pseudo_categories = []\n","category_ix_dict_r = {v: k for k, v in category_ix_dict.items()}\n","eval_ds = tf.data.Dataset.from_tensor_slices(test_ixs)\\\n","                .batch(128)\\\n","                .map(test_container.call)\n","\n","for ix, name, address, position, ix_values, categories, size in tqdm(eval_ds):\n","    X = tf.nn.l2_normalize(classify_model.transform(name, address, ix_values), axis=1)\n","    label = tf.argmax(tf.einsum(\"nd,md-\u003enm\", X, normalized_category_dense), axis=1)\n","    for l in label.numpy():\n","        pseudo_categories.append(category_ix_dict_r[l])\n","        \n","def remove_test_only_category(text):\n","    res = []\n","    for x in text.split(\", \"):\n","        if x in category_ix_dict.keys():\n","            res.append(x)\n","    return \", \".join(res)\n","\n","test_df[\"categories\"] = np.vectorize(remove_test_only_category)(test_df[\"categories\"])\n","test_df[\"categories\"] = np.where(test_df[\"categories\"] == \"\", pseudo_categories, test_df[\"categories\"])\n","test_categories_ix = get_category_ix(test_df, category_ix_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76wJ4zlkFXf1"},"outputs":[],"source":["# name\n","test_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(test_df)))\\\n","                .batch(1024)\\\n","                .map(test_container.call)\n","\n","name_embeddings = []\n","address_embeddings = []\n","ix_values_embeddings = []\n","mix_embeddings = []\n","for query_info in tqdm(test_ds):\n","    ix, name, address, position, ix_values, categories, size = query_info\n","    name_embeddings.append(tf.nn.l2_normalize(tf.reduce_sum(name_spe(name), axis=1), axis=1))\n","    address_embeddings.append(tf.nn.l2_normalize(tf.reduce_sum(address_spe(address), axis=1), axis=1))\n","    ix_values_embeddings.append(tf.nn.l2_normalize(ix_emb_layer(ix_values), axis=1))\n","    mix_embeddings.append(tf.nn.l2_normalize(mixer_model(query_info), axis=1))\n","    \n","\n","name_embeddings = tf.nn.l2_normalize(tf.concat(name_embeddings, axis=0), axis=1)\n","address_embeddings = tf.nn.l2_normalize(tf.concat(address_embeddings, axis=0), axis=1)\n","ix_values_embeddings = tf.nn.l2_normalize(tf.concat(ix_values_embeddings, axis=0), axis=1)\n","mix_embeddings = tf.nn.l2_normalize(tf.concat(mix_embeddings, axis=0), axis=1)\n","\n","category_embeddings = tf.nn.l2_normalize(tf.math.reduce_sum(tf.gather(cat_emb_layer.category_dense, test_categories_ix), axis=1), axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UGWtsAoFaPj"},"outputs":[],"source":["hkmeans = TFMiniBatchHKmeans(255*8, learning_rate=1.)\n","name_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(name_embeddings)[1], learning_rate=1.)\n","address_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(address_embeddings)[1], learning_rate=1.)\n","category_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(category_embeddings)[1], learning_rate=1.)\n","ix_values_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(ix_values_embeddings)[1], learning_rate=1.)\n","mix_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(mix_embeddings)[1], learning_rate=1.)\n","brand_kmeans = TFMiniBatchSKmeans(253*8, n_dim=tf.shape(mix_embeddings)[1], learning_rate=1.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hIq2DGlZqESZ"},"outputs":[],"source":["hkmeans.load_weights(nadare_feature_dir + \"haversine_kmeans\")\n","name_kmeans.load_weights(nadare_feature_dir + \"name_kmeans\")\n","address_kmeans.load_weights(nadare_feature_dir + \"address_kmeans\")\n","ix_values_kmeans.load_weights(nadare_feature_dir + \"ix_value_kmeans\")\n","mix_kmeans.load_weights(nadare_feature_dir + \"mix_kmeans\")\n","category_kmeans.load_weights(nadare_feature_dir + \"category_kmeans\")\n","brand_kmeans.load_weights(nadare_feature_dir + \"brand_kmeans\")\n"]},{"cell_type":"markdown","metadata":{"id":"kifkJAQXH9dz"},"source":["# embeddings\n","似たもの同士でまとめている"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzmzV2lnGmfA"},"outputs":[],"source":["BATCH_SIZE = 1024\n","eval_ds = tf.data.Dataset.from_tensor_slices(test_ixs)\\\n","                .batch(10000)\n","hkmeans_labels = []\n","name_kmeans_labels = []\n","address_kmeans_labels = []\n","ix_values_kmeans_labels = []\n","mix_kmeans_labels = []\n","category_kmeans_labels = []\n","brand_kmeans_labels = []\n","\n","hkmeans_scores = []\n","name_kmeans_scores = []\n","address_kmeans_scores = []\n","ix_values_kmeans_scores = []\n","mix_kmeans_scores = []\n","category_kmeans_scores = []\n","brand_kmeans_scores = []\n","\n","for ix in tqdm(eval_ds):\n","    res = hkmeans.transform(tf.gather(test_container.position[0], ix), return_score=True)\n","    hkmeans_labels.append(res[0])\n","    hkmeans_scores.append(res[1])\n","    \n","    res = name_kmeans.transform(tf.gather(name_embeddings, ix), return_score=True)\n","    name_kmeans_labels.append(res[0])\n","    name_kmeans_scores.append(res[1])\n","\n","    res = address_kmeans.transform(tf.gather(address_embeddings, ix), return_score=True)\n","    address_kmeans_labels.append(res[0])\n","    address_kmeans_scores.append(res[1])\n","\n","    res = ix_values_kmeans.transform(tf.gather(ix_values_embeddings, ix), return_score=True)\n","    ix_values_kmeans_labels.append(res[0])\n","    ix_values_kmeans_scores.append(res[1])\n","\n","    res = mix_kmeans.transform(tf.gather(mix_embeddings, ix), return_score=True)\n","    mix_kmeans_labels.append(res[0])\n","    mix_kmeans_scores.append(res[1])\n","    \n","    res = category_kmeans.transform(tf.gather(category_embeddings, ix), return_score=True)\n","    category_kmeans_labels.append(res[0])\n","    category_kmeans_scores.append(res[1])\n","    \n","    res = brand_kmeans.transform(tf.gather(name_embeddings, ix), return_score=True)\n","    brand_kmeans_labels.append(res[0])\n","    brand_kmeans_scores.append(res[1])    \n","\n","    \n","test_df[\"hkmeans_labels\"] = tf.concat(hkmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"name_kmeans_labels\"] = tf.concat(name_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"address_kmeans_labels\"] = tf.concat(address_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"ix_values_kmeans_labels\"] = tf.concat(ix_values_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"mix_kmeans_labels\"] = tf.concat(mix_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"category_kmeans_labels\"] = tf.concat(category_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"brand_kmeans_labels\"] = tf.concat(brand_kmeans_scores, axis=0).numpy().astype(np.int16)\n","\n","\n","test_df[\"hkmeans_scores\"] = tf.concat(hkmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"name_kmeans_scores\"] = tf.concat(name_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"address_kmeans_scores\"] = tf.concat(address_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"ix_values_kmeans_scores\"] = tf.concat(ix_values_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"mix_kmeans_scores\"] = tf.concat(mix_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"category_kmeans_scores\"] = tf.concat(category_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"brand_kmeans_scores\"] = tf.concat(brand_kmeans_scores, axis=0).numpy().astype(np.float16)\n","\n","\n","hkmeans_labels = []\n","name_kmeans_labels = []\n","address_kmeans_labels = []\n","ix_values_kmeans_labels = []\n","mix_kmeans_labels = []\n","category_kmeans_labels = []\n","brand_kmeans_labels = []\n","\n","hkmeans_scores = []\n","name_kmeans_scores = []\n","address_kmeans_scores = []\n","ix_values_kmeans_scores = []\n","mix_kmeans_scores = []\n","category_kmeans_scores = []\n","brand_kmeans_scores = []\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfFtgW7lqy72"},"outputs":[],"source":["del ix_values_kmeans, mix_kmeans_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpGERDRlISk3"},"outputs":[],"source":["gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZ4MH9b1ITH1"},"outputs":[],"source":["# test_df.to_csv('./output/test_df_intermed2.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4E84Sj87lEwb"},"outputs":[],"source":["# import pickle\n","\n","# with open('./output/name_embeddings.pkl', mode='wb') as f:\n","#     pickle.dump(name_embeddings, f)\n","# with open('./output/address_embeddings.pkl', mode='wb') as f:\n","#     pickle.dump(address_embeddings, f)\n","# with open('./output/ix_values_embeddings.pkl', mode='wb') as f:\n","#     pickle.dump(ix_values_embeddings, f)\n","# with open('./output/mix_embeddings.pkl', mode='wb') as f:\n","#     pickle.dump(mix_embeddings, f)\n","# with open('./output/category_embeddings.pkl', mode='wb') as f:\n","#     pickle.dump(category_embeddings, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqjDdzrY0nxF"},"outputs":[],"source":["import tensorflow_ranking as tfr\n","\n","class LogisticModel(tf.keras.Model):\n","    def __init__(self, dim):\n","        super(LogisticModel, self).__init__()\n","        self.scale = tf.Variable(tf.zeros([dim], dtype=\"float32\"), trainable=True)\n","        self.loss_func = tfr.keras.losses.ApproxNDCGLoss()\n","        \n","    def call(self, X, Y):\n","        logit = tf.einsum(\"nmd,d-\u003enm\", X, self.scale)# + self.bias\n","        loss = self.loss_func(Y, logit)\n","        return loss    \n","\n","simple_logistic_model = LogisticModel(5)\n","simple_logistic_model.load_weights(nadare_feature_dir + \"simple_logistic_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQlhO_zO1KGT"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","bagofword_vectorizer = TfidfVectorizer(min_df=0, binary=True, use_idf=False, norm=\"l2\", dtype=np.float32)\n","name_bow = bagofword_vectorizer.fit_transform(test_df[\"sp_name\"]).tocoo()\n","\n","name_bow_rag_data = tf.RaggedTensor.from_value_rowids(name_bow.data, name_bow.row)\n","name_bow_rag_col = tf.RaggedTensor.from_value_rowids(name_bow.col, name_bow.row)\n","name_bow_coo = tf.SparseTensor(tf.cast(tf.stack([name_bow.row, name_bow.col], axis=-1), \"int64\"), name_bow.data, name_bow.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrZo2etU1LeP"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","test_df[\"pid\"] = test_df[\"point_of_interest\"].map({v: i for i, v in enumerate(test_df[\"point_of_interest\"].unique())})\n","\n","group = -np.ones(len(test_df), dtype=\"int32\")\n","g = 0\n","for g, (dev_p_ids, val_p_ids) in enumerate(KFold(n_splits=5, shuffle=True, random_state=42).split(test_df[\"pid\"].unique())):\n","    ix = test_df[\"pid\"].isin(val_p_ids)\n","    group[ix] = g\n","test_df[\"group\"] = group\n","\n","test_df[\"pid\"] = test_df[\"point_of_interest\"].map({v: i for i, v in enumerate(test_df[\"point_of_interest\"].unique())})\n","\n","container_cols = [\"sp_name\", \"sp_address\", \"latitude\", \"longitude\", \"pid\", \"group\"] \n","test_container =  DataContainer(test_df[container_cols],\n","                                   test_ix_values,\n","                                    test_categories_ix,\n","                                    tf.cast(tf.ragged.constant(test_df.groupby(\"pid\")[\"ix\"].unique().tolist()), \"int32\"),\n","                                None,\n","                                None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csFLIhF81yjj"},"outputs":[],"source":["NUM_CANDIDATE = 20\n","NEAREST = 2\n","NAME_SIM = 2\n","NUM_FEAT = 7\n","test_ds = tf.data.Dataset.from_tensor_slices(test_ixs)\\\n","                .batch(128)\n","\n","# @tf.function(experimental_relax_shapes=True)\n","# def make_pred_feat(ix):\n","#     selected = tf.expand_dims(ix, axis=1)\n","#     pos_ix = tf.gather(test_container.positive_ix, tf.gather(test_container.pid, ix))\n","\n","#     query_position = tf.expand_dims(test_container.get_position(ix), axis=1)\n","#     dist = test_container.log_haversine(query_position, test_container.position)    \n","    \n","#     indices = tf.stack([tf.repeat(tf.range(len(ix)), tf.gather(name_bow_rag_data, ix).row_lengths()),\n","#                         tf.gather(name_bow_rag_col, ix).values], axis=-1)\n","#     values = tf.gather(name_bow_rag_data, ix).values\n","#     shape = [len(ix), name_bow_coo.shape[1]]\n","#     query = tf.scatter_nd(indices, values, shape)\n","#     sparse_name_sim = tf.sparse.sparse_dense_matmul(query, name_bow_coo, adjoint_b=True)\n","        \n","#     name_cossim = tf.einsum(\"nd,md-\u003enm\", tf.gather(name_embeddings, ix), name_embeddings)\n","#     address_cossim = tf.einsum(\"nd,md-\u003enm\", tf.gather(address_embeddings, ix), address_embeddings)\n","#     category_cossim = tf.einsum(\"nd,md-\u003enm\", tf.gather(category_embeddings, ix), category_embeddings)    \n","#     logistic_score = tf.einsum(\"d,nmd-\u003enm\", simple_logistic_model.scale, tf.stack([dist, sparse_name_sim, name_cossim, address_cossim, category_cossim], axis=-1))\n","    \n","    \n","#     score, topk = tf.math.top_k(-dist, NEAREST + tf.shape(selected)[1])\n","#     update_pos = tf.where(tf.reduce_any(tf.expand_dims(topk, axis=2) == tf.expand_dims(selected, axis=1), axis=2))\n","#     score = tf.tensor_scatter_nd_update(score, \n","#                                         update_pos,\n","#                                         tf.ones(tf.shape(update_pos)[0]) * tf.float32.min / 10)\n","#     topk = tf.gather(topk, tf.argsort(-score, axis=1), batch_dims=1)\n","#     selected = tf.concat([selected, tf.slice(topk, [0, 0], [-1, NEAREST])], axis=1)\n","    \n","#     score, topk = tf.math.top_k(name_cossim, NAME_SIM + tf.shape(selected)[1])\n","#     update_pos = tf.where(tf.reduce_any(tf.expand_dims(topk, axis=2) == tf.expand_dims(selected, axis=1), axis=2))\n","#     score = tf.tensor_scatter_nd_update(score, \n","#                                         update_pos,\n","#                                         tf.ones(tf.shape(update_pos)[0]) * tf.float32.min / 10)\n","#     topk = tf.gather(topk, tf.argsort(-score, axis=1), batch_dims=1)\n","#     selected = tf.concat([selected, tf.slice(topk, [0, 0], [-1, NAME_SIM])], axis=1)  \n","    \n","#     score, topk = tf.math.top_k(sparse_name_sim, NAME_SIM + tf.shape(selected)[1])\n","#     update_pos = tf.where(tf.reduce_any(tf.expand_dims(topk, axis=2) == tf.expand_dims(selected, axis=1), axis=2))\n","#     score = tf.tensor_scatter_nd_update(score, \n","#                                         update_pos,\n","#                                         tf.ones(tf.shape(update_pos)[0]) * tf.float32.min / 10)\n","#     topk = tf.gather(topk, tf.argsort(-score, axis=1), batch_dims=1)\n","#     selected = tf.concat([selected, tf.slice(topk, [0, 0], [-1, NAME_SIM])], axis=1)\n","    \n","#     score, topk = tf.math.top_k(logistic_score, NUM_CANDIDATE - 2*NAME_SIM - NEAREST + tf.shape(selected)[1])\n","#     update_pos = tf.where(tf.reduce_any(tf.expand_dims(topk, axis=2) == tf.expand_dims(selected, axis=1), axis=2))\n","#     score = tf.tensor_scatter_nd_update(score, \n","#                                         update_pos,\n","#                                         tf.ones(tf.shape(update_pos)[0]) * tf.float32.min / 10)\n","#     topk = tf.gather(topk, tf.argsort(-score, axis=1), batch_dims=1)\n","#     selected = tf.concat([selected, tf.slice(topk, [0, 0], [-1, NUM_CANDIDATE - 2*NAME_SIM - NEAREST])], axis=1)\n","   \n","    \n","#     candidate_ixs  = tf.cast(tf.slice(selected, [0, 1], [-1, -1]), \"int32\")\n","    \n","#     candidate_dist = tf.gather(dist, candidate_ixs, batch_dims=1)\n","#     candidate_sparse_name_sim = tf.gather(sparse_name_sim, candidate_ixs, batch_dims=1)\n","#     candidate_name_cossim = tf.gather(name_cossim, candidate_ixs, batch_dims=1)\n","#     candidate_address_cossim = tf.gather(address_cossim, candidate_ixs, batch_dims=1)\n","#     candidate_category_cossim = tf.gather(category_cossim, candidate_ixs, batch_dims=1)\n","#     candidate_logistic_score = tf.gather(logistic_score, candidate_ixs, batch_dims=1)\n","#     candidate_mix_cossim = tf.einsum(\"nd,nmd-\u003enm\", tf.gather(mix_embeddings, ix), tf.gather(mix_embeddings, candidate_ixs))\n","    \n","    \n","#     cross_feat = tf.stack([candidate_dist, candidate_sparse_name_sim, candidate_name_cossim, candidate_address_cossim, candidate_category_cossim, candidate_logistic_score, candidate_mix_cossim], axis=-1)\n","    \n","#     candidate_ixs = tf.reshape(candidate_ixs, [-1])\n","#     cross_feat = tf.reshape(cross_feat, [-1, NUM_FEAT])\n","    \n","#     return candidate_ixs, cross_feat\n","\n","\n","# query_ixs = []\n","# candidate_ixs = []\n","# cross_feats = []\n","\n","# for query_ix in tqdm(test_ds):\n","#     query_ix = tf.reshape(query_ix, [-1])\n","#     candidate_ix, cross_feat = make_pred_feat(query_ix)\n","    \n","#     query_ixs.append(query_ix)\n","#     candidate_ixs.append(candidate_ix)\n","#     cross_feats.append(cross_feat)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Og-kzhfG2Cv8"},"outputs":[],"source":["# query_ixs = tf.repeat(tf.concat(test_ixs, axis=0), NUM_CANDIDATE, axis=0)\n","# candidate_ixs = tf.concat(candidate_ixs, axis=0)\n","# cross_feats = tf.concat(cross_feats, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhRmb9HN2DJq"},"outputs":[],"source":["import tensorflow_text as tf_text\n","wsp_tokenizer = tf_text.WhitespaceTokenizer()\n","rouge_l = tf_text.metrics.rouge_l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNB22h1mlmDq"},"outputs":[],"source":["import pickle\n","\n","# with open('./output/query_ixs.pkl', mode='wb') as f:\n","#     pickle.dump(query_ixs, f)\n","# with open('./output/candidate_ixs.pkl', mode='wb') as f:\n","#     pickle.dump(candidate_ixs, f)\n","# with open('./output/cross_feats.pkl', mode='wb') as f:\n","#     pickle.dump(cross_feats, f)\n","with open('./output/query_ixs.pkl', mode='rb') as f:\n","    query_ixs = pickle.load(f)\n","with open('./output/candidate_ixs.pkl', mode='rb') as f:\n","    candidate_ixs = pickle.load(f)\n","with open('./output/cross_feats.pkl', mode='rb') as f:\n","    cross_feats = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvDoDVTwmamL"},"outputs":[],"source":["!pip install phonenumbers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pw9DvV6-2JqK"},"outputs":[],"source":["import phonenumbers\n","from urllib.parse import urlparse\n","\n","def get_phene_info(phone, country):\n","    if phone == \"\":\n","        return {\"phone_country_code\": 0, \"national_number\": 0}\n","    if phonenumbers.country_code_for_region(country) == 0:\n","        country = None\n","    try:\n","        res = phonenumbers.parse(phone, country)\n","    except:\n","        return {\"phone_country_code\": 0, \"national_number\": 0}\n","    return {\"phone_country_code\": res.country_code, \"national_number\": res.national_number}\n","res = []\n","for phone, country in test_df[[\"phone\", \"country\"]].values:\n","    res.append(get_phene_info(phone, country))\n","phone_df = pd.DataFrame(res)\n","\n","test_df[\"url\"] = test_df[\"url\"].fillna(\"\")\n","test_df[\"phone_national_number\"] = phone_df[\"national_number\"].astype(str)\n","test_df[\"url_netloc\"] = test_df[\"url\"].fillna(\"\").apply(lambda x: urlparse(x)[1])\n","test_df[\"url_path\"] = test_df[\"url\"].fillna(\"\").apply(lambda x: urlparse(x)[2])\n","test_df[\"name_numbers\"] = test_df[\"name\"].fillna(\"\").map(lambda x: \" \".join(re.findall(r\"\\d+\", x)))\n","test_df[\"address_numbers\"] = test_df[\"address\"].fillna(\"\").map(lambda x: \" \".join(re.findall(r\"\\d+\", x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gpbYDhv2Rwt"},"outputs":[],"source":["import Levenshtein\n","from joblib import Parallel, delayed\n","from difflib import SequenceMatcher\n","\n","\n","def make_data(query_ix, candidate_ix):\n","    query_info = {}\n","    candidate_info = {}\n","    query_info[\"ix\"] = query_ix\n","    candidate_info[\"ix\"] = candidate_ix\n","    \n","    for col in [\"sp_name\", \"sp_address_raw\", \"phone_national_number\", \"url\", \"phone_national_number\",\n","                \"url_netloc\", \"url_path\", \"name_numbers\", \"address_numbers\"]:\n","        query_info[col] = test_df.at[query_ix, col]\n","        candidate_info[col] = test_df.at[candidate_ix, col]\n","    query_info[\"name_numbers_split_size\"] = len(query_info[\"name_numbers\"].split())\n","    candidate_info[\"name_numbers_split_size\"] = len(candidate_info[\"name_numbers\"].split())\n","    query_info[\"address_numbers_split_size\"] = len(query_info[\"address_numbers\"].split())\n","    candidate_info[\"address_numbers_split_size\"] = len(candidate_info[\"address_numbers\"].split())    \n","        \n","    return query_info, candidate_info\n","\n","def add_features(query_info, candidate_info):\n","    res = []\n","    res.append(query_info[\"ix\"])\n","    res.append(candidate_info[\"ix\"])\n","    \n","    for col in [\"sp_name\", \"sp_address_raw\"]:\n","        s = query_info[col]\n","        match_s = candidate_info[col]\n","        res.append(abs(len(match_s) - len(s)))\n","        res.append(abs(len(match_s.split()) - len(s.split())))\n","        res.append(SequenceMatcher(None, s, match_s).ratio())\n","        res.append(Levenshtein.distance(s, match_s))\n","        res.append(res[-1] / (max(len(s), len(match_s)) + 1e-9))\n","        res.append(Levenshtein.jaro_winkler(s, match_s))\n","    \n","    for col in [\"phone_national_number\", \"url\", \"url_netloc\", \"url_path\"]:\n","        r = 0\n","        if min(len(query_info[col]), len(candidate_info[col])) \u003e 0:\n","            r = -1 + 2 * (query_info[col] == candidate_info[col])\n","        res.append(r)\n","    for col in [\"name_numbers\", \"address_numbers\"]:\n","        res.append(max(query_info[col + \"_split_size\"], candidate_info[col + \"_split_size\"]))\n","        res.append(min(query_info[col + \"_split_size\"], candidate_info[col + \"_split_size\"]))\n","        res.append(SequenceMatcher(None, query_info[col], candidate_info[col]).ratio())\n","    \n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYRF__7I48xP"},"outputs":[],"source":["# !pip install cdifflib polyleven\n","!pip install jarowinkler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hVHweuc48uo"},"outputs":[],"source":["from polyleven import levenshtein\n","from cdifflib import CSequenceMatcher\n","from jarowinkler import jarowinkler_similarity, jaro_similarity\n","\n","import Levenshtein\n","from joblib import Parallel, delayed\n","from difflib import SequenceMatcher\n","\n","\n","def make_data(query_ix, candidate_ix):\n","    query_info = {}\n","    candidate_info = {}\n","    query_info[\"ix\"] = query_ix\n","    candidate_info[\"ix\"] = candidate_ix\n","    \n","    for col in [\"sp_name\", \"sp_address_raw\", \"phone_national_number\", \"url\", \"phone_national_number\",\n","                \"url_netloc\", \"url_path\", \"name_numbers\", \"address_numbers\"]:\n","        query_info[col] = test_df.at[query_ix, col]\n","        candidate_info[col] = test_df.at[candidate_ix, col]\n","    query_info[\"name_numbers_split_size\"] = len(query_info[\"name_numbers\"].split())\n","    candidate_info[\"name_numbers_split_size\"] = len(candidate_info[\"name_numbers\"].split())\n","    query_info[\"address_numbers_split_size\"] = len(query_info[\"address_numbers\"].split())\n","    candidate_info[\"address_numbers_split_size\"] = len(candidate_info[\"address_numbers\"].split())    \n","        \n","    return query_info, candidate_info\n","\n","def add_features(query_info, candidate_info):\n","    res = []\n","    res.append(query_info[\"ix\"])\n","    res.append(candidate_info[\"ix\"])\n","    \n","    for col in [\"sp_name\", \"sp_address_raw\"]:\n","        s = query_info[col]\n","        match_s = candidate_info[col]\n","        if s != '' and match_s != '':\n","            res.append(abs(len(match_s) - len(s)))\n","            res.append(abs(len(match_s.split()) - len(s.split())))\n","            res.append(CSequenceMatcher(None, s, match_s).ratio())\n","            res.append(levenshtein(s, match_s))\n","            res.append(res[-1] / (max(len(s), len(match_s)) + 1e-9))\n","            res.append(Levenshtein.jaro_winkler(s, match_s))\n","#             res.append(jarowinkler_similarity(s, match_s))\n","#             res.append(jaro_similarity(s, match_s))\n","        else:\n","            cnt += 1\n","            res.append(abs(len(match_s) - len(s)))\n","            res.append(abs(len(match_s.split()) - len(s.split())))\n","            res.append(np.nan)\n","            res.append(np.nan)\n","            res.append(res[-1] / (max(len(s), len(match_s)) + 1e-9))\n","            res.append(np.nan)\n","    \n","    for col in [\"phone_national_number\", \"url\", \"url_netloc\", \"url_path\"]:\n","        r = 0\n","        if min(len(query_info[col]), len(candidate_info[col])) \u003e 0:\n","            r = -1 + 2 * (query_info[col] == candidate_info[col])\n","        res.append(r)\n","    for col in [\"name_numbers\", \"address_numbers\"]:\n","        res.append(max(query_info[col + \"_split_size\"], candidate_info[col + \"_split_size\"]))\n","        res.append(min(query_info[col + \"_split_size\"], candidate_info[col + \"_split_size\"]))\n","        s = query_info[col]\n","        match_s = candidate_info[col]\n","        if s != '' and match_s != '':\n","            res.append(CSequenceMatcher(None, query_info[col], candidate_info[col]).ratio())\n","        else:\n","            res.append(np.nan)\n","    \n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvvkWt36elHd"},"outputs":[],"source":["\n","def add_features(query_info, candidate_info):\n","    res = []\n","    res.append(query_info[\"ix\"])\n","    res.append(candidate_info[\"ix\"])\n","    \n","    for col in [\"sp_name\", \"sp_address_raw\"]:\n","        s = query_info[col]\n","        match_s = candidate_info[col]\n","        res.append(abs(len(match_s) - len(s)))\n","        res.append(abs(len(match_s.split()) - len(s.split())))\n","        res.append(CSequenceMatcher(None, s, match_s).ratio())\n","        res.append(levenshtein(s, match_s))\n","        res.append(res[-1] / (max(len(s), len(match_s)) + 1e-9))\n","        res.append(Levenshtein.jaro_winkler(s, match_s))\n","    \n","    for col in [\"phone_national_number\", \"url\", \"url_netloc\", \"url_path\"]:\n","        r = 0\n","        if min(len(query_info[col]), len(candidate_info[col])) \u003e 0:\n","            r = -1 + 2 * (query_info[col] == candidate_info[col])\n","        res.append(r)\n","    for col in [\"name_numbers\", \"address_numbers\"]:\n","        res.append(max(query_info[col + \"_split_size\"], candidate_info[col + \"_split_size\"]))\n","        res.append(min(query_info[col + \"_split_size\"], candidate_info[col + \"_split_size\"]))\n","        res.append(CSequenceMatcher(None, query_info[col], candidate_info[col]).ratio())\n","    \n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6dQCwQlsZow"},"outputs":[],"source":["# 検証用にここで絞る"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dC3ZeS5PSoE4"},"outputs":[],"source":["# query_ixs = query_ixs.numpy()[:1000000]\n","# candidate_ixs = candidate_ixs.numpy()[:1000000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nge7Bm--XSA4"},"outputs":[],"source":["# 高速化後\n","candidate_data_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dk7zGZ-g888"},"outputs":[],"source":["test_df[test_df['url']!=''][['url', 'url_netloc', 'url_path']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1abiwVA22oEK"},"outputs":[],"source":["%%time\n","\n","print(len(query_ixs))\n","results = Parallel(n_jobs=-1, verbose=1)(delayed(add_features)(*make_data(q_ix, c_ix)) for q_ix, c_ix in zip(query_ixs, candidate_ixs))\n","candidate_data_df = pd.DataFrame(results, columns = [\"query_ix\",\n","                                                     \"candidate_ix\",\n","                                                     \"name_delta_len\",\n","                                                     \"name_delta_words\",\n","                                                     \"name_gesh\", \n","                                                     \"name_leven\", \n","                                                     \"name_nleven\", \n","                                                     \"name_jaro\", \n","                                                     \"address_delta_len\", \n","                                                     \"address_delta_words\", \n","                                                     \"address_gesh\", \n","                                                     \"address_leven\", \n","                                                     \"address_nleven\", \n","                                                     \"address_jaro\",\n","                                                     \"phone_national_number_match\",\n","                                                     \"url_match\",\n","                                                     \"url_netloc_match\",\n","                                                     \"url_path_match\",\n","                                                     \"name_numbers_max\",\n","                                                     \"name_numbers_min\",\n","                                                     \"name_numbers_gesh\",\n","                                                     \"address_numbers_max\",\n","                                                     \"address_numbers_min\",\n","                                                     \"address_numbers_gesh\",])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Dtv1dezQqwR"},"outputs":[],"source":["2.9min\n","candidate_data_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSTU_MmJ2p0K"},"outputs":[],"source":["del results\n","gc.collect()\n","\n","int64_cols = ['name_delta_len', 'name_delta_words', 'name_leven', 'address_delta_len',\n","       'address_delta_words', 'address_leven', 'phone_national_number_match',\n","       'url_match', 'url_netloc_match', 'url_path_match', 'name_numbers_max',\n","       'name_numbers_min', 'address_numbers_max', 'address_numbers_min']\n","float64_cols = ['name_gesh', 'name_nleven', 'name_jaro', 'address_gesh',\n","       'address_nleven', 'address_jaro', 'name_numbers_gesh',\n","       'address_numbers_gesh']\n","candidate_data_df[int64_cols] = candidate_data_df[int64_cols].astype(np.int16)\n","candidate_data_df[float64_cols] = candidate_data_df[float64_cols].astype(np.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5mDckB-2r18"},"outputs":[],"source":["sp_names = tf.constant(test_df[\"sp_name\"].values)\n","sp_address = tf.constant(test_df[\"sp_address_raw\"].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOpOCi_d2tXF"},"outputs":[],"source":["text_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(query_ixs), tf.data.Dataset.from_tensor_slices(candidate_ixs)))\\\n","                         .batch(1024)\n","wsp_tokenizer = tf_text.WhitespaceTokenizer()\n","\n","scores = []\n","for q_ix, c_ix in tqdm(text_ds):\n","    q_name = wsp_tokenizer.tokenize(tf.gather(sp_names, q_ix))\n","    c_name = wsp_tokenizer.tokenize(tf.gather(sp_names, c_ix))\n","    name_score = tf.stack(tf_text.metrics.rouge_l(q_name, c_name), axis=-1)\n","    q_address = wsp_tokenizer.tokenize(tf.gather(sp_address, q_ix))\n","    c_address = wsp_tokenizer.tokenize(tf.gather(sp_address, c_ix))\n","    address_score = tf.stack(tf_text.metrics.rouge_l(q_address, c_address), axis=-1)\n","    score = tf.concat([name_score, address_score], axis=-1)\n","    scores.append(score)\n","\n","scores = tf.concat(scores, axis=0)\n","candidate_data_df[[\"name_f\", \"name_p\", \"name_r\", \"address_f\", \"address_p\", \"address_r\"]] = scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3IIRqA_2uys"},"outputs":[],"source":["del scores\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwnQnqY_2wjR"},"outputs":[],"source":["candidate_data_df[\"distance\"] = tf.reshape(tf.gather(cross_feats, 0, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"sparse_name_sim\"] = tf.reshape(tf.gather(cross_feats, 1, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"name_cossim\"] = tf.reshape(tf.gather(cross_feats, 2, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"address_cossim\"] = tf.reshape(tf.gather(cross_feats, 3, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"category_cossim\"] = tf.reshape(tf.gather(cross_feats, 4, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"mix_cossim\"] = tf.reshape(tf.gather(cross_feats, 5, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"logistic_score\"] = tf.reshape(tf.gather(cross_feats, 6, axis=1), [-1]).numpy().astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OXnNhybs2xva"},"outputs":[],"source":["category_tsp_cols = [\"city_pos_tsp_index\", \"state_pos_tsp_index\"]\n","kmeans_labels_cols = ['hkmeans_labels', 'name_kmeans_labels', 'address_kmeans_labels', 'ix_values_kmeans_labels', \"mix_kmeans_labels\", \"category_kmeans_labels\", 'brand_kmeans_labels']\n","kmeans_scores_cols = ['hkmeans_scores', 'name_kmeans_scores', 'address_kmeans_scores', 'ix_values_kmeans_scores', \"mix_kmeans_scores\", \"category_kmeans_scores\", 'brand_kmeans_scores']\n","\n","important_cols = [\"city_pos_tsp_index\",\"category_kmeans_labels\"]\n","\n","candidate_data_df[\"query_city_index\"] = test_df[\"pseudo_city_ix\"].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","candidate_data_df[\"query_geo_city_index\"] = test_df[\"pseudo_geo_city_ix\"].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","candidate_data_df[\"query_country_index\"] = test_df[\"country_ix\"].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","candidate_data_df[\"query_state_index\"] = test_df[\"pseudo_state_ix\"].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","\n","#candidate_data_df[\"query_city_emb_tsp_index\"] = city_index_df[\"city_emb_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"query_city_index\"]]\n","#candidate_data_df[\"query_state_emb_tsp_index\"] = state_index_df[\"state_emb_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"query_state_index\"]]\n","candidate_data_df[\"query_city_pos_tsp_index\"] = city_index_df[\"city_pos_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"query_city_index\"]]\n","candidate_data_df[\"query_geo_city_pos_tsp_index\"] = geo_city_index_df[\"geo_city_pos_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"query_geo_city_index\"]]\n","candidate_data_df[\"query_state_pos_tsp_index\"] = state_index_df[\"state_pos_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"query_state_index\"]]\n","\n","for col in kmeans_labels_cols:\n","    candidate_data_df[f\"query_{col}\"] = test_df[col].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","for col in kmeans_scores_cols:\n","    candidate_data_df[f\"query_{col}\"] = test_df[col].values.astype(np.float32)[candidate_data_df[\"query_ix\"]]\n","\n","candidate_data_df[\"candidate_city_index\"] = test_df[\"pseudo_city_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","candidate_data_df[\"candidate_geo_city_index\"] = test_df[\"pseudo_geo_city_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","\n","candidate_data_df[\"candidate_country_index\"] = test_df[\"country_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","candidate_data_df[\"candidate_state_index\"] = test_df[\"pseudo_state_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","\n","#andidate_data_df[\"candidate_city_emb_tsp_index\"] = city_index_df[\"city_emb_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_city_index\"]]\n","#candidate_data_df[\"candidate_state_emb_tsp_index\"] = state_index_df[\"state_emb_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_state_index\"]]\n","candidate_data_df[\"candidate_city_pos_tsp_index\"] = city_index_df[\"city_pos_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_city_index\"]]\n","candidate_data_df[\"candidate_geo_city_pos_tsp_index\"] = geo_city_index_df[\"geo_city_pos_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_geo_city_index\"]]\n","candidate_data_df[\"candidate_state_pos_tsp_index\"] = state_index_df[\"state_pos_tsp_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_state_index\"]]\n","\n","for col in kmeans_labels_cols:\n","    candidate_data_df[f\"candidate_{col}\"] = test_df[col].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","for col in kmeans_scores_cols:\n","    candidate_data_df[f\"candidate_{col}\"] = test_df[col].values.astype(np.float32)[candidate_data_df[\"candidate_ix\"]]\n","\n","\n","for col in kmeans_labels_cols + category_tsp_cols:\n","    candidate_data_df[f\"{col}_1d_dist\"] = np.abs(candidate_data_df[f\"query_{col}\"] - candidate_data_df[f\"candidate_{col}\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0RJ6AEJ20yj"},"outputs":[],"source":["candidate_data_df.to_csv('./output/test_df_intermed3.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PHjeuLfv3CuU"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"PRED21X SHARE.ipynb","provenance":[{"file_id":"1nXsk8Wc0Vib2--Yke2cqBCT2yTIfHoof","timestamp":1656509444733},{"file_id":"1IPq5vg96uFbHnbcJZiYzREVdkr6J__Gk","timestamp":1656217806725}],"version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"07ac0f78e4404a2c9c730f379e2b5e6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_650881958993442089a4191d8af83e1e","IPY_MODEL_f3ebf9402f3440f1bfceb6b709ca1f83","IPY_MODEL_33008cdf523f49fa936aa614583df6d4"],"layout":"IPY_MODEL_0e444c69d2114262ab9a3c3187b47992"}},"0e444c69d2114262ab9a3c3187b47992":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24d97194ccc9438b8c5011c35b8ede83":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ce8ea3653d341ea80f93791d70d7873":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33008cdf523f49fa936aa614583df6d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_835af163a72c4d9d9a1e0231b811a3dc","placeholder":"​","style":"IPY_MODEL_2ce8ea3653d341ea80f93791d70d7873","value":" 90478/90478 [18:22\u0026lt;00:00, 73.60it/s]"}},"38eb9a0bd27b460bb506fd5231554e3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"399aae6377164e1dac6d264badd71e9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56da661575fc4aa4841c88d6ad7a6640":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60f3abc9c24d4679ad778879a5ed7262":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"650881958993442089a4191d8af83e1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_399aae6377164e1dac6d264badd71e9c","placeholder":"​","style":"IPY_MODEL_732de4cac7d34254a2e2b57ad3e4bd7f","value":"100%"}},"6b5a9fe7f7364c8aa60f40ccda8dcb3f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e8adfa3914d43d3afee77c6ed787013":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b64f574a442240cfbc144282f5f45895","placeholder":"​","style":"IPY_MODEL_bbaabc3dbd184f878d0435d499a30780","value":"100%"}},"732de4cac7d34254a2e2b57ad3e4bd7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c05b3080d6d49198dd2d5a5d3855d85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b5a9fe7f7364c8aa60f40ccda8dcb3f","placeholder":"​","style":"IPY_MODEL_85659bfccb83403ab12619dc4577c9aa","value":" 150443/150443 [13:14\u0026lt;00:00, 239.97it/s]"}},"7c07be70fa4649fdad412afa99b6dbf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56da661575fc4aa4841c88d6ad7a6640","max":150443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38eb9a0bd27b460bb506fd5231554e3c","value":150443}},"835af163a72c4d9d9a1e0231b811a3dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85659bfccb83403ab12619dc4577c9aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bb0be75911444679bb8b58c850f9cdb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e8adfa3914d43d3afee77c6ed787013","IPY_MODEL_7c07be70fa4649fdad412afa99b6dbf0","IPY_MODEL_7c05b3080d6d49198dd2d5a5d3855d85"],"layout":"IPY_MODEL_60f3abc9c24d4679ad778879a5ed7262"}},"b64f574a442240cfbc144282f5f45895":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbaabc3dbd184f878d0435d499a30780":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f16423e4f15040d581c518315f3d1bb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3ebf9402f3440f1bfceb6b709ca1f83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24d97194ccc9438b8c5011c35b8ede83","max":90478,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f16423e4f15040d581c518315f3d1bb4","value":90478}}}}},"nbformat":4,"nbformat_minor":0}