{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:06:33.350237Z","start_time":"2022-07-01T03:06:33.340239Z"},"execution":{"iopub.execute_input":"2022-06-25T00:09:03.474793Z","iopub.status.busy":"2022-06-25T00:09:03.474223Z","iopub.status.idle":"2022-06-25T00:09:03.481057Z","shell.execute_reply":"2022-06-25T00:09:03.479509Z","shell.execute_reply.started":"2022-06-25T00:09:03.474733Z"},"id":"fSDxc2y-ybLN","executionInfo":{"status":"ok","timestamp":1657005465915,"user_tz":-540,"elapsed":6,"user":{"displayName":"クルトン","userId":"17231302919530674825"}}},"outputs":[],"source":["#nadare_data_dir = \"/kaggle/input/nadare-4sq-data\"\n","#nadare_feature_dir = \"/kaggle/input/nadare-4sq-data/FEAT20/\"\n","#nadare_util_dir = \"/kaggle/input/nadare-4sq-data/common_utils/\"\n","nadare_feature_dir = \"../feature/FEAT26/\"\n","nadare_util_dir = \"../feature/common_utils/\"\n"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2022-06-25T11:17:48.103193Z","start_time":"2022-06-25T11:17:48.059193Z"},"execution":{"iopub.execute_input":"2022-06-25T00:09:03.823919Z","iopub.status.busy":"2022-06-25T00:09:03.823498Z","iopub.status.idle":"2022-06-25T00:09:04.655763Z","shell.execute_reply":"2022-06-25T00:09:04.654317Z","shell.execute_reply.started":"2022-06-25T00:09:03.823885Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"wbqTMKpiybLQ","executionInfo":{"status":"ok","timestamp":1657005471303,"user_tz":-540,"elapsed":15,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"e00ba8b6-0064-4b9b-a66e-29ec0ed9120d"},"outputs":[{"output_type":"stream","name":"stdout","text":["unzip:  cannot find or open ../feature/common_utils/pykakasi_deps.dontopenthiskaggle/pykakasi_deps.dontopenthiskaggle, ../feature/common_utils/pykakasi_deps.dontopenthiskaggle/pykakasi_deps.dontopenthiskaggle.zip or ../feature/common_utils/pykakasi_deps.dontopenthiskaggle/pykakasi_deps.dontopenthiskaggle.ZIP.\n"]}],"source":["!unzip {nadare_util_dir}pykakasi_deps.dontopenthiskaggle/pykakasi_deps.dontopenthiskaggle -d ."]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"start_time":"2022-06-25T11:17:48.298Z"},"execution":{"iopub.execute_input":"2022-06-25T00:09:04.658091Z","iopub.status.busy":"2022-06-25T00:09:04.657714Z","iopub.status.idle":"2022-06-25T00:09:41.372085Z","shell.execute_reply":"2022-06-25T00:09:41.370799Z","shell.execute_reply.started":"2022-06-25T00:09:04.658058Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"m5WFuVfKybLR","executionInfo":{"status":"ok","timestamp":1657005473912,"user_tz":-540,"elapsed":637,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"a324712a-160d-4601-9b7a-8c4d21e7f259"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: conda: command not found\n","/bin/bash: conda: command not found\n","/bin/bash: conda: command not found\n"]}],"source":["!conda install ./pykakasi_deps/offline_pykakasi.tar.bz2\n","!conda install ./pykakasi_deps/offline_jaconv.tar.bz2\n","!conda install ./pykakasi_deps/offline_deprecated.tar.bz2"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-06-25T00:09:41.374402Z","iopub.status.busy":"2022-06-25T00:09:41.373755Z","iopub.status.idle":"2022-06-25T00:10:48.940816Z","shell.execute_reply":"2022-06-25T00:10:48.939707Z","shell.execute_reply.started":"2022-06-25T00:09:41.374359Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"z8BHElWeybLS","executionInfo":{"status":"ok","timestamp":1657005478378,"user_tz":-540,"elapsed":2608,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"06085cb2-3910-40e4-c10d-00d15c3564e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Requirement '../feature/common_utils/tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /feature/common_utils/tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/feature/common_utils/tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl'\n","\u001b[0m\n","\u001b[33mWARNING: Requirement '../feature/common_utils/tensorflow_ranking-0.5.0-py2.py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /feature/common_utils/tensorflow_ranking-0.5.0-py2.py3-none-any.whl\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/feature/common_utils/tensorflow_ranking-0.5.0-py2.py3-none-any.whl'\n","\u001b[0m\n","\u001b[33mWARNING: Requirement '../feature/common_utils/mojimoji-0.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /feature/common_utils/mojimoji-0.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/feature/common_utils/mojimoji-0.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'\n","\u001b[0m\n"]}],"source":["!pip install {nadare_util_dir}tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl --no-deps\n","!pip install {nadare_util_dir}tensorflow_ranking-0.5.0-py2.py3-none-any.whl --no-deps\n","!pip install {nadare_util_dir}mojimoji-0.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-06-25T00:10:48.945074Z","iopub.status.busy":"2022-06-25T00:10:48.944035Z","iopub.status.idle":"2022-06-25T00:12:50.293863Z","shell.execute_reply":"2022-06-25T00:12:50.291934Z","shell.execute_reply.started":"2022-06-25T00:10:48.945025Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Rlne2aU3ybLS","executionInfo":{"status":"ok","timestamp":1657005485931,"user_tz":-540,"elapsed":3807,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"7f156a39-ad65-4549-dceb-103991375e8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Requirement '../feature/common_utils/SudachiPy-0.6.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /feature/common_utils/SudachiPy-0.6.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/feature/common_utils/SudachiPy-0.6.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl'\n","\u001b[0m\n","\u001b[33mWARNING: Requirement '../feature/common_utils/SudachiDict_core-20220519-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /feature/common_utils/SudachiDict_core-20220519-py3-none-any.whl\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/feature/common_utils/SudachiDict_core-20220519-py3-none-any.whl'\n","\u001b[0m\n","\u001b[33mWARNING: Requirement '../feature/common_utils/SudachiDict_full-20220519-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /feature/common_utils/SudachiDict_full-20220519-py3-none-any.whl\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/feature/common_utils/SudachiDict_full-20220519-py3-none-any.whl'\n","\u001b[0m\n","\u001b[33mWARNING: Requirement '../feature/common_utils/pythainlp-3.0.8-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /feature/common_utils/pythainlp-3.0.8-py3-none-any.whl\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/feature/common_utils/pythainlp-3.0.8-py3-none-any.whl'\n","\u001b[0m\n","\u001b[33mWARNING: Requirement '../feature/common_utils/tinydb-4.7.0-py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /feature/common_utils/tinydb-4.7.0-py3-none-any.whl\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/feature/common_utils/tinydb-4.7.0-py3-none-any.whl'\n","\u001b[0m\n"]}],"source":["!pip install {nadare_util_dir}SudachiPy-0.6.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl --no-deps\n","!pip install {nadare_util_dir}SudachiDict_core-20220519-py3-none-any.whl --no-deps\n","!pip install {nadare_util_dir}SudachiDict_full-20220519-py3-none-any.whl --no-deps\n","!pip install {nadare_util_dir}pythainlp-3.0.8-py3-none-any.whl --no-deps\n","!pip install {nadare_util_dir}tinydb-4.7.0-py3-none-any.whl --no-deps"]},{"cell_type":"code","source":["!pip install tensorflow_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaSCLZqryuUR","executionInfo":{"status":"ok","timestamp":1656830636891,"user_tz":-540,"elapsed":4040,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"2559e24a-fbd6-4453-b0a9-cc845f2e8bf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.9.0)\n","Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n","Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.9.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.0)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (4.1.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.14.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.2)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.21.6)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (21.3)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.6.3)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.15.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.46.3)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (14.0.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (57.4.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.1.0)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.12)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.26.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.5.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.8.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2022.6.15)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.9)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:06:39.551187Z","start_time":"2022-07-01T03:06:37.727182Z"},"execution":{"iopub.execute_input":"2022-06-25T00:16:43.620665Z","iopub.status.busy":"2022-06-25T00:16:43.620249Z","iopub.status.idle":"2022-06-25T00:16:43.626774Z","shell.execute_reply":"2022-06-25T00:16:43.625677Z","shell.execute_reply.started":"2022-06-25T00:16:43.620631Z"},"id":"MJjOmqllybLS"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_text as tf_text\n","\n","from tqdm.notebook import tqdm\n","import re\n","\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:06:39.566760Z","start_time":"2022-07-01T03:06:39.552189Z"},"execution":{"iopub.execute_input":"2022-06-25T00:16:43.938096Z","iopub.status.busy":"2022-06-25T00:16:43.936956Z","iopub.status.idle":"2022-06-25T00:16:43.942856Z","shell.execute_reply":"2022-06-25T00:16:43.941780Z","shell.execute_reply.started":"2022-06-25T00:16:43.938053Z"},"id":"hBzSHTEiybLT"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:06:42.090759Z","start_time":"2022-07-01T03:06:39.567760Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:30.512040Z","iopub.status.busy":"2022-06-25T00:30:30.511592Z","iopub.status.idle":"2022-06-25T00:30:38.131942Z","shell.execute_reply":"2022-06-25T00:30:38.130897Z","shell.execute_reply.started":"2022-06-25T00:30:30.512003Z"},"papermill":{"duration":7.72778,"end_time":"2022-05-04T10:21:02.526592","exception":false,"start_time":"2022-05-04T10:20:54.798812","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":377},"id":"bmAojLp_ybLT","executionInfo":{"status":"error","timestamp":1656830640021,"user_tz":-540,"elapsed":485,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"8e4608ff-6891-4101-d574-5fe4df96eb46"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-9bb6e20d96b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train_df = pd.read_csv(\"../input/foursquare-location-matching/train.csv\", encoding=\"utf-8\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/foursquare-location-matching/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#test_df.loc[0, \"name\"] = np.NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/foursquare-location-matching/train.csv'"]}],"source":["#train_df = pd.read_csv(\"../input/foursquare-location-matching/train.csv\", encoding=\"utf-8\")\n","test_df = pd.read_csv(\"../input/foursquare-location-matching/train.csv\", encoding=\"utf-8\")\n","#test_df.loc[0, \"name\"] = np.NaN"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:06:42.681758Z","start_time":"2022-07-01T03:06:42.091760Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:38.134140Z","iopub.status.busy":"2022-06-25T00:30:38.133765Z","iopub.status.idle":"2022-06-25T00:30:38.152869Z","shell.execute_reply":"2022-06-25T00:30:38.151413Z","shell.execute_reply.started":"2022-06-25T00:30:38.134106Z"},"papermill":{"duration":2.936041,"end_time":"2022-05-04T10:21:31.818613","exception":false,"start_time":"2022-05-04T10:21:28.882572","status":"completed"},"tags":[],"id":"R2vPs53BybLU"},"outputs":[],"source":["test_id_map = {v: i for i, v in enumerate(test_df[\"id\"].values)}\n","test_df[\"ix\"] = test_df[\"id\"].map(test_id_map)\n","test_df[\"categories\"] = test_df[\"categories\"].fillna(\"\")\n","test_df[\"pid\"] = -1\n","test_df[\"group\"] = -1"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:07:00.861619Z","start_time":"2022-07-01T03:06:47.238621Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:38.155503Z","iopub.status.busy":"2022-06-25T00:30:38.154989Z","iopub.status.idle":"2022-06-25T00:30:38.589901Z","shell.execute_reply":"2022-06-25T00:30:38.588769Z","shell.execute_reply.started":"2022-06-25T00:30:38.155428Z"},"id":"2vH5XnubybLU"},"outputs":[],"source":["from fasttext import load_model\n","ft_model = load_model(nadare_util_dir + \"lid.176.bin\")\n","def predict_language(text):\n","    label, prob = ft_model.predict(text, 1)\n","    return list(zip([l.replace(\"__label__\", \"\") for l in label], prob))[0][0]\n","\n","test_df[\"name_language\"] = np.vectorize(predict_language)(test_df[\"name\"].fillna(\"\"))\n","test_df[\"address_language\"] = np.vectorize(predict_language)(test_df[\"address\"].fillna(\"\"))\n","\n","def language_pattern(country, language):\n","    if country == \"JP\":\n","        return 0\n","    elif country == \"TH\":\n","        return 1\n","    elif country == \"CN\":\n","        return 2\n","    elif language == \"ja\":\n","        return 0\n","    elif language == \"th\":\n","        return 1\n","    elif language == \"zh\":\n","        return 2\n","    else:\n","        return 3\n","test_df[\"name_handle_pattern\"] = np.vectorize(language_pattern)(test_df[\"country\"], test_df[\"name_language\"])     \n","test_df[\"address_handle_pattern\"] = np.vectorize(language_pattern)(test_df[\"country\"], test_df[\"address_language\"])\n","lang_vc = test_df[\"name_language\"].value_counts()\n","del ft_model"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:07:02.713049Z","start_time":"2022-07-01T03:07:00.862619Z"},"id":"3KZ30CTHybLU"},"outputs":[],"source":["from num2words import num2words\n","from collections import defaultdict\n","\n","num2words_languages = [\"ar\", \"cz\", \"en\", \"fr\", \"de\", \"fi\", \"es\", \"id\", \"kn\", \"ko\", \"kz\", \"lt\", \"lv\", \"pl\", \"ro\", \"ru\", \"sl\",\n","                       \"sr\", \"sv\", \"no\", \"dk\", \"pt\", \"he\", \"it\", \"vi\", \"th\", \"tr\", \"nl\", \"uk\", \"te\", \"hu\"]\n","\n","word_number_df = []\n","for lang in tqdm(num2words_languages):\n","    for i in range(1001):\n","        try:\n","            word = num2words(i, lang=lang)\n","        except:\n","            word = np.NaN\n","        try:\n","            ordinal = num2words(i, lang=lang, to=\"ordinal\")\n","        except:\n","            ordinal = np.NaN\n","        try:\n","            ordinal_num = num2words(i, lang=lang, to=\"ordinal_num\")\n","        except:\n","            ordinal_num = np.NaN\n","        if (pd.isna(ordinal)) or (pd.isna(ordinal_num)) or (ordinal == ordinal_num):\n","            ordinal_num = i\n","        if lang == \"ko\" and not pd.isna(ordinal):\n","            ordinal = ordinal[:-3]\n","            ordinal_num = ordinal_num[:-3] \n","        \n","        word_number_df.append({\"lang\": lang,\n","                               \"number\": i,\n","                               \"word\": word,\n","                               \"ordinal\": ordinal,\n","                               \"ordinal_num\": ordinal_num})\n","\n","word_number_df = pd.DataFrame(word_number_df)\n","word_number_df[\"lang_count\"] = word_number_df[\"lang\"].apply(lambda x: lang_vc.get(x, 0))\n","\n","word2num_dict = {}\n","possibly_have_next_dict = {}\n","\n","for i, row in word_number_df.sort_values(by=\"lang_count\").iterrows():\n","    if not pd.isna(row[\"word\"]):\n","        word = row[\"word\"]\n","        if not word in possibly_have_next_dict.keys():\n","            possibly_have_next_dict[word] = False\n","\n","        word2num_dict[word] = str(row[\"number\"])\n","        word2num_dict[word.replace(\"-\", \" \")] = str(row[\"number\"])\n","        \n","        wsp = word.split()\n","        for i in range(1, len(wsp)-1):\n","            possibly_have_next_dict[\" \".join(wsp[:i])] = True\n","        wsp = word.replace(\"-\", \" \").split()[:-1]\n","        for i in range(1, len(wsp)-1):\n","            possibly_have_next_dict[\" \".join(wsp[:i])] = True\n","        \n","    if (not pd.isna(row[\"ordinal\"])) and (not pd.isna(row[\"ordinal_num\"])):\n","        word = row[\"ordinal\"]\n","        if not word in possibly_have_next_dict.keys():\n","            possibly_have_next_dict[word] = False\n","        word2num_dict[word] = str(row[\"ordinal_num\"])\n","        word2num_dict[word.replace(\"-\", \" \")] = str(row[\"ordinal_num\"])\n","        \n","        wsp = word.split()\n","        for i in range(1, len(wsp)-1):\n","            possibly_have_next_dict[\" \".join(wsp[:i])] = True\n","        wsp = word.replace(\"-\", \" \").split()[:-1]\n","        for i in range(1, len(wsp)-1):\n","            possibly_have_next_dict[\" \".join(wsp[:i])] = True\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:07:05.871126Z","start_time":"2022-07-01T03:07:02.714379Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:38.594012Z","iopub.status.busy":"2022-06-25T00:30:38.593478Z","iopub.status.idle":"2022-06-25T00:30:39.868026Z","shell.execute_reply":"2022-06-25T00:30:39.866874Z","shell.execute_reply.started":"2022-06-25T00:30:38.593958Z"},"id":"ghCPCPMrybLV"},"outputs":[],"source":["import string\n","import pycnnum\n","import pythainlp\n","from pythainlp.util import thai_digit_to_arabic_digit\n","from sudachipy import Dictionary, SplitMode\n","import mojimoji\n","import pykakasi\n","from unidecode import unidecode\n","import emoji\n","\n","emoji_set = set()\n","for k, v in emoji.UNICODE_EMOJI.items():\n","    emoji_set.update(v.keys())\n","thai_word2num_dict = {num2words(i, lang=\"th\"):str(i) for i in range(10001)}\n","CHINESE_DIGITS_PATTERN = re.compile(r'[〇〇零一二三四五六七八九零壹贰叁肆伍陆柒捌玖十百千万拾佰仟萬]+')\n","remove_punctuations = '!\"#$%\\'()*+,./:;<=>?@[\\\\]^_`{|}~'\n","punctuation_translater = str.maketrans(remove_punctuations, \" \"*len(remove_punctuations))\n","japanese_char2num_translater = str.maketrans(\"〇零一壱二弐三参四肆五伍六陸七漆八捌九玖\", \"00112233445566778899\")\n","\n","\n","kakasi = pykakasi.kakasi()\n","kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\n","kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\n","kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\n","conversion = kakasi.getConverter()\n","\n","force_katakana = pykakasi.kakasi()\n","kakasi.setMode('H', 'K')  # Convert Hiragana into alphabet\n","kakasi.setMode('K', 'K')  # Convert Katakana into alphabet\n","force_katakana.setMode('J', 'K')  # Convert Hiragana into alphabet\n","force_katakana_conversion = force_katakana.getConverter()\n","\n","sudachi_tokenizer = Dictionary(dict_type=\"full\").create()\n","sudachi_tokenize_mode = SplitMode.A\n","\n","zh_segmenter = tf_text.HubModuleTokenizer(nadare_util_dir + \"zh_segmentation\")\n","\n","def remove_emoji(x):\n","    return ''.join([c for c in x if c not in emoji_set])\n","\n","def remove_marks(x):\n","    x = re.sub(r'[^a-zA-Z0-9& ]', ' ', x)\n","    x = re.sub(r' +', ' ', x)\n","    return x\n","\n","def zenhan_normalize(x):\n","    return mojimoji.han_to_zen(mojimoji.zen_to_han(x, kana=False), digit=False, ascii=False)\n","\n","def japanese_wakachi_reading(morphemes):\n","    readings = []\n","    for m in morphemes:\n","        if (m.normalized_form() == \" \") or (m.reading_form() == \"キゴウ\"):\n","            continue\n","        if m.normalized_form().endswith(\"丁目\") and m.normalized_form() != \"丁目\":\n","            norm = m.normalized_form().translate(japanese_char2num_translater)\n","            norm = re.sub(r\"(\\d+)\", r\" \\1 \", \"6丁目\")\n","            rs = japanese_wakachi_reading(sudachi_tokenizer.tokenize(norm, sudachi_tokenize_mode))\n","            readings.extend(rs)\n","        elif m.normalized_form().isdigit():\n","            readings.append(m.normalized_form())\n","        else:\n","            readings.append(force_katakana_conversion.do(m.reading_form()))\n","    return readings\n","\n","def handle_japanese(x):\n","    x = zenhan_normalize(x)\n","    morphemes = sudachi_tokenizer.tokenize(x, sudachi_tokenize_mode)\n","    reading_forms = japanese_wakachi_reading(morphemes)\n","    romanize_forms = conversion.do(\" \".join(reading_forms))\n","    return \" \".join(reading_forms), romanize_forms\n","\n","def decode_list(x):\n","    if type(x) is list:\n","        return list(map(decode_list, x))\n","    return x.decode(\"UTF-8\")\n","\n","def decode_utf8_tensor(x):\n","    return list(map(decode_list, x.to_list()))\n","\n","def handle_chinese(x):\n","    x = x.replace(\"'\", \"\").translate(punctuation_translater).lower()\n","    if re.fullmatch(\"\\s*\", x):\n","        return \"\", \"\"\n","    with tf.device(\"CPU:0\"):\n","        words_list = decode_utf8_tensor(zh_segmenter.tokenize(x.split()))\n","        \n","    number_words = []\n","    romanized_words = []\n","    for words in words_list:\n","        num_word = []\n","        roman_word = []\n","        for word in words:\n","            if re.fullmatch(CHINESE_DIGITS_PATTERN, word):\n","                word = str(pycnnum.cn2num(word))\n","            if (word[0]) == \"第\" and re.fullmatch(CHINESE_DIGITS_PATTERN, word[1:]):\n","                num = str(pycnnum.cn2num(word[1:]))\n","                word = \"第\"\n","                num_word.append(word + num)\n","                roman_word.append(unidecode(word[0]).lower().replace(\" \", \"\") + \" \" + num)\n","            else:\n","                num_word.append(word)\n","                roman_word.append(unidecode(word).lower().replace(\" \", \"\"))\n","        number_words.append(\"\".join(num_word))\n","        romanized_words.append(\" \".join(roman_word))\n","    return \" \".join(number_words), \" \".join(romanized_words)\n","\n","def handle_thai(x):\n","    x = thai_digit_to_arabic_digit(x)\n","    tokenized_word = pythainlp.tokenize.word_tokenize(x, engine=\"newmm\")\n","    number_words = []\n","    romanized_words = []\n","    for word in tokenized_word:\n","        word = thai_word2num_dict.get(word, word)\n","        number_words.append(word)\n","        try:\n","            romanized_words.append(pythainlp.romanize(word, engine='royin'))\n","        except:\n","            pass\n","    return \"\".join(number_words), \" \".join(romanized_words)\n","\n","\n","def language_normalize(word):\n","    result = []\n","    word = word.replace(\"'\", \"\").translate(punctuation_translater).lower()\n","    tmp = \"\"\n","    for w in word.split():\n","        if len(tmp):\n","            tmp = tmp + \" \" + w\n","            if tmp in possibly_have_next_dict.keys():\n","                if possibly_have_next_dict[tmp]:\n","                    continue\n","                else:\n","                    result.append(word2num_dict[tmp])\n","                    tmp = \"\"\n","        else:\n","            if w in possibly_have_next_dict.keys():\n","                if possibly_have_next_dict[w]:\n","                    tmp = w\n","                    continue\n","                else:\n","                    result.append(word2num_dict[w])\n","            else:\n","                result.append(w)\n","    if len(tmp):\n","        result.append(word2num_dict.get(tmp, tmp))\n","            \n","    return re.sub(r' +', ' ', \" \".join(result))"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:07:05.887087Z","start_time":"2022-07-01T03:07:05.872087Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:39.871106Z","iopub.status.busy":"2022-06-25T00:30:39.870140Z","iopub.status.idle":"2022-06-25T00:30:39.887233Z","shell.execute_reply":"2022-06-25T00:30:39.886105Z","shell.execute_reply.started":"2022-06-25T00:30:39.871054Z"},"id":"dFgpn8A5ybLW"},"outputs":[],"source":["def language_handle(handle_pattern, text):\n","    if handle_pattern == 0:\n","        normalized, romanized = handle_japanese(text)\n","    elif handle_pattern == 1:\n","        normalized, romanized = handle_thai(text)\n","    elif handle_pattern == 2:\n","        normalized, romanized = handle_chinese(text)\n","    return normalized, romanized\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:08:11.531293Z","start_time":"2022-07-01T03:07:05.888087Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:39.889218Z","iopub.status.busy":"2022-06-25T00:30:39.888702Z","iopub.status.idle":"2022-06-25T00:30:43.316464Z","shell.execute_reply":"2022-06-25T00:30:43.315242Z","shell.execute_reply.started":"2022-06-25T00:30:39.889174Z"},"id":"aCiUZZd1ybLW"},"outputs":[],"source":["name_handle_index = np.where((test_df[\"name_handle_pattern\"] <= 2) & (~test_df[\"name\"].isna()))[0]\n","\n","name_normalized = []\n","name_romanized = []\n","for pattern, name in tqdm(zip(test_df[\"name_handle_pattern\"].values[name_handle_index],\n","                              test_df[\"name\"].fillna(\"\").astype(str).values[name_handle_index]), total=len(name_handle_index)):\n","    normalized, romanized = language_handle(pattern, name)\n","    name_normalized.append(normalized)\n","    name_romanized.append(romanized)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:08:44.846292Z","start_time":"2022-07-01T03:08:11.532292Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:43.318590Z","iopub.status.busy":"2022-06-25T00:30:43.318060Z","iopub.status.idle":"2022-06-25T00:30:47.968535Z","shell.execute_reply":"2022-06-25T00:30:47.967496Z","shell.execute_reply.started":"2022-06-25T00:30:43.318541Z"},"id":"OakGcI-0ybLX"},"outputs":[],"source":["address_handle_index = np.where((test_df[\"address_handle_pattern\"] <= 2) & (~test_df[\"address\"].isna()))[0]\n","\n","address_normalized = []\n","address_romanized = []\n","for pattern, address in tqdm(zip(test_df[\"address_handle_pattern\"].values[address_handle_index],\n","                              test_df[\"address\"].fillna(\"\").astype(str).values[address_handle_index]), total=len(address_handle_index)):\n","    normalized, romanized = language_handle(pattern, address)\n","    address_normalized.append(normalized)\n","    address_romanized.append(romanized)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:08:45.325293Z","start_time":"2022-07-01T03:08:44.847294Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:47.971469Z","iopub.status.busy":"2022-06-25T00:30:47.970209Z","iopub.status.idle":"2022-06-25T00:30:49.794248Z","shell.execute_reply":"2022-06-25T00:30:49.793025Z","shell.execute_reply.started":"2022-06-25T00:30:47.971392Z"},"id":"DvhF-yRQybLX"},"outputs":[],"source":["import gc\n","del zh_segmenter, kakasi, conversion, sudachi_tokenizer\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:08:59.003654Z","start_time":"2022-07-01T03:08:45.326293Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:49.796554Z","iopub.status.busy":"2022-06-25T00:30:49.796012Z","iopub.status.idle":"2022-06-25T00:30:49.949864Z","shell.execute_reply":"2022-06-25T00:30:49.948850Z","shell.execute_reply.started":"2022-06-25T00:30:49.796505Z"},"id":"zGACiUyWybLX"},"outputs":[],"source":["test_df[\"name_normalized\"] = np.vectorize(language_normalize)(test_df[\"name\"].fillna(\"\").astype(str))\n","test_df[\"address_normalized\"] = np.vectorize(language_normalize)(test_df[\"address\"].fillna(\"\").astype(str))\n","\n","\n","from_cols = [\"name_normalized\", \"address_normalized\"]\n","to_cols = ['sp_name', \"sp_address_raw\"]\n","for from_, to_ in zip(from_cols, to_cols):\n","    test_df[to_] = test_df[from_].fillna('').apply(unidecode)\n","    test_df[to_] = test_df[to_].str.lower()\n","    test_df[to_] = test_df[to_].apply(remove_emoji)\n","    test_df[to_] = test_df[to_].apply(remove_marks)\n","    \n","test_df.loc[name_handle_index, \"name_normalized\"] = name_normalized\n","test_df.loc[name_handle_index, \"sp_name\"] = name_romanized\n","\n","test_df.loc[address_handle_index, \"address_normalized\"] = address_normalized\n","test_df.loc[address_handle_index, \"sp_address_raw\"] = address_romanized"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:10:36.467123Z","start_time":"2022-07-01T03:08:59.004657Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:49.953926Z","iopub.status.busy":"2022-06-25T00:30:49.953513Z","iopub.status.idle":"2022-06-25T00:30:50.534091Z","shell.execute_reply":"2022-06-25T00:30:50.533053Z","shell.execute_reply.started":"2022-06-25T00:30:49.953893Z"},"id":"hTaRm_gSybLX"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","address_index = np.where(~np.vectorize(lambda x: bool(re.fullmatch(\"\\s*\", x)))(test_df[\"sp_address_raw\"].fillna(\"\")))[0]\n","knn = KNeighborsRegressor(n_neighbors=min(3, (~test_df[\"address\"].isna()).sum()), \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(test_df[['latitude','longitude']].values)[address_index], address_index)\n","address_neiighbor = address_index[knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']].values))[1]]\n","test_df[\"sp_address\"] = np.vectorize(lambda x: \" \".join(x))(pd.Series(test_df[\"sp_address_raw\"].values[address_neiighbor].tolist()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:10:41.095121Z","start_time":"2022-07-01T03:10:36.469128Z"},"id":"T__xfkdSybLY"},"outputs":[],"source":["test_df[\"sp_name\"] = test_df[\"sp_name\"].apply(remove_marks)\n","test_df[\"sp_address\"] = test_df[\"sp_address\"].apply(remove_marks)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-06-30T15:08:12.288201Z","start_time":"2022-06-30T15:08:12.278202Z"},"id":"FXu0Tr6_ybLY"},"outputs":[],"source":["#test_df[[\"sp_name\", \"sp_address\", \"sp_address_raw\"]].to_csv(\"../processed/sp_input2.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-06-30T15:08:12.683710Z","start_time":"2022-06-30T15:08:12.677710Z"},"id":"roPyJzwKybLY"},"outputs":[],"source":["#test_df[[\"sp_name\", \"sp_address\", \"sp_address_raw\"]] = pd.read_csv(\"../processed/sp_input2.csv\").fillna(\" \")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:36.960663Z","start_time":"2022-07-01T03:26:36.941722Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:50.536191Z","iopub.status.busy":"2022-06-25T00:30:50.535612Z","iopub.status.idle":"2022-06-25T00:30:50.576560Z","shell.execute_reply":"2022-06-25T00:30:50.575613Z","shell.execute_reply.started":"2022-06-25T00:30:50.536139Z"},"id":"CazOc0vyybLY"},"outputs":[],"source":["import os\n","\n","country_index_df = pd.read_csv(nadare_feature_dir + 'country_tsp_index.csv', encoding=\"utf-8\")\n","state_index_df = pd.read_csv(nadare_feature_dir + 'state_tsp_index.csv', encoding=\"utf-8\")\n","city_index_df = pd.read_csv(nadare_feature_dir + 'city_tsp_index.csv', encoding=\"utf-8\")\n","geo_name_index_df = pd.read_csv(nadare_feature_dir + 'geo_name_tsp_index.csv', encoding=\"utf-8\")\n","category_index_df = pd.read_csv(nadare_feature_dir + 'category_tsp_index.csv', encoding=\"utf-8\").fillna(\"nan\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:38.734284Z","start_time":"2022-07-01T03:26:38.720285Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:50.582760Z","iopub.status.busy":"2022-06-25T00:30:50.581889Z","iopub.status.idle":"2022-06-25T00:30:50.624620Z","shell.execute_reply":"2022-06-25T00:30:50.623363Z","shell.execute_reply.started":"2022-06-25T00:30:50.582710Z"},"id":"JEWr108GybLY"},"outputs":[],"source":["country_index_map = {country: i for country, i in country_index_df[[\"country\", \"country_index\"]].values}\n","state_index_map = {state: i for state, i in state_index_df[[\"state\", \"state_index\"]].values}\n","city_index_map = {city: i for city, i in city_index_df[[\"city\", \"city_index\"]].values}\n","geo_name_index_map = {geo_name: i for geo_name, i in geo_name_index_df[[\"geo_name\", \"geo_name_index\"]].values}"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:39.242389Z","start_time":"2022-07-01T03:26:39.237389Z"},"id":"ocK870dVybLY"},"outputs":[],"source":["def haversine(X, Y):\n","    delta = Y - X\n","    x_lats = tf.gather(X, 0, axis=-1)\n","    y_lats = tf.gather(Y, 0, axis=-1)\n","    dlat = tf.gather(delta, 0, axis=-1)\n","    dlon = tf.gather(delta, 1, axis=-1)\n","\n","    a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","    c = 2 * tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a))\n","    return c"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:39.701389Z","start_time":"2022-07-01T03:26:39.698390Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:50.626723Z","iopub.status.busy":"2022-06-25T00:30:50.626204Z","iopub.status.idle":"2022-06-25T00:30:50.638167Z","shell.execute_reply":"2022-06-25T00:30:50.636953Z","shell.execute_reply.started":"2022-06-25T00:30:50.626675Z"},"id":"_iu_THsIybLZ"},"outputs":[],"source":["category_ix_dict = {k: v for k, v in category_index_df[[\"category\", \"category_ix\"]].values}"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:40.130389Z","start_time":"2022-07-01T03:26:40.128389Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:50.641339Z","iopub.status.busy":"2022-06-25T00:30:50.640083Z","iopub.status.idle":"2022-06-25T00:30:50.649951Z","shell.execute_reply":"2022-06-25T00:30:50.648854Z","shell.execute_reply.started":"2022-06-25T00:30:50.641286Z"},"id":"4CuSZZQhybLZ"},"outputs":[],"source":["len(category_ix_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:41.573486Z","start_time":"2022-07-01T03:26:40.645132Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:50.652360Z","iopub.status.busy":"2022-06-25T00:30:50.651576Z","iopub.status.idle":"2022-06-25T00:30:50.699415Z","shell.execute_reply":"2022-06-25T00:30:50.698300Z","shell.execute_reply.started":"2022-06-25T00:30:50.652311Z"},"id":"924xYpS2ybLZ"},"outputs":[],"source":["def get_value_with_default(dict_, value, default=0):\n","    return np.vectorize(lambda x: dict_.get(x, default))(value)\n","\n","def get_category_ix(df, category_ix_dict):\n","    cat_pairs = []\n","    all_category = set()\n","    for row in df[\"categories\"].drop_duplicates().dropna().values:\n","        for cat in row.split(\", \"):\n","            if len(cat):\n","                cat_pairs.append([row, cat])\n","                all_category.add(cat)\n","\n","    category_df = pd.DataFrame(cat_pairs, columns=[\"categories\", \"category\"])\n","    categories_df = df[[\"ix\", \"categories\"]].merge(category_df, on=\"categories\", how=\"left\").sort_values(by=\"ix\")\n","\n","    categories_df[\"category_ix\"] = get_value_with_default(category_ix_dict, categories_df[\"category\"].values, -1)\n","    categories_df = categories_df[categories_df[\"category_ix\"] >= 0]\n","    categories_ix = tf.RaggedTensor.from_value_rowids(values=tf.constant(categories_df[\"category_ix\"].values, \"int32\"),\n","                                                              value_rowids=tf.constant(categories_df[\"ix\"].values, \"int32\"),\n","                                                              nrows=len(df))\n","    return categories_ix\n","category_ix_dict = {k: v for k, v in category_index_df[[\"category\", \"category_ix\"]].values}\n","test_categories_ix = get_category_ix(test_df, category_ix_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:47.343159Z","start_time":"2022-07-01T03:26:42.292162Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:51.648652Z","iopub.status.busy":"2022-06-25T00:30:51.648177Z","iopub.status.idle":"2022-06-25T00:30:51.886907Z","shell.execute_reply":"2022-06-25T00:30:51.885797Z","shell.execute_reply.started":"2022-06-25T00:30:51.648620Z"},"id":"Dqr7zuorybLZ"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(city_index_df[['latitude','longitude']]), city_index_df[\"city_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']]))[1].T[0]\n","test_df[\"pseudo_city\"] = np.where(test_df[\"city\"].isin(city_index_df[\"city\"].values), test_df[\"city\"].values, city_index_df[\"city\"][pseudo_index].values)\n","test_df[\"pseudo_city_ix\"] = test_df[\"pseudo_city\"].map(city_index_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:54.387160Z","start_time":"2022-07-01T03:26:47.344160Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:51.889214Z","iopub.status.busy":"2022-06-25T00:30:51.888488Z","iopub.status.idle":"2022-06-25T00:30:52.018291Z","shell.execute_reply":"2022-06-25T00:30:52.017214Z","shell.execute_reply.started":"2022-06-25T00:30:51.889176Z"},"id":"Oth8svvXybLZ"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(city_index_df[['latitude','longitude']]), city_index_df[\"city_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']]))[1].T[0]\n","test_df[\"pseudo_city\"] = np.where(test_df[\"city\"].isin(city_index_df[\"city\"].values), test_df[\"city\"].values, city_index_df[\"city\"][pseudo_index].values)\n","test_df[\"pseudo_city_ix\"] = test_df[\"pseudo_city\"].map(city_index_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:27:01.780159Z","start_time":"2022-07-01T03:26:54.388160Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:52.019953Z","iopub.status.busy":"2022-06-25T00:30:52.019594Z","iopub.status.idle":"2022-06-25T00:30:52.150313Z","shell.execute_reply":"2022-06-25T00:30:52.149349Z","shell.execute_reply.started":"2022-06-25T00:30:52.019920Z"},"id":"0ikbFUjPybLZ"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(state_index_df[['latitude','longitude']]), state_index_df[\"state_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']]))[1].T[0]\n","test_df[\"pseudo_state\"] = np.where(test_df[\"state\"].isin(state_index_df[\"state\"].values), test_df[\"state\"].values, state_index_df[\"state\"][pseudo_index].values)\n","test_df[\"pseudo_state_ix\"] = test_df[\"pseudo_state\"].map(state_index_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:29:07.491864Z","start_time":"2022-07-01T03:29:07.472866Z"},"id":"L3CXST0oybLZ"},"outputs":[],"source":["geo_name_index_df"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:29:21.957465Z","start_time":"2022-07-01T03:29:14.339469Z"},"id":"FDqgcGsOybLZ"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(geo_name_index_df[['latitude','longitude']]), geo_name_index_df[\"geo_name_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(test_df[['latitude','longitude']]))[1].T[0]\n","test_df[\"pseudo_geo_name_ix\"] = pseudo_index"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:29:26.198806Z","start_time":"2022-07-01T03:29:26.090808Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:52.151750Z","iopub.status.busy":"2022-06-25T00:30:52.151403Z","iopub.status.idle":"2022-06-25T00:30:52.160345Z","shell.execute_reply":"2022-06-25T00:30:52.159384Z","shell.execute_reply.started":"2022-06-25T00:30:52.151719Z"},"id":"PQj2ay9BybLa"},"outputs":[],"source":["test_df[\"country_ix\"] = get_value_with_default(country_index_map, test_df[\"country\"], np.where(country_index_df[\"country\"] == \"NAN\")[0][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:29:29.718087Z","start_time":"2022-07-01T03:29:29.707087Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:53.798737Z","iopub.status.busy":"2022-06-25T00:30:53.798301Z","iopub.status.idle":"2022-06-25T00:30:53.816125Z","shell.execute_reply":"2022-06-25T00:30:53.815291Z","shell.execute_reply.started":"2022-06-25T00:30:53.798701Z"},"id":"b9e6Ftp4ybLa"},"outputs":[],"source":["ix_params_dict = {\"country\": {\"dimention\": 64, \"num_category\": len(country_index_df)+1},\n","                  \"state\": {\"dimention\": 64, \"num_category\": len(state_index_df)},\n","                  \"city\": {\"dimention\": 64, \"num_category\": len(city_index_df)},\n","                  \"geo_name\": {\"dimention\": 64, \"num_category\": len(geo_name_index_df)}\n","                  }\n","test_ix_values = {\"country\": tf.convert_to_tensor(test_df[\"country_ix\"].values.astype(np.int32)),\n","                   \"state\": tf.convert_to_tensor(test_df[\"pseudo_state_ix\"].values.astype(np.int32)),\n","                   \"city\": tf.convert_to_tensor(test_df[\"pseudo_city_ix\"].values.astype(np.int32)),\n","                   \"geo_name\": tf.convert_to_tensor(test_df[\"pseudo_geo_name_ix\"].values.astype(np.int32)),\n","                   }\n","\n","class IxEmbeddingLayer(tf.keras.layers.Layer):\n","    def __init__(self, ix_params_dict, num_layer=1, out_dim=128):\n","        super(IxEmbeddingLayer, self).__init__()\n","        self.ix_params_dict = ix_params_dict\n","        self.num_layer = num_layer\n","        self.out_dim = out_dim\n","        self.embedding_layers = {k: tf.keras.layers.Embedding(v[\"num_category\"], v[\"dimention\"]) for k, v in ix_params_dict.items()}\n","        self.denses = [tf.keras.layers.Dense(out_dim, activation=\"gelu\")]\n","        self.out = tf.keras.layers.Dense(out_dim)\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","    \n","    def call(self, ix_values):\n","        results = []\n","        for k, v in ix_values.items():\n","            results.append(self.embedding_layers[k](v))\n","        X = tf.concat(results, axis=-1)\n","        \n","        for i in range(self.num_layer):\n","            X = self.drop_out(self.denses[i](X))\n","        return self.out(X)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:29:33.111848Z","start_time":"2022-07-01T03:29:33.099881Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:53.818204Z","iopub.status.busy":"2022-06-25T00:30:53.817257Z","iopub.status.idle":"2022-06-25T00:30:53.829691Z","shell.execute_reply":"2022-06-25T00:30:53.828661Z","shell.execute_reply.started":"2022-06-25T00:30:53.818166Z"},"id":"5Pp4mYVuybLa"},"outputs":[],"source":["def approx_ranks(logits):\n","    r\"\"\"Computes approximate ranks given a list of logits.\n","    Given a list of logits, the rank of an item in the list is one plus the total\n","    number of items with a larger logit. In other words,\n","    rank_i = 1 + \\sum_{j \\neq i} I_{s_j > s_i},\n","    where \"I\" is the indicator function. The indicator function can be\n","    approximated by a generalized sigmoid:\n","    I_{s_j < s_i} \\approx 1/(1 + exp(-(s_j - s_i)/temperature)).\n","    This function approximates the rank of an item using this sigmoid\n","    approximation to the indicator function. This technique is at the core\n","    of \"A general approximation framework for direct optimization of\n","    information retrieval measures\" by Qin et al.\n","    Args:\n","    logits: A `Tensor` with shape [batch_size, list_size]. Each value is the\n","      ranking score of the corresponding item.\n","    Returns:\n","    A `Tensor` of ranks with the same shape as logits.\n","    \"\"\"\n","    list_size = tf.shape(input=logits)[1]\n","    x = tf.tile(tf.expand_dims(logits, 2), [1, 1, list_size])\n","    y = tf.tile(tf.expand_dims(logits, 1), [1, list_size, 1])\n","    pairs = tf.sigmoid(y - x)\n","    return tf.reduce_sum(input_tensor=pairs, axis=-1) + .5\n","    \n","    \n","class RankMSE(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(RankMSE, self).__init__()\n","    \n","    def call(self, y_true, y_pred):        \n","        weight = 1 / y_true\n","        rmse = tf.reduce_sum(tf.math.sqrt(tf.math.square(y_true - y_pred))*weight, axis=-1) / tf.reduce_sum(weight, axis=1)\n","        \n","        return tf.reduce_mean(rmse)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:29:39.135226Z","start_time":"2022-07-01T03:29:39.121227Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:53.831849Z","iopub.status.busy":"2022-06-25T00:30:53.831209Z","iopub.status.idle":"2022-06-25T00:30:53.862997Z","shell.execute_reply":"2022-06-25T00:30:53.862090Z","shell.execute_reply.started":"2022-06-25T00:30:53.831812Z"},"id":"BkPZlzTeybLa"},"outputs":[],"source":["class MixerModel(tf.keras.Model):\n","    \n","    def __init__(self, name_model, address_model, ix_emb_layer, cat_emb_layer, num_layer=2, num_dim=128):\n","        super(MixerModel, self).__init__()\n","        self.num_layer = num_layer\n","        self.num_dim = num_dim\n","        self.name_model = name_model\n","        self.address_model = address_model\n","        self.ix_emb_layer = ix_emb_layer\n","        self.cat_emb_layer = cat_emb_layer\n","        \n","        self.tokenize_norm = [tf.keras.layers.LayerNormalization() for i in range(4)]\n","    \n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        \n","        self.filter_denses = [tf.keras.layers.DepthwiseConv2D(kernel_size=(1, 4),\n","                                                              strides=1,\n","                                                              padding=\"valid\",\n","                                                              depth_multiplier=4,\n","                                                              activation=\"gelu\") for _ in range(self.num_layer)]\n","        self.out_denses = [tf.keras.layers.Dense(num_dim) for _ in range(self.num_layer)]\n","        \n","        #self.pointwise_filter_denses = [tf.keras.layers.Dense(num_dim, activation=\"gelu\") for _ in range(self.num_layer)]\n","        #self.pointwise_out_denses = [tf.keras.layers.Dense(num_dim) for _ in range(self.num_layer)]\n","        self.norm_layers = [tf.keras.layers.LayerNormalization() for _ in range(self.num_layer)]\n","        \n","    def check_tensor(self, x, ix):\n","        if isinstance(x, tf.RaggedTensor):\n","            x = x.to_tensor()\n","        return x\n","        \n","    def call(self, info, training=False):\n","        ix, name, address, position, ix_values, categories, pids, size = info\n","        X_name = self.tokenize_norm[0](tf.reduce_sum(self.name_model(name), axis=-2))\n","        X_address = self.tokenize_norm[1](tf.reduce_sum(self.address_model(address), axis=-2))\n","        X_ix = self.tokenize_norm[2](self.drop_out(self.ix_emb_layer(ix_values)))\n","        X_categories = self.tokenize_norm[3](tf.reduce_sum(self.drop_out(self.cat_emb_layer(categories)), axis=-2))\n","        \n","        X = tf.stack([X_name, X_address, X_ix, X_categories], axis=-2)\n","        original_shape = tf.shape(X)\n","        X = tf.expand_dims(X, axis=1)\n","        expand_shape = tf.shape(X)\n","        for i in range(self.num_layer):\n","            X_ = self.norm_layers[i](X)\n","            X_ = self.out_denses[i](tf.reshape(self.filter_denses[i](X_), expand_shape))\n","            X = X + X_\n","        X = tf.reshape(X, original_shape)\n","        return tf.reduce_mean(X, axis=-2)\n","\n","def simcse_task(mixer_model, loss_func, neighbor_info):\n","    X_a = tf.nn.l2_normalize(mixer_model(neighbor_info, training=True), axis=-1)\n","    X_b = tf.nn.l2_normalize(mixer_model(neighbor_info, training=True), axis=-1)\n","    \n","    X_p = tf.reshape(neighbor_info[-2], [-1, neighbor_info[-1]])\n","    \n","    N = tf.shape(X_p)[0]\n","    R = tf.shape(X_p)[1]\n","    \n","    shape = tf.shape(X_a)\n","    X_a = tf.reshape(X_a, [-1, neighbor_info[-1], shape[-1]])\n","    X_b = tf.reshape(X_b, [-1, neighbor_info[-1], shape[-1]])\n","    shape = tf.shape(X_a)\n","    \n","    cossim_c = tf.reshape(tf.einsum(\"nad,nbd->nab\", X_a, X_b), [shape[0]*shape[1], shape[1]])\n","    cossim_r = tf.reshape(tf.einsum(\"nad,nbd->nab\", tf.transpose(X_a, [1, 0, 2]), tf.transpose(X_b, [1, 0, 2])), [shape[0]*shape[1], shape[0]])\n"," \n","    mask_c = tf.reshape(tf.expand_dims(X_p, axis=1) != tf.expand_dims(X_p, axis=2), [shape[0]*shape[1], shape[1]])\n","    mask_r = tf.reshape(tf.expand_dims(X_p, axis=0) != tf.expand_dims(X_p, axis=1), [shape[0]*shape[1], shape[0]])    \n","\n","    label_c = tf.reshape(tf.repeat(tf.expand_dims(tf.eye(tf.shape(X_a)[1]), axis=0), tf.shape(X_a)[0], axis=0), tf.shape(cossim_c))\n","    label_r = tf.reshape(tf.repeat(tf.expand_dims(tf.eye(tf.shape(X_a)[0]), axis=1), tf.shape(X_a)[1], axis=0), tf.shape(cossim_r))\n","    \n","    mask_c = tf.math.logical_or(label_c == 1., mask_c)\n","    mask_r = tf.math.logical_or(label_r == 1., mask_r)\n","    \n","    cossim_c = tf.where(mask_c, cossim_c, -1.)\n","    cossim_r = tf.where(mask_r, cossim_r, -1.)\n","    \n","    scale_c = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label_c)[1], \"float32\") - 1.)\n","    scale_r = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label_r)[1], \"float32\") - 1.)\n","    loss = loss_func(label_c, cossim_c * scale_c) + loss_func(label_r, cossim_r * scale_r)\n","    return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:29:53.665095Z","start_time":"2022-07-01T03:29:53.649133Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:53.894667Z","iopub.status.busy":"2022-06-25T00:30:53.893813Z","iopub.status.idle":"2022-06-25T00:30:53.911881Z","shell.execute_reply":"2022-06-25T00:30:53.911038Z","shell.execute_reply.started":"2022-06-25T00:30:53.894627Z"},"id":"Ntb8x0WkybLa"},"outputs":[],"source":["class CategoryEmbeddingLayer(tf.keras.layers.Layer):\n","    \n","    def __init__(self, num_categories, dim_categories, init_embedding=None):\n","        super(CategoryEmbeddingLayer, self).__init__()\n","        self.num_categories = num_categories\n","        self.dim_categories = dim_categories\n","        self.category_dense = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=(num_categories, dim_categories)), trainable=True, name=self.name + \"/category_embedding\")\n","        if not init_embedding is None:\n","            self.category_dense.assign(init_embedding)\n","            \n","    def call(self, C):\n","        X = tf.gather(self.category_dense, C)\n","        return X"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:30:11.989223Z","start_time":"2022-07-01T03:30:11.970223Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:53.917011Z","iopub.status.busy":"2022-06-25T00:30:53.915933Z","iopub.status.idle":"2022-06-25T00:30:53.974002Z","shell.execute_reply":"2022-06-25T00:30:53.973023Z","shell.execute_reply.started":"2022-06-25T00:30:53.916953Z"},"id":"ye5mHBtyybLa"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow_text import SentencepieceTokenizer\n","\n","class SupervisedContrastiveLoss(tf.keras.layers.Layer):\n","    def __init__(self, from_logits=False):\n","        super(SupervisedContrastiveLoss, self).__init__()\n","        if not from_logits:\n","            NotImplementedError\n","        \n","    @tf.function\n","    def call(self, true, pred, sample_weight=None):\n","        if sample_weight is None:\n","            sample_weight = tf.ones(tf.shape(pred), dtype=tf.float32)\n","        epred = tf.exp(pred) * sample_weight\n","        scale = tf.reduce_sum(epred, axis=1, keepdims=True)\n","        loss = tf.reduce_sum((tf.math.log(scale - epred * true) - true*pred) * true * sample_weight) / tf.reduce_sum(true*sample_weight)\n","        return loss\n","    \n","class SentencePieceEmbeddingLayer(tf.keras.layers.Layer):\n","    \n","    def __init__(self, vocab_size, out_dim, sp_model_path, init_embedding=None):\n","        super(SentencePieceEmbeddingLayer, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.out_dim = out_dim\n","        model = open(sp_model_path, \"rb\").read()\n","        self.tokenizer = SentencepieceTokenizer(model)\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, out_dim)\n","        if not init_embedding is None:\n","            self.embedding(0)\n","            self.embedding.trainable_variables[0].assign(init_embedding)\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","    \n","    def call(self, X):\n","        token = self.tokenizer.tokenize(X)\n","        X = self.drop_out(self.embedding(token))\n","        return X\n","    \n","class DpConvWrapper(tf.keras.layers.Layer):\n","    \n","    def __init__(self, sp_layer, num_layer=2):\n","        super(DpConvWrapper, self).__init__()\n","        self.sp_layer = sp_layer\n","        self.num_layer = num_layer\n","        self.out_dim = self.sp_layer.out_dim\n","        self.norms = [tf.keras.layers.LayerNormalization() for i in range(num_layer)]\n","        self.dep_convs = [tf.keras.layers.DepthwiseConv1D(kernel_size=3, strides=1, padding=\"same\", activation=\"gelu\") for _ in range(num_layer)]\n","        self.denses = [tf.keras.layers.Dense(sp_layer.out_dim) for _ in range(num_layer)]\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        \n","        \n","    def call(self, X):\n","        token = name_dpc.sp_layer.tokenizer.tokenize(X) + 1\n","        start = tf.cast(tf.concat([tf.zeros([1], dtype=\"int64\"), tf.cumsum(token.row_lengths())], axis=0), \"int32\")\n","        row_ids = tf.cast(token.value_rowids(), \"int32\")\n","        col_ids = tf.range(tf.shape(token.value_rowids())[0], dtype=\"int32\") - tf.gather(start, token.value_rowids())\n","        X = self.drop_out(name_dpc.sp_layer.embedding(token.to_tensor(default_value=2)))\n","        rank = tf.rank(X)\n","        shape = tf.shape(X)\n","        \n","        for i in range(name_dpc.num_layer):\n","            X_ = name_dpc.norms[i](X)\n","            X_ = name_dpc.dep_convs[i](X_)\n","            X_ = name_dpc.denses[i](X_)\n","            X += X_\n","            \n","        X = tf.RaggedTensor.from_value_rowids(tf.gather_nd(X, tf.stack([row_ids, col_ids], axis=1)),\n","                                              token.value_rowids(),\n","                                              nrows=tf.cast(shape[0], \"int64\"))\n","        return X        \n","\n","    \n","class SkipgramModel(tf.keras.Model):\n","    \n","    def __init__(self, name_spe):\n","        super(SkipgramModel, self).__init__()\n","        self.name_spe = name_spe\n","        \n","    #@tf.function(experimental_relax_shapes=True)\n","    def call(self, name, n_name, n_pid):\n","        X = skipgram_model.name_spe(name)\n","        X_sum_neighbor = tf.reduce_sum(skipgram_model.name_spe(tf.slice(n_name, [0, 1], [-1, -1])), axis=2).to_tensor()\n","        name_skip_loss = skipgram_model.skipgram_task(X, X_sum_neighbor, n_pid)\n","\n","        return name_skip_loss\n","    \n","    #@tf.function(experimental_relax_shapes=True)\n","    def skipgram_task(self, X, X_sum_neighbor, n_pid):\n","\n","        my_pid, other_pid = tf.split(n_pid, [1, tf.shape(n_pid)[1]-1], axis=1)\n","        \n","        X_sum = tf.reduce_sum(X, axis=1)\n","        X_sum_other = tf.gather(X_sum, X.value_rowids()) - X.values\n","\n","        \n","        X_sum_norm = tf.nn.l2_normalize(X_sum, axis=1)\n","        X_sum_other_norm = tf.nn.l2_normalize(X_sum_other, axis=1)\n","        X_sum_neighbor_norm = tf.slice(tf.gather(tf.nn.l2_normalize(X_sum_neighbor, axis=-1), X.value_rowids()), [0, 1, 0], [-1, -1, -1])\n","        X_value_norm = tf.nn.l2_normalize(X.values, axis=1)\n","        \n","\n","        correct_cossim = tf.expand_dims(tf.einsum(\"Vd,Vd->V\", X_value_norm, X_sum_other_norm), axis=1)\n","        wrong_cossim = tf.einsum(\"Vd,Nd->VN\", X_value_norm, X_sum_norm)\n","        neighbor_cossim = tf.einsum(\"Vd,VNd->VN\", X_value_norm, X_sum_neighbor_norm)\n","        cossim = tf.concat([correct_cossim, wrong_cossim, neighbor_cossim], axis=1)\n","\n","        rowwise_mask = tf.expand_dims(tf.gather(X.row_lengths(), X.value_rowids()) > 1, axis=1)\n","        sameid_mask = tf.concat([tf.expand_dims(tf.cast(X.value_rowids(), \"int32\"), axis=1) != tf.expand_dims(tf.range(tf.shape(X.row_lengths())[0] + 1)-1, axis=0),\n","                                 tf.ones(tf.gather(tf.shape(X_sum_neighbor_norm), [0, 1], axis=0), dtype=\"bool\")], axis=1)\n","        same_pid_mask = tf.concat([tf.gather(my_pid, X.value_rowids(), axis=0) != tf.linalg.matrix_transpose(my_pid),\n","                                   tf.gather(my_pid, X.value_rowids()) != tf.gather(other_pid, X.value_rowids())], axis=1)\n","        \n","        mask = tf.math.logical_and(tf.math.logical_and(rowwise_mask, sameid_mask), same_pid_mask)\n","        label = tf.concat([tf.ones(tf.shape(correct_cossim)), tf.zeros(tf.shape(wrong_cossim)), tf.zeros(tf.shape(neighbor_cossim))], axis=1)\n","        \n","        # Fixed AdaCos paramet2r\n","        scale = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label)[1], \"float32\") - 1.)\n","\n","        label = tf.where(mask, label, tf.zeros(tf.shape(mask)))\n","        pred = tf.where(mask, scale * cossim, scale * -1.)\n","        loss = tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(label, pred, from_logits=True))\n","        return loss   \n","    \n","class ClassifyTrainModel(tf.keras.Model):\n","    \n","    def __init__(self, name_model, address_model, ix_emb_layer, cat_emb_layer):\n","        super(ClassifyTrainModel, self).__init__()\n","        self.name_model = name_model\n","        self.address_model = address_model\n","        self.ix_emb_layer = ix_emb_layer\n","        self.cat_emb_layer = cat_emb_layer\n","        self.dense = tf.keras.layers.Dense(cat_emb_layer.dim_categories, use_bias=True)\n","        self.scale = tf.sqrt(2.) * tf.math.log(tf.cast(cat_emb_layer.num_categories, \"float32\") - 1.)\n","        self.loss_function = SupervisedContrastiveLoss(from_logits=True)\n","        \n","        self.layer_norms = [tf.keras.layers.LayerNormalization() for i in range(3)]\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        self.nan_mask = tf.concat([tf.zeros([1, 1]), tf.ones([1, cat_emb_layer.num_categories-1])], axis=1)\n","        \n","    def transform(self, name, address, ix_values):\n","        X_name = self.layer_norms[0](tf.reduce_sum(self.drop_out(self.name_model(name)), axis=1))\n","        X_address = self.layer_norms[1](tf.reduce_sum(self.drop_out(self.address_model(address)), axis=1))\n","        X_ix = self.layer_norms[2](self.ix_emb_layer(ix_values))\n","        X = self.dense(tf.concat([X_name, X_address, X_ix], axis=-1))\n","        return X\n","        \n","    def call(self, name, address, ix_values, categories):\n","        X = self.transform(name, address, ix_values)\n","        classify_loss = self.classify_task(X, categories)\n","        return classify_loss\n","    \n","    def classify_task(self, X, C):\n","        cossim = tf.einsum(\"ND,CD->NC\", tf.nn.l2_normalize(X, axis=1), tf.nn.l2_normalize(self.cat_emb_layer.category_dense, axis=1))\n","        \n","        true = tf.scatter_nd(tf.stack([C.value_rowids(), C.values], axis=1),\n","                              tf.ones(tf.shape(C.values)),\n","                              [tf.shape(X)[0], self.cat_emb_layer.num_categories])\n","        true = true * self.nan_mask\n","        calc_loc = tf.where(tf.reduce_sum(true, axis=1) > 0.)\n","        pred = self.scale * cossim\n","        loss = self.loss_function(tf.gather_nd(true, calc_loc), tf.gather_nd(pred, calc_loc))\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:30:18.732701Z","start_time":"2022-07-01T03:30:18.703701Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:53.976468Z","iopub.status.busy":"2022-06-25T00:30:53.975613Z","iopub.status.idle":"2022-06-25T00:30:54.023554Z","shell.execute_reply":"2022-06-25T00:30:54.022783Z","shell.execute_reply.started":"2022-06-25T00:30:53.976392Z"},"id":"nWE2pOusybLb"},"outputs":[],"source":["def haversine(X, Y):\n","    delta = Y - X\n","    x_lats = tf.gather(X, 0, axis=-1)\n","    y_lats = tf.gather(Y, 0, axis=-1)\n","    dlat = tf.gather(delta, 0, axis=-1)\n","    dlon = tf.gather(delta, 1, axis=-1)\n","\n","    a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","    c = 2 * tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a))\n","    return c\n","\n","\n","class TFMiniBatchHKmeans(tf.keras.Model):\n","    def __init__(self, n_cluster=64, learning_rate=0.1):\n","        super(TFMiniBatchHKmeans, self).__init__()\n","        self.n_cluster = n_cluster\n","        self.learning_rate = learning_rate\n","        self.centroids = tf.Variable(tf.zeros((n_cluster, 2), dtype=\"float32\"), trainable=False)\n","    \n","    def init_centroid(self, X):\n","        new_centroids = []\n","        choosed = tf.zeros(len(X), dtype=\"bool\")\n","\n","        ix = tf.cast(tf.random.uniform([1], maxval=len(X))//1, \"int32\")[0]\n","        choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","        new_centroids.append(tf.gather(X, ix))\n","        dist = tf.ones([tf.shape(X)[0]]) * tf.float32.max / 10\n","\n","        for i in tqdm(range(self.n_cluster-1)):\n","            centroid = tf.gather(X, ix)\n","    \n","            dist = tf.where(choosed, tf.keras.backend.epsilon() , tf.minimum(dist, tf.math.square(hkmeans.haversine(centroid, X))))\n","            prob = dist / tf.reduce_sum(dist, axis=0, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            ix = tf.cast(tf.argmax(gumbel), \"int32\")\n","            choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","            new_centroids.append(tf.gather(X, ix))\n","        self.centroids.assign(tf.stack(new_centroids, axis=0))\n","        \n","    def partial_fit(self, X):\n","        score = self.haversine(tf.expand_dims(X, axis=1), tf.expand_dims(self.centroids, axis=0))\n","        maxix = tf.argmin(score, axis=1)\n","        new_centroids = tf.math.unsorted_segment_mean(X, maxix, num_segments=self.n_cluster)\n","        num_member = tf.math.unsorted_segment_sum(tf.ones((len(X), 1)), maxix, num_segments=self.n_cluster)\n","        new_centroids = tf.where(num_member > 0, new_centroids, self.centroids)\n","        new_centroids = new_centroids * self.learning_rate + self.centroids * (1 - self.learning_rate)\n","        dist = tf.reduce_mean(tf.reduce_min(score, axis=1))\n","        self.centroids.assign(new_centroids)\n","        return dist\n","    \n","    def haversine(self, X, Y):\n","        delta = Y - X\n","        x_lats = tf.gather(X, 0, axis=-1)\n","        y_lats = tf.gather(Y, 0, axis=-1)\n","        dlat = tf.gather(delta, 0, axis=-1)\n","        dlon = tf.gather(delta, 1, axis=-1)\n","\n","        a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","        c = tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a))\n","        return c\n","    \n","    def transform(self, X, return_score=False):\n","        score = tf.math.log(self.haversine(tf.expand_dims(X, axis=1), tf.expand_dims(self.centroids, axis=0)) + tf.keras.backend.epsilon())\n","        res = tf.argmin(score, axis=-1)\n","        if return_score:\n","            dist = tf.reduce_min(score, axis=-1)\n","            return res, dist\n","        else:\n","            return res\n","\n","class TFMiniBatchSKmeans(tf.keras.Model):\n","    def __init__(self, n_cluster=64, n_dim=100, alpha=3/2, learning_rate=0.1):\n","        super(TFMiniBatchSKmeans, self).__init__()\n","        self.n_dim = n_dim\n","        self.n_cluster = n_cluster\n","        self.alpha = alpha\n","        self.learning_rate = learning_rate\n","\n","        self.centroids = tf.Variable(tf.zeros((n_cluster, n_dim), dtype=\"float32\"), trainable=False)\n","    \n","    def init_centroid(self, X):\n","        new_centroids = []\n","        X = tf.nn.l2_normalize(X, axis=-1)\n","        choosed = tf.zeros(len(X), dtype=\"bool\")\n","\n","        ix = tf.cast(tf.random.uniform([1], maxval=len(X))//1, \"int32\")[0]\n","        choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","        new_centroids.append(tf.gather(X, ix))\n","        \n","        dist = tf.ones([tf.shape(X)[0]]) * tf.float32.max / 10\n","\n","        for i in tqdm(range(self.n_cluster-1)):\n","            centroid = tf.gather(X, ix)\n","            cos = tf.einsum(\"d,nd->n\", centroid, X)\n","            dist = tf.where(choosed, tf.keras.backend.epsilon() , tf.maximum(tf.keras.backend.epsilon(), tf.minimum(dist, 1. - cos)))         \n","            prob = dist / tf.reduce_sum(dist, axis=0, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            ix = tf.cast(tf.argmax(gumbel), \"int32\")\n","            choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","            new_centroids.append(X[ix])\n","        self.centroids.assign(tf.stack(new_centroids, axis=0))\n","        \n","    @tf.function\n","    def partial_fit(self, X):\n","        score = tf.einsum(\"nd,kd->nk\", X, self.centroids)\n","        maxix = tf.argmax(score, axis=1)\n","        new_centroids = tf.math.unsorted_segment_mean(X, maxix, num_segments=self.n_cluster)\n","        num_member = tf.math.unsorted_segment_sum(tf.ones((len(X), 1)), maxix, num_segments=self.n_cluster)\n","        new_centroids = tf.where(num_member > 0, new_centroids, self.centroids)\n","        new_centroids = tf.nn.l2_normalize(new_centroids, axis=1)\n","        new_centroids = new_centroids * self.learning_rate + self.centroids * (1 - self.learning_rate)\n","        new_centroids = tf.nn.l2_normalize(new_centroids, axis=1)\n","        dist = tf.reduce_mean(self.alpha - tf.reduce_max(score, axis=1))\n","        self.centroids.assign(new_centroids)\n","        return dist\n","    \n","    def transform(self, X, return_score=False):\n","        score = tf.einsum(\"...d,kd->...k\", X, self.centroids)\n","        res = tf.argmax(score, axis=-1)\n","        if return_score:\n","            cossim = tf.reduce_max(score, axis=-1)\n","            return res, cossim\n","        else:\n","            return res"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:30:25.962698Z","start_time":"2022-07-01T03:30:25.941698Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:54.025840Z","iopub.status.busy":"2022-06-25T00:30:54.025170Z","iopub.status.idle":"2022-06-25T00:30:54.064029Z","shell.execute_reply":"2022-06-25T00:30:54.062758Z","shell.execute_reply.started":"2022-06-25T00:30:54.025791Z"},"id":"YUhJ4xzSybLb"},"outputs":[],"source":["class DataContainer():\n","    def __init__(self, df, ix_values, category_ix, positive_ix, neighbor_ix, neighbor_dist):\n","        self.name = tf.constant(df[\"sp_name\"].values)\n","        self.address = tf.constant(df[\"sp_address\"].values)\n","        self.position = tf.expand_dims(tf.constant(np.deg2rad(df[[\"latitude\", \"longitude\"]].astype(np.float32).values), dtype=\"float32\"), axis=0)\n","        self.pid = tf.constant(df[\"pid\"].astype(np.int32).values, dtype=\"int32\")\n","        self.group = tf.constant(df[\"group\"].astype(np.int32).values, dtype=\"int32\")\n","        self.ix_values = ix_values\n","        self.category_ix = category_ix\n","        self.positive_ix = positive_ix\n","        self.neighbor_ix = neighbor_ix\n","        self.neighbor_dist = neighbor_dist\n","        \n","    def get_position(self, ix):\n","        return tf.gather(tf.gather(self.position, ix, axis=1), 0)\n","\n","    def call(self, ix, size=1):\n","        name = tf.gather(self.name, ix)\n","        address = tf.gather(self.address, ix)\n","        position = self.get_position(ix)\n","        categories = tf.gather(self.category_ix, ix)\n","        ix_values = {k: tf.gather(self.ix_values[k], ix) for k in self.ix_values.keys()}\n","        pids = tf.gather(self.pid, ix)\n","        return ix, name, address, position, ix_values, categories, pids, size\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def log_haversine(self, X, Y):\n","        delta = Y - X\n","        x_lats = tf.gather(X, 0, axis=-1)\n","        y_lats = tf.gather(Y, 0, axis=-1)\n","        dlat = tf.gather(delta, 0, axis=-1)\n","        dlon = tf.gather(delta, 1, axis=-1)\n","\n","        a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","        c = tf.math.log(2 * tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a)) + tf.keras.backend.epsilon())\n","        return c\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def negative_sample_by_score_top_k(self, ix, score, except_loc=None, k=128, distcut=4):\n","        pos_ix = tf.gather(self.positive_ix, tf.gather(self.pid, ix))\n","        score = tf.tensor_scatter_nd_update(score,\n","                                            tf.stack([tf.cast(pos_ix.value_rowids(), \"int32\"), pos_ix.values], axis=1),\n","                                            tf.ones([tf.shape(pos_ix.values)[0]])*tf.float32.min/10.)\n","        if not except_loc is None:\n","            score = tf.tensor_scatter_nd_update(score,\n","                                                except_loc,\n","                                                tf.ones([tf.shape(except_loc)[0]])*tf.float32.min/10.)            \n","        score, neighbor = tf.math.top_k(score, k=k)\n","        _, order = tf.math.top_k(tf.random.uniform([tf.shape(ix)[0], k]), k=k)\n","        score = tf.gather(score, order, batch_dims=1)\n","        neighbor = tf.gather(neighbor, order, batch_dims=1)\n","        \n","        score = tf.reverse_sequence(score, \n","                                    tf.ones([tf.shape(score)[0]], dtype=\"int32\") * tf.shape(score)[1],\n","                                    seq_axis=1)\n","        neighbor = tf.reverse_sequence(neighbor,\n","                                  tf.ones([tf.shape(neighbor)[0]], dtype=\"int32\") * tf.shape(neighbor)[1],\n","                                  seq_axis=1)\n","        return [score, neighbor]\n","    \n","    def get_positive_info(self, ix):\n","        pos_ix = tf.gather(self.positive_ix, tf.gather(self.pid, ix))\n","        Y = self.get_position(pos_ix)\n","        X = self.get_position(tf.gather(ix, pos_ix.value_rowids()))\n","        pos_dist = tf.RaggedTensor.from_value_rowids(self.log_haversine(X, Y.values), Y.value_rowids())\n","        pos_ix = pos_ix.to_tensor(default_value=-1)\n","        pos_ix = tf.where(tf.expand_dims(ix, axis=1) != pos_ix, pos_ix, -1)\n","        pos_dist = pos_dist.to_tensor(default_value=0.)\n","        return pos_dist, pos_ix\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def random_walk_sampling(self, query_ix, step, top_k):\n","        query_ix = tf.cast(query_ix, \"int32\")\n","        searched = tf.expand_dims(query_ix, axis=1)\n","        current = tf.identity(query_ix)\n","\n","        for i in range(step):\n","            n_ix = tf.gather(self.neighbor_ix, current)\n","            n_dist = tf.gather(self.neighbor_dist, current)\n","            n_dist = tf.where(tf.reduce_any(tf.expand_dims(n_ix, axis=2) == tf.expand_dims(searched, axis=1), axis=2), tf.keras.backend.epsilon(), n_dist)\n","\n","            prob = n_dist / tf.reduce_sum(n_dist, axis=1, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            nexts_ = tf.gather(n_ix, tf.math.top_k(gumbel, k=top_k)[1], batch_dims=1)\n","            searched = tf.concat([searched, nexts_], axis=1)\n","            current = tf.gather(nexts_, 0, axis=1)\n","        return tf.reshape(searched, [-1]), step*top_k + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:34:02.216743Z","start_time":"2022-07-01T03:34:02.190743Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:54.109418Z","iopub.status.busy":"2022-06-25T00:30:54.109036Z","iopub.status.idle":"2022-06-25T00:30:54.224744Z","shell.execute_reply":"2022-06-25T00:30:54.223988Z","shell.execute_reply.started":"2022-06-25T00:30:54.109384Z"},"id":"hgAgrBlrybLb"},"outputs":[],"source":["DIMSIZE = 256\n","WALK_STEP = 128\n","WALK_TOPK = 2\n","num_categories = max(category_ix_dict.values()) + 1\n","\n","name_spe = SentencePieceEmbeddingLayer(32000, DIMSIZE, nadare_feature_dir + \"sp_name.model\", None)\n","address_spe = SentencePieceEmbeddingLayer(32000, DIMSIZE, nadare_feature_dir + \"sp_address.model\", None)\n","\n","ix_emb_layer = IxEmbeddingLayer(ix_params_dict, num_layer=1, out_dim=DIMSIZE)\n","cat_emb_layer = CategoryEmbeddingLayer(num_categories, DIMSIZE)\n","\n","#locrank_model = LocationModel(address_dpc, ix_emb_layer, num_rank=128)\n","classify_model = ClassifyTrainModel(name_spe, address_spe, ix_emb_layer, cat_emb_layer)\n","mixer_model = MixerModel(name_spe, address_spe, ix_emb_layer, cat_emb_layer, num_layer=2, num_dim=DIMSIZE)\n","\n","mixer_loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","\n","\n","name_spe.embedding(0)\n","address_spe.embedding(0)\n","for k, v in ix_emb_layer.embedding_layers.items():\n","    v(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:34:05.702311Z","start_time":"2022-07-01T03:34:05.517311Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:54.230572Z","iopub.status.busy":"2022-06-25T00:30:54.228526Z","iopub.status.idle":"2022-06-25T00:30:54.324410Z","shell.execute_reply":"2022-06-25T00:30:54.323473Z","shell.execute_reply.started":"2022-06-25T00:30:54.230529Z"},"id":"2mJNp8_AybLb"},"outputs":[],"source":["classify_model.load_weights(nadare_feature_dir + \"classify_model\")\n","mixer_model.load_weights(nadare_feature_dir + \"mixer_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:31:28.970756Z","start_time":"2022-07-01T03:31:28.778757Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:54.326104Z","iopub.status.busy":"2022-06-25T00:30:54.325743Z","iopub.status.idle":"2022-06-25T00:30:54.342469Z","shell.execute_reply":"2022-06-25T00:30:54.341302Z","shell.execute_reply.started":"2022-06-25T00:30:54.326070Z"},"id":"UgmrCcIEybLb"},"outputs":[],"source":["container_cols = [\"sp_name\", \"sp_address\", \"latitude\", \"longitude\", \"pid\", \"group\"] \n","test_container =  DataContainer(test_df[container_cols],\n","                                   test_ix_values,\n","                                    test_categories_ix,\n","                                    None,\n","                                None,\n","                                None)\n","test_ixs = tf.range(len(test_container.name), dtype=\"int32\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:33:19.232832Z","start_time":"2022-07-01T03:31:29.364738Z"},"execution":{"iopub.execute_input":"2022-06-25T00:30:54.344724Z","iopub.status.busy":"2022-06-25T00:30:54.343876Z","iopub.status.idle":"2022-06-25T00:30:55.986255Z","shell.execute_reply":"2022-06-25T00:30:55.985313Z","shell.execute_reply.started":"2022-06-25T00:30:54.344687Z"},"id":"Yc-wU_XpybLc"},"outputs":[],"source":["normalized_category_dense = tf.nn.l2_normalize(cat_emb_layer.category_dense, axis=1)\n","pseudo_categories = []\n","category_ix_dict_r = {v: k for k, v in category_ix_dict.items()}\n","eval_ds = tf.data.Dataset.from_tensor_slices(test_ixs)\\\n","                .batch(128)\\\n","                .map(test_container.call)\n","\n","for ix, name, address, position, ix_values, categories, pid, size in tqdm(eval_ds):\n","    X = tf.nn.l2_normalize(classify_model.transform(name, address, ix_values), axis=1)\n","    label = tf.argmax(tf.einsum(\"nd,md->nm\", X, normalized_category_dense), axis=1)\n","    for l in label.numpy():\n","        pseudo_categories.append(category_ix_dict_r[l])\n","        \n","def remove_test_only_category(text):\n","    res = []\n","    for x in text.split(\", \"):\n","        if x in category_ix_dict.keys():\n","            res.append(x)\n","    return \", \".join(res)\n","\n","test_df[\"categories\"] = np.vectorize(remove_test_only_category)(test_df[\"categories\"])\n","test_df[\"categories\"] = np.where(test_df[\"categories\"] == \"\", pseudo_categories, test_df[\"categories\"])\n","test_categories_ix = get_category_ix(test_df, category_ix_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:34:57.670056Z","start_time":"2022-07-01T03:34:24.044210Z"},"execution":{"iopub.execute_input":"2022-06-25T00:39:11.930166Z","iopub.status.busy":"2022-06-25T00:39:11.929687Z","iopub.status.idle":"2022-06-25T00:39:13.359053Z","shell.execute_reply":"2022-06-25T00:39:13.358013Z","shell.execute_reply.started":"2022-06-25T00:39:11.930132Z"},"id":"pWF5xXGyybLc"},"outputs":[],"source":["# name\n","test_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(test_df)))\\\n","                .batch(1024)\\\n","                .map(test_container.call)\n","\n","name_embeddings = []\n","address_embeddings = []\n","ix_values_embeddings = []\n","mix_embeddings = []\n","for query_info in tqdm(test_ds):\n","    ix, name, address, position, ix_values, categories, pid, size = query_info\n","    name_embeddings.append(tf.nn.l2_normalize(tf.reduce_sum(name_spe(name), axis=1), axis=1))\n","    address_embeddings.append(tf.nn.l2_normalize(tf.reduce_sum(address_spe(address), axis=1), axis=1))\n","    ix_values_embeddings.append(tf.nn.l2_normalize(ix_emb_layer(ix_values), axis=1))\n","    mix_embeddings.append(tf.nn.l2_normalize(mixer_model(query_info), axis=1))\n","    \n","\n","name_embeddings = tf.nn.l2_normalize(tf.concat(name_embeddings, axis=0), axis=1)\n","address_embeddings = tf.nn.l2_normalize(tf.concat(address_embeddings, axis=0), axis=1)\n","ix_values_embeddings = tf.nn.l2_normalize(tf.concat(ix_values_embeddings, axis=0), axis=1)\n","mix_embeddings = tf.nn.l2_normalize(tf.concat(mix_embeddings, axis=0), axis=1)\n","\n","category_embeddings = tf.nn.l2_normalize(tf.math.reduce_sum(tf.gather(cat_emb_layer.category_dense, test_categories_ix), axis=1), axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:35:24.491319Z","start_time":"2022-07-01T03:35:24.479319Z"},"id":"KZjJuZH0ybLc"},"outputs":[],"source":["name_embeddings.shape[0] * name_embeddings.shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:36:00.752419Z","start_time":"2022-07-01T03:36:00.732418Z"},"execution":{"iopub.execute_input":"2022-06-25T00:42:22.391001Z","iopub.status.busy":"2022-06-25T00:42:22.390424Z","iopub.status.idle":"2022-06-25T00:42:22.489390Z","shell.execute_reply":"2022-06-25T00:42:22.488258Z","shell.execute_reply.started":"2022-06-25T00:42:22.390953Z"},"id":"_FlGESG0ybLc"},"outputs":[],"source":["hkmeans = TFMiniBatchHKmeans(255*8, learning_rate=1.)\n","name_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(name_embeddings)[1], learning_rate=1.)\n","address_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(address_embeddings)[1], learning_rate=1.)\n","category_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(category_embeddings)[1], learning_rate=1.)\n","ix_values_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(ix_values_embeddings)[1], learning_rate=1.)\n","mix_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(mix_embeddings)[1], learning_rate=1.)\n","brand_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(mix_embeddings)[1], learning_rate=1.)\n","brand_pid_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(mix_embeddings)[1], learning_rate=1.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:36:03.229506Z","start_time":"2022-07-01T03:36:03.194506Z"},"id":"9pkL6o7VybLc"},"outputs":[],"source":["hkmeans.load_weights(nadare_feature_dir + \"haversine_kmeans\")\n","name_kmeans.load_weights(nadare_feature_dir + \"name_kmeans\")\n","address_kmeans.load_weights(nadare_feature_dir + \"address_kmeans\")\n","ix_values_kmeans.load_weights(nadare_feature_dir + \"ix_value_kmeans\")\n","mix_kmeans.load_weights(nadare_feature_dir + \"mix_kmeans\")\n","category_kmeans.load_weights(nadare_feature_dir + \"category_kmeans\")\n","brand_kmeans.load_weights(nadare_feature_dir + \"brand_kmeans\")\n","brand_pid_kmeans.load_weights(nadare_feature_dir + \"brand_pid_kmeans\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:36:16.367588Z","start_time":"2022-07-01T03:36:06.659590Z"},"execution":{"iopub.execute_input":"2022-06-25T01:05:33.664856Z","iopub.status.busy":"2022-06-25T01:05:33.663938Z","iopub.status.idle":"2022-06-25T01:05:33.876367Z","shell.execute_reply":"2022-06-25T01:05:33.875631Z","shell.execute_reply.started":"2022-06-25T01:05:33.664803Z"},"id":"D7l--8clybLc"},"outputs":[],"source":["BATCH_SIZE = 1024\n","eval_ds = tf.data.Dataset.from_tensor_slices(test_ixs)\\\n","                .batch(10000)\n","hkmeans_labels = []\n","name_kmeans_labels = []\n","address_kmeans_labels = []\n","ix_values_kmeans_labels = []\n","mix_kmeans_labels = []\n","category_kmeans_labels = []\n","brand_kmeans_labels = []\n","brand_pid_kmeans_labels = []\n","\n","\n","hkmeans_scores = []\n","name_kmeans_scores = []\n","address_kmeans_scores = []\n","ix_values_kmeans_scores = []\n","mix_kmeans_scores = []\n","category_kmeans_scores = []\n","brand_kmeans_scores = []\n","brand_pid_kmeans_scores = []\n","\n","\n","for ix in tqdm(eval_ds):\n","    res = hkmeans.transform(tf.gather(test_container.position[0], ix), return_score=True)\n","    hkmeans_labels.append(res[0])\n","    hkmeans_scores.append(res[1])\n","    \n","    res = name_kmeans.transform(tf.gather(name_embeddings, ix), return_score=True)\n","    name_kmeans_labels.append(res[0])\n","    name_kmeans_scores.append(res[1])\n","\n","    res = address_kmeans.transform(tf.gather(address_embeddings, ix), return_score=True)\n","    address_kmeans_labels.append(res[0])\n","    address_kmeans_scores.append(res[1])\n","\n","    res = ix_values_kmeans.transform(tf.gather(ix_values_embeddings, ix), return_score=True)\n","    ix_values_kmeans_labels.append(res[0])\n","    ix_values_kmeans_scores.append(res[1])\n","\n","    res = mix_kmeans.transform(tf.gather(mix_embeddings, ix), return_score=True)\n","    mix_kmeans_labels.append(res[0])\n","    mix_kmeans_scores.append(res[1])\n","    \n","    res = category_kmeans.transform(tf.gather(category_embeddings, ix), return_score=True)\n","    category_kmeans_labels.append(res[0])\n","    category_kmeans_scores.append(res[1])\n","    \n","    res = brand_kmeans.transform(tf.gather(name_embeddings, ix), return_score=True)\n","    brand_kmeans_labels.append(res[0])\n","    brand_kmeans_scores.append(res[1]) \n","    \n","    res = brand_pid_kmeans.transform(tf.gather(name_embeddings, ix), return_score=True)\n","    brand_pid_kmeans_labels.append(res[0])\n","    brand_pid_kmeans_scores.append(res[1]) \n","\n","    \n","test_df[\"hkmeans_labels\"] = tf.concat(hkmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"name_kmeans_labels\"] = tf.concat(name_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"address_kmeans_labels\"] = tf.concat(address_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"ix_values_kmeans_labels\"] = tf.concat(ix_values_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"mix_kmeans_labels\"] = tf.concat(mix_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"category_kmeans_labels\"] = tf.concat(category_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"brand_kmeans_labels\"] = tf.concat(brand_kmeans_labels, axis=0).numpy().astype(np.int16)\n","test_df[\"brand_pid_kmeans_labels\"] = tf.concat(brand_pid_kmeans_labels, axis=0).numpy().astype(np.int16)\n","\n","test_df[\"hkmeans_scores\"] = tf.concat(hkmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"name_kmeans_scores\"] = tf.concat(name_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"address_kmeans_scores\"] = tf.concat(address_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"ix_values_kmeans_scores\"] = tf.concat(ix_values_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"mix_kmeans_scores\"] = tf.concat(mix_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"category_kmeans_scores\"] = tf.concat(category_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"brand_kmeans_scores\"] = tf.concat(brand_kmeans_scores, axis=0).numpy().astype(np.float32)\n","test_df[\"brand_pid_kmeans_scores\"] = tf.concat(brand_pid_kmeans_scores, axis=0).numpy().astype(np.float32)\n","\n","\n","hkmeans_labels = []\n","name_kmeans_labels = []\n","address_kmeans_labels = []\n","ix_values_kmeans_labels = []\n","mix_kmeans_labels = []\n","category_kmeans_labels = []\n","brand_kmeans_labels = []\n","brand_pid_kmeans_labels = []\n","\n","\n","hkmeans_scores = []\n","name_kmeans_scores = []\n","address_kmeans_scores = []\n","ix_values_kmeans_scores = []\n","mix_kmeans_scores = []\n","category_kmeans_scores = []\n","brand_kmeans_scores = []\n","brand_pid_kmeans_scores = []\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JXBXKvR7ybLd"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:36:18.629683Z","start_time":"2022-07-01T03:36:18.219720Z"},"id":"Q7Qfl5ijybLd"},"outputs":[],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:36:19.185698Z","start_time":"2022-07-01T03:36:18.719696Z"},"id":"XrYz0njtybLd"},"outputs":[],"source":["del ix_values_kmeans, mix_kmeans_labels\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:36:20.427404Z","start_time":"2022-07-01T03:36:20.400403Z"},"execution":{"iopub.execute_input":"2022-06-25T01:05:38.092855Z","iopub.status.busy":"2022-06-25T01:05:38.092261Z","iopub.status.idle":"2022-06-25T01:05:38.162846Z","shell.execute_reply":"2022-06-25T01:05:38.162055Z","shell.execute_reply.started":"2022-06-25T01:05:38.092810Z"},"id":"wpFEDz2nybLd"},"outputs":[],"source":["# simple version\n","\n","import tensorflow_ranking as tfr\n","\n","class LogisticModel(tf.keras.Model):\n","    def __init__(self, dim):\n","        super(LogisticModel, self).__init__()\n","        self.scale = tf.Variable(tf.zeros([dim], dtype=\"float32\"), trainable=True)\n","        self.loss_func = tfr.keras.losses.ApproxNDCGLoss()\n","        \n","    def call(self, X, Y):\n","        logit = tf.einsum(\"nmd,d->nm\", X, self.scale)# + self.bias\n","        loss = self.loss_func(Y, logit)\n","        return loss    \n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:37:33.312021Z","start_time":"2022-07-01T03:37:33.291022Z"},"execution":{"iopub.execute_input":"2022-06-21T23:38:40.775844Z","iopub.status.busy":"2022-06-21T23:38:40.775142Z","iopub.status.idle":"2022-06-21T23:38:40.788024Z","shell.execute_reply":"2022-06-21T23:38:40.787227Z","shell.execute_reply.started":"2022-06-21T23:38:40.775802Z"},"id":"zLqaFh8eybLd"},"outputs":[],"source":["simple_logistic_model = LogisticModel(5)\n","simple_logistic_model.load_weights(nadare_feature_dir + \"simple_logistic_model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:37:51.460451Z","start_time":"2022-07-01T03:37:51.447451Z"},"id":"hwGIeC5TybLd"},"outputs":[],"source":["simple_logistic_model.scale"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:37:57.772894Z","start_time":"2022-07-01T03:37:54.378926Z"},"id":"0I9lw_25ybLd"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","bagofword_vectorizer = TfidfVectorizer(min_df=0, binary=True, use_idf=False, norm=\"l2\", dtype=np.float32)\n","name_bow = bagofword_vectorizer.fit_transform(test_df[\"sp_name\"]).tocoo()\n","\n","name_bow_rag_data = tf.RaggedTensor.from_value_rowids(name_bow.data, name_bow.row)\n","name_bow_rag_col = tf.RaggedTensor.from_value_rowids(name_bow.col, name_bow.row)\n","name_bow_coo = tf.SparseTensor(tf.cast(tf.stack([name_bow.row, name_bow.col], axis=-1), \"int64\"), name_bow.data, name_bow.shape)\n","\n","bigram_word_vectorizer = TfidfVectorizer(min_df=0, binary=True, use_idf=False, norm=\"l2\", dtype=np.float32, ngram_range=(2, 2))\n","bi_name_bow = bigram_word_vectorizer.fit_transform(test_df[\"sp_name\"]).tocoo()\n","bi_name_bow_rag_col = tf.RaggedTensor.from_value_rowids(bi_name_bow.col, bi_name_bow.row)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:40:34.056288Z","start_time":"2022-07-01T03:40:19.623242Z"},"id":"pI5u4-2NybLd"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","test_df[\"pid\"] = test_df[\"point_of_interest\"].map({v: i for i, v in enumerate(test_df[\"point_of_interest\"].unique())})\n","\n","group = -np.ones(len(test_df), dtype=\"int32\")\n","g = 0\n","for g, (dev_p_ids, val_p_ids) in enumerate(KFold(n_splits=5, shuffle=True, random_state=42).split(test_df[\"pid\"].unique())):\n","    ix = test_df[\"pid\"].isin(val_p_ids)\n","    group[ix] = g\n","test_df[\"group\"] = group\n","\n","test_df[\"pid\"] = test_df[\"point_of_interest\"].map({v: i for i, v in enumerate(test_df[\"point_of_interest\"].unique())})\n","\n","container_cols = [\"sp_name\", \"sp_address\", \"latitude\", \"longitude\", \"pid\", \"group\"] \n","test_container =  DataContainer(test_df[container_cols],\n","                                   test_ix_values,\n","                                    test_categories_ix,\n","                                    tf.cast(tf.ragged.constant(test_df.groupby(\"pid\")[\"ix\"].unique().tolist()), \"int32\"),\n","                                None,\n","                                None)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:04:41.831096Z","start_time":"2022-07-01T03:40:40.068391Z"},"id":"bU3rtHbXybLe"},"outputs":[],"source":["NUM_CANDIDATE = 32\n","NEAREST = 4\n","NAME_SIM = 2\n","NUM_FEAT = 7\n","test_ds = tf.data.Dataset.from_tensor_slices(test_ixs)\\\n","                .batch(128)\n","\n","@tf.function(experimental_relax_shapes=True)\n","def make_pred_feat(ix):\n","    selected = tf.expand_dims(ix, axis=1)\n","    pos_ix = tf.gather(test_container.positive_ix, tf.gather(test_container.pid, ix))\n","\n","    query_position = tf.expand_dims(test_container.get_position(ix), axis=1)\n","    dist = test_container.log_haversine(query_position, test_container.position)    \n","    \n","    indices = tf.stack([tf.repeat(tf.range(len(ix)), tf.gather(name_bow_rag_data, ix).row_lengths()),\n","                        tf.gather(name_bow_rag_col, ix).values], axis=-1)\n","    values = tf.gather(name_bow_rag_data, ix).values\n","    shape = [len(ix), name_bow_coo.shape[1]]\n","    query = tf.scatter_nd(indices, values, shape)\n","    sparse_name_sim = tf.sparse.sparse_dense_matmul(query, name_bow_coo, adjoint_b=True)\n","        \n","    name_cossim = tf.einsum(\"nd,md->nm\", tf.gather(name_embeddings, ix), name_embeddings)\n","    address_cossim = tf.einsum(\"nd,md->nm\", tf.gather(address_embeddings, ix), address_embeddings)\n","    category_cossim = tf.einsum(\"nd,md->nm\", tf.gather(category_embeddings, ix), category_embeddings)    \n","    logistic_score = tf.einsum(\"d,nmd->nm\", simple_logistic_model.scale, tf.stack([dist, sparse_name_sim, name_cossim, address_cossim, category_cossim], axis=-1))\n","    \n","    \n","    score, topk = tf.math.top_k(-dist, NEAREST + tf.shape(selected)[1])\n","    update_pos = tf.where(tf.reduce_any(tf.expand_dims(topk, axis=2) == tf.expand_dims(selected, axis=1), axis=2))\n","    score = tf.tensor_scatter_nd_update(score, \n","                                        update_pos,\n","                                        tf.ones(tf.shape(update_pos)[0]) * tf.float32.min / 10)\n","    topk = tf.gather(topk, tf.argsort(-score, axis=1), batch_dims=1)\n","    selected = tf.concat([selected, tf.slice(topk, [0, 0], [-1, NEAREST])], axis=1)\n","    \n","    score, topk = tf.math.top_k(name_cossim, NAME_SIM + tf.shape(selected)[1])\n","    update_pos = tf.where(tf.reduce_any(tf.expand_dims(topk, axis=2) == tf.expand_dims(selected, axis=1), axis=2))\n","    score = tf.tensor_scatter_nd_update(score, \n","                                        update_pos,\n","                                        tf.ones(tf.shape(update_pos)[0]) * tf.float32.min / 10)\n","    topk = tf.gather(topk, tf.argsort(-score, axis=1), batch_dims=1)\n","    selected = tf.concat([selected, tf.slice(topk, [0, 0], [-1, NAME_SIM])], axis=1)  \n","    \n","    score, topk = tf.math.top_k(sparse_name_sim, NAME_SIM + tf.shape(selected)[1])\n","    update_pos = tf.where(tf.reduce_any(tf.expand_dims(topk, axis=2) == tf.expand_dims(selected, axis=1), axis=2))\n","    score = tf.tensor_scatter_nd_update(score, \n","                                        update_pos,\n","                                        tf.ones(tf.shape(update_pos)[0]) * tf.float32.min / 10)\n","    topk = tf.gather(topk, tf.argsort(-score, axis=1), batch_dims=1)\n","    selected = tf.concat([selected, tf.slice(topk, [0, 0], [-1, NAME_SIM])], axis=1)\n","    \n","    score, topk = tf.math.top_k(logistic_score, NUM_CANDIDATE - 2*NAME_SIM - NEAREST + tf.shape(selected)[1])\n","    update_pos = tf.where(tf.reduce_any(tf.expand_dims(topk, axis=2) == tf.expand_dims(selected, axis=1), axis=2))\n","    score = tf.tensor_scatter_nd_update(score, \n","                                        update_pos,\n","                                        tf.ones(tf.shape(update_pos)[0]) * tf.float32.min / 10)\n","    topk = tf.gather(topk, tf.argsort(-score, axis=1), batch_dims=1)\n","    selected = tf.concat([selected, tf.slice(topk, [0, 0], [-1, NUM_CANDIDATE - 2*NAME_SIM - NEAREST])], axis=1)\n","   \n","    \n","    candidate_ixs  = tf.cast(tf.slice(selected, [0, 1], [-1, -1]), \"int32\")\n","    \n","    candidate_dist = tf.gather(dist, candidate_ixs, batch_dims=1)\n","    candidate_sparse_name_sim = tf.gather(sparse_name_sim, candidate_ixs, batch_dims=1)\n","    candidate_name_cossim = tf.gather(name_cossim, candidate_ixs, batch_dims=1)\n","    candidate_address_cossim = tf.gather(address_cossim, candidate_ixs, batch_dims=1)\n","    candidate_category_cossim = tf.gather(category_cossim, candidate_ixs, batch_dims=1)\n","    candidate_logistic_score = tf.gather(logistic_score, candidate_ixs, batch_dims=1)\n","    candidate_mix_cossim = tf.einsum(\"nd,nmd->nm\", tf.gather(mix_embeddings, ix), tf.gather(mix_embeddings, candidate_ixs))\n","    \n","    \n","    cross_feat = tf.stack([candidate_dist, candidate_sparse_name_sim, candidate_name_cossim, candidate_address_cossim, candidate_category_cossim, candidate_logistic_score, candidate_mix_cossim], axis=-1)\n","    \n","    candidate_ixs = tf.reshape(candidate_ixs, [-1])\n","    cross_feat = tf.reshape(cross_feat, [-1, NUM_FEAT])\n","    \n","    return candidate_ixs, cross_feat\n","\n","\n","query_ixs = []\n","candidate_ixs = []\n","cross_feats = []\n","\n","for query_ix in tqdm(test_ds):\n","    query_ix = tf.reshape(query_ix, [-1])\n","    candidate_ix, cross_feat = make_pred_feat(query_ix)\n","    \n","    query_ixs.append(query_ix)\n","    candidate_ixs.append(candidate_ix)\n","    cross_feats.append(cross_feat)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:04:41.910099Z","start_time":"2022-07-01T04:04:41.832097Z"},"execution":{"iopub.execute_input":"2022-06-21T01:57:03.779277Z","iopub.status.busy":"2022-06-21T01:57:03.778137Z","iopub.status.idle":"2022-06-21T01:57:03.785257Z","shell.execute_reply":"2022-06-21T01:57:03.784245Z","shell.execute_reply.started":"2022-06-21T01:57:03.779191Z"},"id":"Hgnwy-LmybLe"},"outputs":[],"source":["query_ixs = tf.repeat(tf.concat(test_ixs, axis=0), NUM_CANDIDATE, axis=0)\n","candidate_ixs = tf.concat(candidate_ixs, axis=0)\n","cross_feats = tf.concat(cross_feats, axis=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:04:41.926098Z","start_time":"2022-07-01T04:04:41.911099Z"},"id":"4UCWfYwiybLe"},"outputs":[],"source":["import tensorflow_text as tf_text\n","wsp_tokenizer = tf_text.WhitespaceTokenizer()\n","uch_tokenizer = tf_text.UnicodeCharTokenizer()\n","rouge_l = tf_text.metrics.rouge_l"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:04:53.981050Z","start_time":"2022-07-01T04:04:41.927099Z"},"id":"PPI2dq_-ybLe"},"outputs":[],"source":["import phonenumbers\n","from urllib.parse import urlparse\n","\n","def get_phene_info(phone, country):\n","    if phone == \"\":\n","        return {\"phone_country_code\": 0, \"national_number\": 0}\n","    if phonenumbers.country_code_for_region(country) == 0:\n","        country = None\n","    try:\n","        res = phonenumbers.parse(phone, country)\n","    except:\n","        return {\"phone_country_code\": 0, \"national_number\": 0}\n","    return {\"phone_country_code\": res.country_code, \"national_number\": res.national_number}\n","res = []\n","for phone, country in test_df[[\"phone\", \"country\"]].values:\n","    res.append(get_phene_info(phone, country))\n","phone_df = pd.DataFrame(res)\n","\n","test_df[\"url\"] = test_df[\"url\"].fillna(\"\")\n","test_df[\"phone_national_number\"] = phone_df[\"national_number\"].astype(str)\n","test_df[\"url_netloc\"] = test_df[\"url\"].fillna(\"\").apply(lambda x: urlparse(x)[1])\n","test_df[\"url_path\"] = test_df[\"url\"].fillna(\"\").apply(lambda x: urlparse(x)[2])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:04:57.873049Z","start_time":"2022-07-01T04:04:53.982050Z"},"id":"FkVqPMymybLe"},"outputs":[],"source":["def change_number(text):\n","    res = []\n","    for word in text.split():\n","        if word.isnumeric():\n","            res.append(word)\n","        else:\n","            for r in re.findall(r\"\\d+\", str(word)):\n","                res.append(r)\n","    return \" \".join(res)\n","\n","test_df[\"name_numbers\"] = np.vectorize(change_number)(test_df[\"name_normalized\"].fillna(\"\").astype(str))\n","test_df[\"address_numbers\"] = np.vectorize(change_number)(test_df[\"address_normalized\"].fillna(\"\").astype(str))"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:04:57.889050Z","start_time":"2022-07-01T04:04:57.874051Z"},"id":"du7tvJezybLe"},"outputs":[],"source":["from polyleven import levenshtein\n","from cdifflib import CSequenceMatcher\n","from jarowinkler import jarowinkler_similarity, jaro_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:04:57.905050Z","start_time":"2022-07-01T04:04:57.890050Z"},"id":"2dEbZJ0AybLe"},"outputs":[],"source":["from joblib import Parallel, delayed\n","import Levenshtein\n","from difflib import SequenceMatcher\n","from polyleven import levenshtein\n","from cdifflib import CSequenceMatcher\n","from jarowinkler import jarowinkler_similarity, jaro_similarity\n","\n","\n","def make_data(query_ix, candidate_ix):\n","    query_info = {}\n","    candidate_info = {}\n","    query_info[\"ix\"] = query_ix\n","    candidate_info[\"ix\"] = candidate_ix\n","    \n","    for col in [\"sp_name\", \"name_normalized\", \"sp_address_raw\", \"phone_national_number\", \"url\", \"phone_national_number\",\n","                \"url_netloc\", \"name_numbers\", \"address_numbers\"]:\n","        query_info[col] = test_df.at[query_ix, col]\n","        candidate_info[col] = test_df.at[candidate_ix, col]\n","    query_info[\"name_numbers_split_size\"] = len(query_info[\"name_numbers\"].split())\n","    candidate_info[\"name_numbers_split_size\"] = len(candidate_info[\"name_numbers\"].split())\n","    query_info[\"address_numbers_split_size\"] = len(query_info[\"address_numbers\"].split())\n","    candidate_info[\"address_numbers_split_size\"] = len(candidate_info[\"address_numbers\"].split())    \n","        \n","    return query_info, candidate_info\n","\n","def add_features(query_info, candidate_info):\n","    res = []\n","    res.append(query_info[\"ix\"])\n","    res.append(candidate_info[\"ix\"])\n","    \n","    for col in [\"sp_name\", \"name_normalized\", \"sp_address_raw\"]:\n","        s = query_info[col]\n","        match_s = candidate_info[col]\n","        if len(s) and len(match_s):\n","            res.append(abs(len(match_s) - len(s)))\n","            res.append(abs(len(match_s.split()) - len(s.split())))\n","            res.append(CSequenceMatcher(None, s, match_s).ratio())\n","            res.append(levenshtein(s, match_s))\n","            res.append(res[-1] / (max(len(s), len(match_s)) + 1e-9))\n","            res.append(Levenshtein.jaro_winkler(s, match_s))\n","#             res.append(jarowinkler_similarity(s, match_s))\n","#             res.append(jaro_similarity(s, match_s))\n","        else:\n","            res.append(abs(len(match_s) - len(s)))\n","            res.append(abs(len(match_s.split()) - len(s.split())))\n","            res.append(np.nan)\n","            res.append(np.nan)\n","            res.append(res[-1] / (max(len(s), len(match_s)) + 1e-9))\n","            res.append(np.nan)\n","    \n","    for col in [\"phone_national_number\", \"url\", \"url_netloc\"]:\n","        r = 0\n","        if min(len(query_info[col]), len(candidate_info[col])) > 0:\n","            r = -1 + 2 * (query_info[col] == candidate_info[col])\n","        res.append(r)\n","    for col in [\"name_numbers\", \"address_numbers\"]:\n","        res.append(max(query_info[col + \"_split_size\"], candidate_info[col + \"_split_size\"]))\n","        res.append(min(query_info[col + \"_split_size\"], candidate_info[col + \"_split_size\"]))\n","        s = query_info[col]\n","        match_s = candidate_info[col]\n","        if s != '' and match_s != '':\n","            res.append(SequenceMatcher(None, query_info[col], candidate_info[col]).ratio())\n","        else:\n","            res.append(np.nan)\n","    \n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gxIpsFVybLf"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:36:20.527261Z","start_time":"2022-07-01T04:04:57.906051Z"},"scrolled":true,"id":"bfGmBDvpybLf"},"outputs":[],"source":["print(len(query_ixs))\n","results = Parallel(n_jobs=-1, verbose=1)(delayed(add_features)(*make_data(q_ix, c_ix)) for q_ix, c_ix in zip(query_ixs.numpy(), candidate_ixs.numpy()))\n","candidate_data_df = pd.DataFrame(results, columns = [\"query_ix\",\n","                                                     \"candidate_ix\",\n","                                                     \"name_delta_len\",\n","                                                     \"name_delta_words\",\n","                                                     \"name_gesh\", \n","                                                     \"name_leven\", \n","                                                     \"name_nleven\", \n","                                                     \"name_jaro\",                                                     \n","                                                     \"name_normalized_delta_len\",\n","                                                     \"name_normalized_delta_words\",\n","                                                     \"name_normalized_gesh\", \n","                                                     \"name_normalized_leven\", \n","                                                     \"name_normalized_nleven\", \n","                                                     \"name_normalized_jaro\", \n","                                                     \"address_delta_len\", \n","                                                     \"address_delta_words\", \n","                                                     \"address_gesh\", \n","                                                     \"address_leven\", \n","                                                     \"address_nleven\", \n","                                                     \"address_jaro\",\n","                                                     \"phone_national_number_match\",\n","                                                     \"url_match\",\n","                                                     \"url_netloc_match\",\n","                                                     \"name_numbers_max\",\n","                                                     \"name_numbers_min\",\n","                                                     \"name_numbers_gesh\",\n","                                                     \"address_numbers_max\",\n","                                                     \"address_numbers_min\",\n","                                                     \"address_numbers_gesh\",])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-06-26T18:32:04.558306Z","start_time":"2022-06-26T18:31:37.811525Z"},"id":"45Dj3CMXybLf"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:36:35.605260Z","start_time":"2022-07-01T04:36:20.551261Z"},"id":"akt3iK80ybLf"},"outputs":[],"source":["#del results\n","#gc.collect()\n","\n","int64_cols = ['name_delta_len', 'name_delta_words', \n","              'name_normalized_delta_len', 'name_normalized_delta_words',\n","              'address_delta_len','address_delta_words', \n","              'phone_national_number_match', 'url_match', 'url_netloc_match',\n","              'name_numbers_max', 'name_numbers_min',\n","              'address_numbers_max', 'address_numbers_min']\n","float64_cols = ['name_gesh', 'name_nleven', 'name_jaro', 'name_leven',\n","                'name_normalized_gesh', 'name_normalized_nleven', 'name_normalized_jaro', 'name_normalized_leven',\n","                'address_gesh','address_nleven', 'address_jaro', 'address_leven',\n","                'name_numbers_gesh','address_numbers_gesh']\n","candidate_data_df[int64_cols] = candidate_data_df[int64_cols].astype(np.int16)\n","candidate_data_df[float64_cols] = candidate_data_df[float64_cols].astype(np.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:36:45.846261Z","start_time":"2022-07-01T04:36:35.612262Z"},"id":"D-9NJXTCybLf"},"outputs":[],"source":["gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:36:46.352803Z","start_time":"2022-07-01T04:36:45.847261Z"},"id":"HdGJRvWrybLf"},"outputs":[],"source":["sp_names = tf.constant(test_df[\"sp_name\"].values)\n","normalized_names = tf.constant(test_df[\"name_normalized\"].values)\n","sp_address = tf.constant(test_df[\"sp_address_raw\"].values)\n","normalized_address = tf.constant(test_df[\"address_normalized\"].values)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:31:04.705728Z","start_time":"2022-07-01T08:31:04.687728Z"},"id":"D70zDdy8ybLf"},"outputs":[],"source":["@tf.function(experimental_relax_shapes=True)\n","def calc_rouge_1_2(q_ix, c_ix):\n","    q_code = tf.gather(name_bow_rag_col, q_ix).to_tensor(default_value=-1)\n","    c_code = tf.gather(name_bow_rag_col, c_ix).to_tensor(default_value=-2)\n","    q_cnt = tf.cast(tf.gather(name_bow_rag_col.row_lengths(), q_ix), \"float32\")\n","    c_cnt = tf.cast(tf.gather(name_bow_rag_col.row_lengths(), c_ix), \"float32\")\n","\n","    is_na = tf.minimum(q_cnt, c_cnt) == 0.\n","    match = tf.expand_dims(q_code, axis=2) == tf.expand_dims(c_code, axis=1)\n","    precision = tf.math.count_nonzero(tf.math.reduce_any(match, axis=2), axis=1, dtype=\"float32\") / (q_cnt + tf.keras.backend.epsilon())\n","    recall = tf.math.count_nonzero(tf.math.reduce_any(match, axis=1), axis=1, dtype=\"float32\") / (c_cnt + tf.keras.backend.epsilon())\n","    harmony = 2. / ((1 / (precision + tf.keras.backend.epsilon())) + (1 / (recall + tf.keras.backend.epsilon())))\n","    res_1 = tf.where(tf.expand_dims(is_na, axis=-1), np.nan, tf.stack([harmony, precision, recall], axis=1))\n","    \n","    q_code = tf.gather(bi_name_bow_rag_col, q_ix).to_tensor(default_value=-1)\n","    c_code = tf.gather(bi_name_bow_rag_col, c_ix).to_tensor(default_value=-2)\n","    q_cnt = tf.cast(tf.gather(bi_name_bow_rag_col.row_lengths(), q_ix), \"float32\")\n","    c_cnt = tf.cast(tf.gather(bi_name_bow_rag_col.row_lengths(), c_ix), \"float32\")\n","\n","    is_na = tf.minimum(q_cnt, c_cnt) == 0.\n","    match = tf.expand_dims(q_code, axis=2) == tf.expand_dims(c_code, axis=1)\n","    precision = tf.math.count_nonzero(tf.math.reduce_any(match, axis=2), axis=1, dtype=\"float32\") / (q_cnt + tf.keras.backend.epsilon())\n","    recall = tf.math.count_nonzero(tf.math.reduce_any(match, axis=1), axis=1, dtype=\"float32\") / (c_cnt + tf.keras.backend.epsilon())\n","    harmony = 2. / ((1 / (precision + tf.keras.backend.epsilon())) + (1 / (recall + tf.keras.backend.epsilon())))\n","    res_2 = tf.where(tf.expand_dims(is_na, axis=-1), np.nan, tf.stack([harmony, precision, recall], axis=1))\n","    \n","    return tf.concat([res_1, res_2], axis=-1)\n","\n","@tf.function(experimental_relax_shapes=True)\n","def calc_rouge_l(q_ix, c_ix):\n","    q_name = wsp_tokenizer.tokenize(tf.gather(sp_names, q_ix))\n","    c_name = wsp_tokenizer.tokenize(tf.gather(sp_names, c_ix))\n","    name_score = tf.stack(tf_text.metrics.rouge_l(q_name, c_name), axis=-1)\n","    q_name = uch_tokenizer.tokenize(tf.gather(normalized_names, q_ix))\n","    c_name = uch_tokenizer.tokenize(tf.gather(normalized_names, c_ix))\n","    n_name_score = tf.stack(tf_text.metrics.rouge_l(q_name, c_name), axis=-1)\n","    \n","    q_address = wsp_tokenizer.tokenize(tf.gather(sp_address, q_ix))\n","    c_address = wsp_tokenizer.tokenize(tf.gather(sp_address, c_ix))\n","    address_score = tf.stack(tf_text.metrics.rouge_l(q_address, c_address), axis=-1)\n","    q_address = uch_tokenizer.tokenize(tf.gather(normalized_address, q_ix))\n","    c_address = uch_tokenizer.tokenize(tf.gather(normalized_address, c_ix))\n","    n_address_score = tf.stack(tf_text.metrics.rouge_l(q_address, c_address), axis=-1)\n","    score = tf.concat([name_score, n_name_score, address_score, n_address_score], axis=-1)\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DJLM0vqybLf"},"outputs":[],"source":["rouge_l_"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:41:03.568885Z","start_time":"2022-07-01T08:39:14.365887Z"},"id":"J1V3AbZhybLf"},"outputs":[],"source":["text_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(query_ixs), tf.data.Dataset.from_tensor_slices(candidate_ixs)))\\\n","                         .batch(1024)\n","wsp_tokenizer = tf_text.WhitespaceTokenizer()\n","uch_tokenizer = tf_text.UnicodeCharTokenizer()\n","\n","n_scores = []\n","l_scores = []\n","for q_ix, c_ix in tqdm(text_ds):\n","    n_scores.append(calc_rouge_1_2(q_ix, c_ix))\n","    l_scores.append(calc_rouge_l(q_ix, c_ix))\n","\n","n_scores = tf.concat(n_scores, axis=0)\n","l_scores = tf.concat(l_scores, axis=0)\n","candidate_data_df[[\"rouge_1_name_f\", \"rouge_1_name_p\", \"rouge_1_name_r\",\n","                   \"rouge_2_name_f\", \"rouge_2_name_p\", \"rouge_2_name_r\"]] = n_scores.numpy().astype(np.float32)\n","candidate_data_df[[\"name_f\", \"name_p\", \"name_r\",\"n_name_f\", \"n_name_p\", \"n_name_r\",\n","                   \"address_f\", \"address_p\", \"address_r\",\"n_address_f\", \"n_address_p\", \"n_address_r\"]] = l_scores.numpy().astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:36:20.732024Z","start_time":"2022-07-01T08:36:08.114298Z"},"id":"vPUavuXQybLg"},"outputs":[],"source":["old_names = [\"name_f\", \"name_p\", \"name_r\",\"n_name_f\", \"n_name_p\", \"n_name_r\",\n","                   \"address_f\", \"address_p\", \"address_r\",\"n_address_f\", \"n_address_p\", \"n_address_r\"]\n","new_names = [\"rouge_l_name_f\", \"rouge_l_name_p\", \"rouge_l_name_r\",\"rouge_l_n_name_f\", \"rouge_l_n_name_p\", \"rouge_l_n_name_r\",\n","                   \"rouge_l_address_f\", \"rouge_l_address_p\", \"rouge_l_address_r\",\"rouge_l_n_address_f\", \"rouge_l_n_address_p\", \"rouge_l_n_address_r\"]\n","\n","candidate_data_df = candidate_data_df.rename({o:n for o, n in zip(old_names, new_names)}, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:36:39.548745Z","start_time":"2022-07-01T08:36:39.537746Z"},"id":"keG5TwNaybLg"},"outputs":[],"source":["candidate_data_df.columns.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:59:51.627754Z","start_time":"2022-07-01T04:59:43.606754Z"},"id":"jP-Up4wAybLg"},"outputs":[],"source":["del scores\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T04:59:52.594754Z","start_time":"2022-07-01T04:59:51.628756Z"},"id":"jDfPFF4pybLg"},"outputs":[],"source":["candidate_data_df[\"distance\"] = tf.reshape(tf.gather(cross_feats, 0, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"sparse_name_sim\"] = tf.reshape(tf.gather(cross_feats, 1, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"name_cossim\"] = tf.reshape(tf.gather(cross_feats, 2, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"address_cossim\"] = tf.reshape(tf.gather(cross_feats, 3, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"category_cossim\"] = tf.reshape(tf.gather(cross_feats, 4, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"mix_cossim\"] = tf.reshape(tf.gather(cross_feats, 5, axis=1), [-1]).numpy().astype(np.float32)\n","candidate_data_df[\"logistic_score\"] = tf.reshape(tf.gather(cross_feats, 6, axis=1), [-1]).numpy().astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T05:05:17.996321Z","start_time":"2022-07-01T05:05:17.969320Z"},"id":"SgMbbCN2ybLg"},"outputs":[],"source":["pd.to_pickle(candidate_data_df, \"candidate_df\""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T05:06:13.362416Z","start_time":"2022-07-01T05:06:08.553816Z"},"id":"aU0VAmMNybLg"},"outputs":[],"source":["pd.to_pickle(candidate_data_df, \"../processed/candidate_df_pred23.pickle\", protocol=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T05:09:10.221080Z","start_time":"2022-07-01T05:09:10.199080Z"},"id":"3U5G6hvVybLg"},"outputs":[],"source":["city_index_df"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T05:09:34.588513Z","start_time":"2022-07-01T05:09:28.718344Z"},"execution":{"iopub.execute_input":"2022-06-21T01:57:04.364863Z","iopub.status.busy":"2022-06-21T01:57:04.364478Z","iopub.status.idle":"2022-06-21T01:57:04.400892Z","shell.execute_reply":"2022-06-21T01:57:04.400104Z","shell.execute_reply.started":"2022-06-21T01:57:04.36482Z"},"id":"miOPU9DiybLg"},"outputs":[],"source":["category_tsp_cols = [\"city_pos_tsp_index\", \"state_pos_tsp_index\", \"geo_name_pos_tsp_index\"]\n","kmeans_labels_cols = ['hkmeans_labels', 'name_kmeans_labels', 'address_kmeans_labels', 'ix_values_kmeans_labels', \"mix_kmeans_labels\", \"category_kmeans_labels\", 'brand_kmeans_labels', \"brand_pid_kmeans_labels\"]\n","kmeans_scores_cols = ['hkmeans_scores', 'name_kmeans_scores', 'address_kmeans_scores', 'ix_values_kmeans_scores', \"mix_kmeans_scores\", \"category_kmeans_scores\", 'brand_kmeans_scores', \"brand_pid_kmeans_scores\"]\n","\n","candidate_data_df[\"query_city_index\"] = test_df[\"pseudo_city_ix\"].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","candidate_data_df[\"query_geo_name_index\"] = test_df[\"pseudo_geo_name_ix\"].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","candidate_data_df[\"query_country_index\"] = test_df[\"country_ix\"].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","candidate_data_df[\"query_state_index\"] = test_df[\"pseudo_state_ix\"].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","\n","#candidate_data_df[\"query_city_emb_tsp_index\"] = city_index_df[\"city_emb_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"query_city_index\"]]\n","#candidate_data_df[\"query_state_emb_tsp_index\"] = state_index_df[\"state_emb_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"query_state_index\"]]\n","candidate_data_df[\"query_city_pos_tsp_index\"] = city_index_df[\"city_pos_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"query_city_index\"]]\n","candidate_data_df[\"query_geo_name_pos_tsp_index\"] = geo_name_index_df[\"geo_name_pos_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"query_geo_name_index\"]]\n","candidate_data_df[\"query_state_pos_tsp_index\"] = state_index_df[\"state_pos_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"query_state_index\"]]\n","\n","for col in kmeans_labels_cols:\n","    candidate_data_df[f\"query_{col}\"] = test_df[col].values.astype(np.int16)[candidate_data_df[\"query_ix\"]]\n","for col in kmeans_scores_cols:\n","    candidate_data_df[f\"query_{col}\"] = test_df[col].values.astype(np.float32)[candidate_data_df[\"query_ix\"]]\n","\n","candidate_data_df[\"candidate_city_index\"] = test_df[\"pseudo_city_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","candidate_data_df[\"candidate_geo_name_index\"] = test_df[\"pseudo_geo_name_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","\n","candidate_data_df[\"candidate_country_index\"] = test_df[\"country_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","candidate_data_df[\"candidate_state_index\"] = test_df[\"pseudo_state_ix\"].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","\n","#andidate_data_df[\"candidate_city_emb_tsp_index\"] = city_index_df[\"city_emb_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"candidate_city_index\"]]\n","#candidate_data_df[\"candidate_state_emb_tsp_index\"] = state_index_df[\"state_emb_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"candidate_state_index\"]]\n","candidate_data_df[\"candidate_city_pos_tsp_index\"] = city_index_df[\"city_pos_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"candidate_city_index\"]]\n","candidate_data_df[\"candidate_geo_name_pos_tsp_index\"] = geo_name_index_df[\"geo_name_pos_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"candidate_geo_name_index\"]]\n","candidate_data_df[\"candidate_state_pos_tsp_index\"] = state_index_df[\"state_pos_tsp_index\"].values.astype(np.int16)[candidate_data_df[\"candidate_state_index\"]]\n","\n","for col in kmeans_labels_cols:\n","    candidate_data_df[f\"candidate_{col}\"] = test_df[col].values.astype(np.int16)[candidate_data_df[\"candidate_ix\"]]\n","for col in kmeans_scores_cols:\n","    candidate_data_df[f\"candidate_{col}\"] = test_df[col].values.astype(np.float32)[candidate_data_df[\"candidate_ix\"]]\n","\n","\n","for col in kmeans_labels_cols + category_tsp_cols:\n","    candidate_data_df[f\"{col}_1d_dist\"] = np.abs(candidate_data_df[f\"query_{col}\"] - candidate_data_df[f\"candidate_{col}\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:43:23.407974Z","start_time":"2022-07-01T08:43:23.399975Z"},"execution":{"iopub.execute_input":"2022-06-21T01:57:04.536441Z","iopub.status.busy":"2022-06-21T01:57:04.535702Z","iopub.status.idle":"2022-06-21T01:57:04.543976Z","shell.execute_reply":"2022-06-21T01:57:04.54293Z","shell.execute_reply.started":"2022-06-21T01:57:04.536399Z"},"id":"_9ZeLTTvybLj"},"outputs":[],"source":["feature_cols = [ 'name_delta_len',\n"," 'name_delta_words',\n"," 'name_gesh',\n"," 'name_leven',\n"," 'name_nleven',\n"," 'name_jaro',\n"," 'name_normalized_delta_len',\n"," 'name_normalized_delta_words',\n"," 'name_normalized_gesh',\n"," 'name_normalized_leven',\n"," 'name_normalized_nleven',\n"," 'name_normalized_jaro',\n"," 'address_delta_len',\n"," 'address_delta_words',\n"," 'address_gesh',\n"," 'address_leven',\n"," 'address_nleven',\n"," 'address_jaro',\n"," 'phone_national_number_match',\n"," 'url_match',\n"," 'url_netloc_match',\n"," 'name_numbers_max',\n"," 'name_numbers_min',\n"," 'name_numbers_gesh',\n"," 'address_numbers_max',\n"," 'address_numbers_min',\n"," 'address_numbers_gesh',\n"," 'rouge_1_name_f',\n"," 'rouge_1_name_p',\n"," 'rouge_1_name_r',\n"," 'rouge_2_name_f',\n"," 'rouge_2_name_p',\n"," 'rouge_2_name_r',\n"," 'rouge_l_name_f',\n"," 'rouge_l_name_p',\n"," 'rouge_l_name_r',\n"," 'rouge_l_n_name_f',\n"," 'rouge_l_n_name_p',\n"," 'rouge_l_n_name_r',\n"," 'rouge_l_address_f',\n"," 'rouge_l_address_p',\n"," 'rouge_l_address_r',\n"," 'rouge_l_n_address_f',\n"," 'rouge_l_n_address_p',\n"," 'rouge_l_n_address_r',\n"," 'distance',\n"," 'sparse_name_sim',\n"," 'name_cossim',\n"," 'address_cossim',\n"," 'category_cossim',\n"," 'mix_cossim',\n"," 'logistic_score',\n"," 'query_city_index',\n"," 'query_geo_name_index',\n"," 'query_country_index',\n"," 'query_state_index',\n"," 'query_city_pos_tsp_index',\n"," 'query_geo_name_pos_tsp_index',\n"," 'query_state_pos_tsp_index',\n"," 'query_hkmeans_labels',\n"," 'query_name_kmeans_labels',\n"," 'query_address_kmeans_labels',\n"," 'query_ix_values_kmeans_labels',\n"," 'query_mix_kmeans_labels',\n"," 'query_category_kmeans_labels',\n"," 'query_brand_kmeans_labels',\n"," 'query_brand_pid_kmeans_labels',\n"," 'query_hkmeans_scores',\n"," 'query_name_kmeans_scores',\n"," 'query_address_kmeans_scores',\n"," 'query_ix_values_kmeans_scores',\n"," 'query_mix_kmeans_scores',\n"," 'query_category_kmeans_scores',\n"," 'query_brand_kmeans_scores',\n"," 'query_brand_pid_kmeans_scores',\n"," 'candidate_city_index',\n"," 'candidate_geo_name_index',\n"," 'candidate_country_index',\n"," 'candidate_state_index',\n"," 'candidate_city_pos_tsp_index',\n"," 'candidate_geo_name_pos_tsp_index',\n"," 'candidate_state_pos_tsp_index',\n"," 'candidate_hkmeans_labels',\n"," 'candidate_name_kmeans_labels',\n"," 'candidate_address_kmeans_labels',\n"," 'candidate_ix_values_kmeans_labels',\n"," 'candidate_mix_kmeans_labels',\n"," 'candidate_category_kmeans_labels',\n"," 'candidate_brand_kmeans_labels',\n"," 'candidate_brand_pid_kmeans_labels',\n"," 'candidate_hkmeans_scores',\n"," 'candidate_name_kmeans_scores',\n"," 'candidate_address_kmeans_scores',\n"," 'candidate_ix_values_kmeans_scores',\n"," 'candidate_mix_kmeans_scores',\n"," 'candidate_category_kmeans_scores',\n"," 'candidate_brand_kmeans_scores',\n"," 'candidate_brand_pid_kmeans_scores',\n"," 'hkmeans_labels_1d_dist',\n"," 'name_kmeans_labels_1d_dist',\n"," 'address_kmeans_labels_1d_dist',\n"," 'ix_values_kmeans_labels_1d_dist',\n"," 'mix_kmeans_labels_1d_dist',\n"," 'category_kmeans_labels_1d_dist',\n"," 'brand_kmeans_labels_1d_dist',\n"," 'brand_pid_kmeans_labels_1d_dist',\n"," 'city_pos_tsp_index_1d_dist',\n"," 'state_pos_tsp_index_1d_dist',\n"," 'geo_name_pos_tsp_index_1d_dist']"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T05:10:32.450660Z","start_time":"2022-07-01T05:10:28.600620Z"},"id":"M-bxmVHHybLj"},"outputs":[],"source":["test_df[\"true_count\"] = test_df.groupby(\"pid\")[\"ix\"].transform(\"count\")\n","candidate_data_df[\"label\"] = test_df[\"pid\"][candidate_data_df[\"query_ix\"].values].values == test_df[\"pid\"][candidate_data_df[\"candidate_ix\"].values].values\n","candidate_data_df[\"detected\"] = True\n","candidate_data_df[\"index\"] = candidate_data_df.index\n","candidate_data_df[\"rank\"] = candidate_data_df.groupby(\"query_ix\")[\"index\"].rank(\"first\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T05:10:40.028497Z","start_time":"2022-07-01T05:10:40.008499Z"},"id":"gp6AJZcVybLj"},"outputs":[],"source":["dev_ixs = np.where(test_df[\"group\"] != 0)[0]\n","val_ixs = np.where(test_df[\"group\"] == 0)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T05:10:41.882726Z","start_time":"2022-07-01T05:10:41.877727Z"},"id":"ozNLxAGoybLk"},"outputs":[],"source":["from itertools import groupby\n","\n","class UnionFind():\n","    \n","    def __init__(self, N):\n","        self.parent = [-1] * N\n","        self.size = [1] * N\n","        \n","    def find(self, x):\n","        p = self.parent[x]\n","        if p == -1:\n","            return x\n","        p = self.find(p)\n","        self.parent[x] = p\n","        return p\n","    \n","    def unite(self, x, y):\n","        px = self.find(x)\n","        py = self.find(y)\n","        if px == py:\n","            return\n","        if self.size[px] < self.size[py]:\n","            px, py = py, px\n","        self.size[px] += self.size[py]\n","        self.parent[py] = px\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:43:02.496634Z","start_time":"2022-07-01T08:42:50.570118Z"},"id":"TINzaQTrybLk"},"outputs":[],"source":["dev_loc = (candidate_data_df[\"query_ix\"].isin(dev_ixs) & (~candidate_data_df[\"candidate_ix\"].isin(val_ixs)) &\n","           (candidate_data_df[\"detected\"]) & (candidate_data_df[\"rank\"] <= 32))\n","val_loc = (candidate_data_df[\"query_ix\"].isin(val_ixs) & (candidate_data_df[\"detected\"])  &\n","           (candidate_data_df[\"rank\"] <= 32))\n","\n","\n","dev_data_df = candidate_data_df[dev_loc]\n","val_data_df = candidate_data_df[val_loc]\n","dev_data_df[\"true_count\"] = test_df[\"true_count\"].values[dev_data_df[\"query_ix\"].values]\n","\n","dev_data_df[\"weight\"] = np.where(dev_data_df[\"label\"],\n","                                 1 - (dev_data_df[\"true_count\"] - 1) / dev_data_df[\"true_count\"],\n","                                 1 - (dev_data_df[\"true_count\"]) / (dev_data_df[\"true_count\"]+1))\n","dev_data_df[\"weight\"] = dev_data_df[\"weight\"] / dev_data_df[\"weight\"].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T05:10:59.795674Z","start_time":"2022-07-01T05:10:59.780673Z"},"id":"t3nQY25uybLk"},"outputs":[],"source":["val_data_df[\"rank\"].max()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"start_time":"2022-07-01T09:47:04.510Z"},"id":"mHOJjrQZybLk"},"outputs":[],"source":["import lightgbm as lgb\n","\n","lgb_params = {\n","    \"objective\" : \"binary\",\n","    \"metric\" : \"auc\",\n","    \"boosting\": 'gbdt',\n","    \"max_depth\" : -1,\n","    \"num_leaves\" : 2**11 - 1,\n","    \"learning_rate\" : 0.1,\n","    \"bagging_freq\": 1,\n","     \"bin_construct_sample_cnt\": 200000,\n","    \"lambda_l1\": 1.,\n","    \"lambda_l2\": 1.,\n","    \"bagging_fraction\" : 0.9,\n","    \"feature_fraction\" : 0.8,\n","    \"seed\": 0}\n","\n","dev_data = lgb.Dataset(dev_data_df[feature_cols], label=dev_data_df[\"label\"].astype(np.float32), weight=dev_data_df[\"weight\"].values)\n","val_data = lgb.Dataset(val_data_df[feature_cols], label=val_data_df[\"label\"].astype(np.float32))\n","\n","lgb_model = lgb.train(lgb_params,\n","                      dev_data,\n","                      100000,\n","                      valid_sets = [dev_data, val_data],\n","                      early_stopping_rounds=10,\n","                      verbose_eval = 10\n","                      )\n","f_df = pd.DataFrame(lgb_model.feature_importance(importance_type=\"gain\"), index=feature_cols).sort_values(by=0, ascending=False)\n","\n","\n","val_data_df[\"pred\"] = lgb_model.predict(val_data_df[feature_cols])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRd48MR-ybLk"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T09:33:14.392807Z","start_time":"2022-07-01T09:29:30.877724Z"},"id":"fbPFYT1RybLk"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import warnings\n","from heapq import heappop, heappush\n","from collections import defaultdict\n","from tqdm.notebook import tqdm\n","warnings.simplefilter('ignore')\n","\n","max_dist = 3\n","max_N = 64\n","eval_points = val_data_df[\"query_ix\"].unique()\n","pids = test_df[\"pid\"].values\n","\n","best_score = 0\n","best_th = 0\n","\n","pred_df = val_data_df[[\"query_ix\", \"candidate_ix\", \"pred\", \"rank\"]]#.query(\"rank <= 16\")\n","\n","for th in tqdm(np.linspace(0.5, 0.9, 40)):\n","    tmp_df = pred_df[pred_df[\"pred\"] >= th][[\"query_ix\", \"candidate_ix\", \"pred\"]]\n","    uft = UnionFind(len(test_df))\n","\n","    for ix, nix in tmp_df[[\"query_ix\", \"candidate_ix\"]].values:\n","        uft.unite(ix, nix)\n","    group_members = defaultdict(list)\n","    group_size = defaultdict(int)\n","    group_map = {}\n","    for i in range(len(test_df)):\n","        group_members[uft.find(i)].append(i)\n","        group_size[uft.find(i)] = uft.size[uft.find(i)]\n","        group_map[i] = uft.find(i)\n","\n","    large_groups = set([k for k, v in group_size.items() if v > min(max_N, max_dist+1)])\n","\n","    tmp_df[\"group\"] = tmp_df[\"query_ix\"].map(group_map)\n","    tmp_df[\"size\"] = tmp_df[\"group\"].map(group_size)\n","    tmp_df[\"left\"] = np.minimum(tmp_df[\"query_ix\"], tmp_df[\"candidate_ix\"])\n","    tmp_df[\"right\"] = np.maximum(tmp_df[\"query_ix\"], tmp_df[\"candidate_ix\"])\n","    tmp_df = tmp_df.sort_values(by=\"pred\", ascending=False).drop_duplicates([\"left\", \"right\"])\n","\n","    graphs = {k: nx.Graph() for k in large_groups}\n","    for large_group in large_groups:\n","        for member in group_members[large_group]:\n","            graphs[large_group].add_node(member)\n","\n","    neighbors = defaultdict(list)\n","    for l, r, s in tmp_df[tmp_df[\"group\"].isin(large_groups)][[\"left\", \"right\", \"pred\"]].values:\n","        l, r = int(l), int(r)\n","        g = uft.find(l)\n","        neighbors[l].append((r, s))\n","        neighbors[r].append((l, s))\n","        graphs[g].add_edge(l, r)\n","\n","    shortest_paths = {g: {k: d for k, d in nx.all_pairs_shortest_path_length(graphs[g])} for g in large_groups if len(group_members[g]) < 100}\n","\n","    scores = []\n","    for i in eval_points:\n","        true_count = test_df.at[i, \"true_count\"]\n","        qp = pids[i]\n","        g = uft.find(i)\n","        preds = []\n","        if g in large_groups:\n","            if g in shortest_paths.keys():\n","                preds = []\n","                for n, d in shortest_paths[g][i].items():\n","                    if d <= max_dist:\n","                        preds.append(n)\n","            if (len(preds) > max_N) or (not g in shortest_paths.keys()):\n","                searched = set()\n","                heapq = [(-1., 0, i)]\n","                while len(heapq) and (len(searched) < max_N):\n","                    _, step, x = heappop(heapq)\n","                    if x in searched:\n","                        continue\n","                    searched.add(x)\n","                    if step >= max_dist:\n","                        continue\n","                    for n, s in neighbors[x]:\n","                        if n in searched:\n","                            continue\n","                        heappush(heapq, (-s, step+1, n))\n","                preds = list(searched)\n","        else:\n","            preds = group_members[g]\n","        correct = sum([qp == pids[j] for j in preds])\n","        scores.append(correct / (true_count + len(preds) - correct))\n","\n","    score = np.mean(scores)\n","    if score > best_score:\n","        best_score = score\n","        best_th = th\n","print(best_th, best_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajOp5DoUybLk"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T09:34:48.211601Z","start_time":"2022-07-01T09:34:47.360609Z"},"id":"-AE1v-wkybLk"},"outputs":[],"source":["#lgb_model.save_model(nadare_feature_dir + \"lgb_model_mod0.lgb\")\n","import seaborn as sns\n","\n","plt.figure(figsize=(20, 30))\n","sns.barplot(x=0, y=\"index\", data=f_df.reset_index())\n","#plt.savefig(nadare_feature_dir + \"feature_importances_mod0.png\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T06:51:52.561550Z","start_time":"2022-07-01T05:11:03.004380Z"},"id":"QPyrUMoqybLl"},"outputs":[],"source":["import lightgbm as lgb\n","\n","lgb_params = {\n","    \"objective\" : \"binary\",\n","    \"metric\" : \"auc\",\n","    \"boosting\": 'gbdt',\n","    \"max_depth\" : -1,\n","    \"num_leaves\" : 2**11 - 1,\n","    \"learning_rate\" : 0.1,\n","    \"bagging_freq\": 1,\n","     \"bin_construct_sample_cnt\": 200000,\n","    \"lambda_l1\": 1.,\n","    \"lambda_l2\": 1.,\n","    \"bagging_fraction\" : 0.9,\n","    \"feature_fraction\" : 0.6,\n","    \"seed\": 0}\n","\n","dev_data = lgb.Dataset(dev_data_df[feature_cols], label=dev_data_df[\"label\"].astype(np.float32), weight=dev_data_df[\"weight\"].values)\n","val_data = lgb.Dataset(val_data_df[feature_cols], label=val_data_df[\"label\"].astype(np.float32))\n","\n","lgb_model = lgb.train(lgb_params,\n","                      dev_data,\n","                      100000,\n","                      valid_sets = [dev_data, val_data],\n","                      early_stopping_rounds=10,\n","                      verbose_eval = 10\n","                      )\n","f_df = pd.DataFrame(lgb_model.feature_importance(importance_type=\"gain\"), index=feature_cols).sort_values(by=0, ascending=False)\n","\n","\n","val_data_df[\"pred\"] = lgb_model.predict(val_data_df[feature_cols])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T06:55:04.773409Z","start_time":"2022-07-01T06:51:52.566549Z"},"id":"CNuq046DybLl"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import warnings\n","from heapq import heappop, heappush\n","from collections import defaultdict\n","from tqdm.notebook import tqdm\n","warnings.simplefilter('ignore')\n","\n","max_dist = 3\n","max_N = 64\n","eval_points = val_data_df[\"query_ix\"].unique()\n","pids = test_df[\"pid\"].values\n","\n","best_score = 0\n","best_th = 0\n","\n","pred_df = val_data_df[[\"query_ix\", \"candidate_ix\", \"pred\", \"rank\"]]#.query(\"rank <= 16\")\n","\n","for th in tqdm(np.linspace(0.5, 0.9, 40)):\n","    tmp_df = pred_df[pred_df[\"pred\"] >= th][[\"query_ix\", \"candidate_ix\", \"pred\"]]\n","    uft = UnionFind(len(test_df))\n","\n","    for ix, nix in tmp_df[[\"query_ix\", \"candidate_ix\"]].values:\n","        uft.unite(ix, nix)\n","    group_members = defaultdict(list)\n","    group_size = defaultdict(int)\n","    group_map = {}\n","    for i in range(len(test_df)):\n","        group_members[uft.find(i)].append(i)\n","        group_size[uft.find(i)] = uft.size[uft.find(i)]\n","        group_map[i] = uft.find(i)\n","\n","    large_groups = set([k for k, v in group_size.items() if v > min(max_N, max_dist+1)])\n","\n","    tmp_df[\"group\"] = tmp_df[\"query_ix\"].map(group_map)\n","    tmp_df[\"size\"] = tmp_df[\"group\"].map(group_size)\n","    tmp_df[\"left\"] = np.minimum(tmp_df[\"query_ix\"], tmp_df[\"candidate_ix\"])\n","    tmp_df[\"right\"] = np.maximum(tmp_df[\"query_ix\"], tmp_df[\"candidate_ix\"])\n","    tmp_df = tmp_df.sort_values(by=\"pred\", ascending=False).drop_duplicates([\"left\", \"right\"])\n","\n","    graphs = {k: nx.Graph() for k in large_groups}\n","    for large_group in large_groups:\n","        for member in group_members[large_group]:\n","            graphs[large_group].add_node(member)\n","\n","    neighbors = defaultdict(list)\n","    for l, r, s in tmp_df[tmp_df[\"group\"].isin(large_groups)][[\"left\", \"right\", \"pred\"]].values:\n","        l, r = int(l), int(r)\n","        g = uft.find(l)\n","        neighbors[l].append((r, s))\n","        neighbors[r].append((l, s))\n","        graphs[g].add_edge(l, r)\n","\n","    shortest_paths = {g: {k: d for k, d in nx.all_pairs_shortest_path_length(graphs[g])} for g in large_groups if len(group_members[g]) < 100}\n","\n","    scores = []\n","    for i in eval_points:\n","        true_count = test_df.at[i, \"true_count\"]\n","        qp = pids[i]\n","        g = uft.find(i)\n","        preds = []\n","        if g in large_groups:\n","            if g in shortest_paths.keys():\n","                preds = []\n","                for n, d in shortest_paths[g][i].items():\n","                    if d <= max_dist:\n","                        preds.append(n)\n","            if (len(preds) > max_N) or (not g in shortest_paths.keys()):\n","                searched = set()\n","                heapq = [(-1., 0, i)]\n","                while len(heapq) and (len(searched) < max_N):\n","                    _, step, x = heappop(heapq)\n","                    if x in searched:\n","                        continue\n","                    searched.add(x)\n","                    if step >= max_dist:\n","                        continue\n","                    for n, s in neighbors[x]:\n","                        if n in searched:\n","                            continue\n","                        heappush(heapq, (-s, step+1, n))\n","                preds = list(searched)\n","        else:\n","            preds = group_members[g]\n","        correct = sum([qp == pids[j] for j in preds])\n","        scores.append(correct / (true_count + len(preds) - correct))\n","\n","    score = np.mean(scores)\n","    if score > best_score:\n","        best_score = score\n","        best_th = th\n","print(best_th, best_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T06:56:39.203500Z","start_time":"2022-07-01T06:56:37.321089Z"},"id":"0SiSgEtsybLl"},"outputs":[],"source":["lgb_model.save_model(nadare_feature_dir + \"lgb_model_mod0.lgb\")\n","import seaborn as sns\n","\n","plt.figure(figsize=(20, 30))\n","sns.barplot(x=0, y=\"index\", data=f_df.reset_index())\n","plt.savefig(nadare_feature_dir + \"feature_importances_mod0.png\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkT0pM8vybLl"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T07:51:57.311147Z","start_time":"2022-07-01T07:51:51.663164Z"},"id":"ZR14lu8dybLl"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T07:54:14.460665Z","start_time":"2022-07-01T07:54:14.240615Z"},"id":"r3IkqIUTybLl"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T07:56:49.804244Z","start_time":"2022-07-01T07:56:49.759152Z"},"id":"Zoa6RH3oybLl"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:28:50.674954Z","start_time":"2022-07-01T08:28:50.647955Z"},"id":"MxW0vayBybLl"},"outputs":[],"source":["@tf.function(experimental_relax_shapes=True)\n","def calc_rouge_1_2(q_ix, c_ix):\n","    q_code = tf.gather(name_bow_rag_col, q_ix).to_tensor(default_value=-1)\n","    c_code = tf.gather(name_bow_rag_col, c_ix).to_tensor(default_value=-2)\n","    q_cnt = tf.cast(tf.gather(name_bow_rag_col.row_lengths(), q_ix), \"float32\")\n","    c_cnt = tf.cast(tf.gather(name_bow_rag_col.row_lengths(), c_ix), \"float32\")\n","\n","    is_na = tf.minimum(q_cnt, c_cnt) == 0.\n","    match = tf.expand_dims(q_code, axis=2) == tf.expand_dims(c_code, axis=1)\n","    precision = tf.math.count_nonzero(tf.math.reduce_any(match, axis=2), axis=1, dtype=\"float32\") / (q_cnt + tf.keras.backend.epsilon())\n","    recall = tf.math.count_nonzero(tf.math.reduce_any(match, axis=1), axis=1, dtype=\"float32\") / (c_cnt + tf.keras.backend.epsilon())\n","    harmony = 2. / ((1 / (precision + tf.keras.backend.epsilon())) + (1 / (recall + tf.keras.backend.epsilon())))\n","    res_1 = tf.where(tf.expand_dims(is_na, axis=-1), np.nan, tf.stack([harmony, precision, recall], axis=1))\n","    \n","    q_code = tf.gather(bi_name_bow_rag_col, q_ix).to_tensor(default_value=-1)\n","    c_code = tf.gather(bi_name_bow_rag_col, c_ix).to_tensor(default_value=-2)\n","    q_cnt = tf.cast(tf.gather(bi_name_bow_rag_col.row_lengths(), q_ix), \"float32\")\n","    c_cnt = tf.cast(tf.gather(bi_name_bow_rag_col.row_lengths(), c_ix), \"float32\")\n","\n","    is_na = tf.minimum(q_cnt, c_cnt) == 0.\n","    match = tf.expand_dims(q_code, axis=2) == tf.expand_dims(c_code, axis=1)\n","    precision = tf.math.count_nonzero(tf.math.reduce_any(match, axis=2), axis=1, dtype=\"float32\") / (q_cnt + tf.keras.backend.epsilon())\n","    recall = tf.math.count_nonzero(tf.math.reduce_any(match, axis=1), axis=1, dtype=\"float32\") / (c_cnt + tf.keras.backend.epsilon())\n","    harmony = 2. / ((1 / (precision + tf.keras.backend.epsilon())) + (1 / (recall + tf.keras.backend.epsilon())))\n","    res_2 = tf.where(tf.expand_dims(is_na, axis=-1), np.nan, tf.stack([harmony, precision, recall], axis=1))\n","    \n","    return tf.concat([res_1, res_2], axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:29:08.280698Z","start_time":"2022-07-01T08:29:05.457700Z"},"id":"wDjZoc45ybLl"},"outputs":[],"source":["%%timeit\n","with tf.device(\"CPU: 0\"):\n","    calc_rouge_1_2(q_ix, c_ix)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:28:12.887449Z","start_time":"2022-07-01T08:28:12.564870Z"},"id":"E5zaQPdcybLm"},"outputs":[],"source":["calc_rouge_1_2(q_ix, c_ix)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:12:33.815562Z","start_time":"2022-07-01T08:12:33.794607Z"},"id":"NuAE_2SkybLm"},"outputs":[],"source":["harmony"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-07-01T08:13:11.171675Z","start_time":"2022-07-01T08:13:11.157676Z"},"id":"SmI-FzXJybLm"},"outputs":[],"source":["tf.minimum(precision, recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUNcuTqxybLm"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"PRED23 256dim.ipynb","provenance":[],"machine_shape":"hm","background_execution":"on"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}