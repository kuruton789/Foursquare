{"cells":[{"cell_type":"code","execution_count":1,"id":"fed2bfb2","metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:13:40.513760Z","start_time":"2022-07-01T03:13:40.499759Z"},"execution":{"iopub.execute_input":"2022-05-04T10:20:01.246118Z","iopub.status.busy":"2022-05-04T10:20:01.245189Z","iopub.status.idle":"2022-05-04T10:20:54.771970Z","shell.execute_reply":"2022-05-04T10:20:54.771203Z"},"papermill":{"duration":53.552676,"end_time":"2022-05-04T10:20:54.774628","exception":false,"start_time":"2022-05-04T10:20:01.221952","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":389},"id":"fed2bfb2","executionInfo":{"status":"error","timestamp":1656673836440,"user_tz":-540,"elapsed":4692,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"0d83d919-8ba6-4948-d198-be7108700213"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-bd270559437a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as tf_text\n","\n","from tqdm.notebook import tqdm\n","import re\n","\n","import gc"]},{"cell_type":"code","execution_count":null,"id":"c8d84c03","metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:13:40.529759Z","start_time":"2022-07-01T03:13:40.514760Z"},"id":"c8d84c03","executionInfo":{"status":"aborted","timestamp":1656673836434,"user_tz":-540,"elapsed":16,"user":{"displayName":"クルトン","userId":"17231302919530674825"}}},"outputs":[],"source":["nadare_feature_dir = \"../feature/FEAT26/\"\n","nadare_util_dir = \"../feature/common_utils/\"\n","import os\n","os.makedirs(nadare_feature_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"id":"c9802d19","metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:13:43.203805Z","start_time":"2022-07-01T03:13:40.530758Z"},"execution":{"iopub.execute_input":"2022-05-04T10:20:54.823348Z","iopub.status.busy":"2022-05-04T10:20:54.823071Z","iopub.status.idle":"2022-05-04T10:21:02.524500Z","shell.execute_reply":"2022-05-04T10:21:02.523797Z"},"papermill":{"duration":7.72778,"end_time":"2022-05-04T10:21:02.526592","exception":false,"start_time":"2022-05-04T10:20:54.798812","status":"completed"},"tags":[],"id":"c9802d19","executionInfo":{"status":"aborted","timestamp":1656673836438,"user_tz":-540,"elapsed":19,"user":{"displayName":"クルトン","userId":"17231302919530674825"}}},"outputs":[],"source":["train_df = pd.read_csv(\"../input/foursquare-location-matching/train.csv\", encoding=\"utf-8\")\n","test_df = pd.read_csv(\"../input/foursquare-location-matching/test.csv\", encoding=\"utf-8\")\n","#test_df.loc[0, \"name\"] = np.NaN"]},{"cell_type":"code","execution_count":null,"id":"ba388054","metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:15:07.456353Z","start_time":"2022-07-01T03:15:06.180354Z"},"execution":{"iopub.execute_input":"2022-05-04T10:21:28.912353Z","iopub.status.busy":"2022-05-04T10:21:28.912101Z","iopub.status.idle":"2022-05-04T10:21:31.816511Z","shell.execute_reply":"2022-05-04T10:21:31.815789Z"},"papermill":{"duration":2.936041,"end_time":"2022-05-04T10:21:31.818613","exception":false,"start_time":"2022-05-04T10:21:28.882572","status":"completed"},"tags":[],"id":"ba388054"},"outputs":[],"source":["train_id_map = {v: i for i, v in enumerate(train_df[\"id\"].values)}\n","train_df[\"ix\"] = train_df[\"id\"].map(train_id_map)\n","train_df[\"categories\"] = train_df[\"categories\"].fillna(\"nan\")\n","train_df[\"pid\"] = train_df[\"point_of_interest\"].map({v: i for i, v in enumerate(train_df[\"point_of_interest\"].unique())})\n","\n","test_id_map = {v: i for i, v in enumerate(test_df[\"id\"].values)}\n","test_df[\"ix\"] = test_df[\"id\"].map(test_id_map)\n","test_df[\"categories\"] = test_df[\"categories\"].fillna(\"nan\")\n","test_df[\"pid\"] = -1"]},{"cell_type":"code","execution_count":null,"id":"3814550b","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:03:22.181258Z","start_time":"2022-06-30T21:03:21.831259Z"},"id":"3814550b"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","group = -np.ones(len(train_df), dtype=\"int32\")\n","g = 0\n","for g, (dev_p_ids, val_p_ids) in enumerate(KFold(n_splits=5, shuffle=True, random_state=42).split(train_df[\"pid\"].unique())):\n","    ix = train_df[\"pid\"].isin(val_p_ids)\n","    group[ix] = g\n","train_df[\"group\"] = group\n","test_df[\"group\"] = -1"]},{"cell_type":"code","execution_count":null,"id":"47fb3a97","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:03:25.311322Z","start_time":"2022-06-30T21:03:22.182260Z"},"id":"47fb3a97"},"outputs":[],"source":["def get_value_with_default(dict_, value, default=0):\n","    return np.vectorize(lambda x: dict_.get(x, default))(value)\n","\n","def make_category_dict(df, category_threthold = 1):\n","    cat_pairs = []\n","    all_category = set()\n","    for row in df[\"categories\"].drop_duplicates().dropna().values:\n","        for cat in row.split(\", \"):\n","            if len(cat):\n","                cat_pairs.append([row, cat])\n","                all_category.add(cat)\n","\n","    category_df = pd.DataFrame(cat_pairs, columns=[\"categories\", \"category\"])\n","    categories_df = df[[\"ix\", \"categories\"]].merge(category_df, on=\"categories\", how=\"left\").sort_values(by=\"ix\")\n","    \n","    category_df = category_df.merge(categories_df.groupby(\"category\")[\"ix\"].count().reset_index().rename({\"ix\": \"category_appear_count\"}, axis=1),\n","                                    on=\"category\",\n","                                    how=\"left\")\n","\n","    # if category doesn`t appear over threthold, it defines as out of vocab\n","    \n","    category_ix_df = category_df.drop_duplicates(\"category\").sort_values(by=\"category_appear_count\", ascending=False)[[\"category\", \"category_appear_count\"]]\n","    category_ix_df[\"category_ix\"] = np.where(category_ix_df[\"category_appear_count\"] >= category_threthold, np.arange(len(category_ix_df)), -1)\n","    category_ix_df = category_ix_df.sort_values(by=[\"category_appear_count\"], ascending=[False])\n","    category_ix_dict = pd.Series(index=category_ix_df[\"category\"].values, data=category_ix_df[\"category_ix\"].values).to_dict()\n","    #skip_nan_value\n","    return category_ix_dict\n","\n","def get_category_ix(df, category_ix_dict):\n","    cat_pairs = []\n","    all_category = set()\n","    for row in df[\"categories\"].drop_duplicates().dropna().values:\n","        for cat in row.split(\", \"):\n","            if len(cat):\n","                cat_pairs.append([row, cat])\n","                all_category.add(cat)\n","\n","    category_df = pd.DataFrame(cat_pairs, columns=[\"categories\", \"category\"])\n","    categories_df = df[[\"ix\", \"categories\"]].merge(category_df, on=\"categories\", how=\"left\").sort_values(by=\"ix\")\n","\n","    categories_df[\"category_ix\"] = get_value_with_default(category_ix_dict, categories_df[\"category\"].values, -1)\n","    categories_df = categories_df[categories_df[\"category_ix\"] >= 0]\n","    categories_ix = tf.RaggedTensor.from_value_rowids(values=tf.constant(categories_df[\"category_ix\"].values, \"int32\"),\n","                                                              value_rowids=tf.constant(categories_df[\"ix\"].values, \"int32\"),\n","                                                              nrows=len(df))\n","    return categories_ix\n","\n","def remove_test_only_category(text):\n","    res = []\n","    for x in text.split(\", \"):\n","        if x in category_ix_dict.keys():\n","            res.append(x)\n","    return \", \".join(res)\n","\n","\n","category_ix_dict = make_category_dict(train_df, 1)\n","train_df[\"categories\"] = np.vectorize(remove_test_only_category)(train_df[\"categories\"])\n","train_categories_ix = get_category_ix(train_df, category_ix_dict)"]},{"cell_type":"code","execution_count":null,"id":"1556499a","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:03:58.587774Z","start_time":"2022-06-30T21:03:25.312324Z"},"id":"1556499a"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","geocode_df = pd.read_csv(nadare_util_dir + \"geocode.csv\", header=None)\n","geocode_df.columns = [\"latitude\", \"longitude\", \"country\", \"geo_city\"]\n","geocode_df[\"geo_name\"] = geocode_df[\"country\"] + \"_\" + geocode_df[\"geo_city\"]\n","geocode_df = geocode_df.groupby(\"geo_name\")[[\"latitude\", \"longitude\"]].mean().reset_index()\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(geocode_df[['latitude','longitude']]), geocode_df.index)\n","geo_index = knn.kneighbors(np.deg2rad(train_df[['latitude','longitude']]))[1].T[0]"]},{"cell_type":"code","execution_count":null,"id":"deb190e1","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:03:58.619553Z","start_time":"2022-06-30T21:03:58.588776Z"},"id":"deb190e1"},"outputs":[],"source":["train_df[\"geo_name\"] = geocode_df[\"geo_name\"].values[geo_index]"]},{"cell_type":"code","execution_count":null,"id":"e337543e","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:03:58.683552Z","start_time":"2022-06-30T21:03:58.620554Z"},"id":"e337543e"},"outputs":[],"source":["geo_name_vc = train_df[\"geo_name\"].value_counts().head(253*8)\n","geo_name_index_df = geo_name_vc.reset_index()\n","geo_name_index_df.columns = [\"geo_name\", \"geo_name_count\"]\n","geo_name_index_df[\"geo_name_index\"] = np.arange(len(geo_name_index_df))\n","geo_name_index_map = {geo_name: i for i, geo_name in enumerate(geo_name_index_df[\"geo_name\"].values)}\n","geo_name_index_df = geo_name_index_df.merge(geocode_df[[\"geo_name\", \"latitude\", \"longitude\"]], on=\"geo_name\", how=\"left\")\n","#geo_name_index_df = geo_name_index_df.merge(geo_name_position_df, on=\"geo_name\", how=\"left\")"]},{"cell_type":"code","execution_count":null,"id":"926190c2","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:04:05.179116Z","start_time":"2022-06-30T21:03:58.684554Z"},"id":"926190c2"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(geo_name_index_df[['latitude','longitude']]), geo_name_index_df[\"geo_name_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(train_df[['latitude','longitude']]))[1].T[0]\n","train_df[\"pseudo_geo_name\"] = np.where(train_df[\"geo_name\"].isin(geo_name_index_df[\"geo_name\"].values), train_df[\"geo_name\"].values, geo_name_index_df[\"geo_name\"][pseudo_index].values)\n","train_df[\"pseudo_geo_name_ix\"] = train_df[\"pseudo_geo_name\"].map(geo_name_index_map)"]},{"cell_type":"code","execution_count":null,"id":"0c983ec4","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:04:05.622116Z","start_time":"2022-06-30T21:04:05.180118Z"},"id":"0c983ec4"},"outputs":[],"source":["city_vc = train_df[\"city\"].value_counts().head(253*8)\n","city_index_df = city_vc.reset_index()\n","city_index_df.columns = [\"city\", \"city_count\"]\n","city_index_df[\"city_index\"] = np.arange(len(city_index_df))\n","city_index_map = {city: i for i, city in enumerate(city_index_df[\"city\"].values)}\n","city_position_df = train_df.groupby(\"city\")[[\"latitude\", \"longitude\"]].mean().reset_index()\n","city_index_df = city_index_df.merge(city_position_df, on=\"city\", how=\"left\")"]},{"cell_type":"code","execution_count":null,"id":"a4dd9ab7","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:04:11.974720Z","start_time":"2022-06-30T21:04:05.623117Z"},"id":"a4dd9ab7"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(city_index_df[['latitude','longitude']]), city_index_df[\"city_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(train_df[['latitude','longitude']]))[1].T[0]\n","train_df[\"pseudo_city\"] = np.where(train_df[\"city\"].isin(city_index_df[\"city\"].values), train_df[\"city\"].values, city_index_df[\"city\"][pseudo_index].values)\n","train_df[\"pseudo_city_ix\"] = train_df[\"pseudo_city\"].map(city_index_map)"]},{"cell_type":"code","execution_count":null,"id":"82e2d8f6","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:04:12.324229Z","start_time":"2022-06-30T21:04:11.975225Z"},"id":"82e2d8f6"},"outputs":[],"source":["state_index_df = train_df[\"state\"].value_counts().head(253*8).reset_index()\n","state_index_df.columns = [\"state\", \"state_count\"]\n","state_index_df[\"state_index\"] = np.arange(len(state_index_df))\n","state_index_map = {state: i for i, state in enumerate(state_index_df[\"state\"].values)}\n","state_position_df = train_df.groupby(\"state\")[[\"latitude\", \"longitude\"]].mean().reset_index()\n","state_index_df = state_index_df.merge(state_position_df, on=\"state\", how=\"left\")"]},{"cell_type":"code","execution_count":null,"id":"a1051e90","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:04:19.174769Z","start_time":"2022-06-30T21:04:12.325231Z"},"id":"a1051e90"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","knn = KNeighborsRegressor(n_neighbors=1, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(state_index_df[['latitude','longitude']]), state_index_df[\"state_index\"])\n","pseudo_index = knn.kneighbors(np.deg2rad(train_df[['latitude','longitude']]))[1].T[0]\n","train_df[\"pseudo_state\"] = np.where(train_df[\"state\"].isin(state_index_df[\"state\"].values), train_df[\"state\"].values, state_index_df[\"state\"][pseudo_index].values)\n","train_df[\"pseudo_state_ix\"] = train_df[\"pseudo_state\"].map(state_index_map)"]},{"cell_type":"code","execution_count":null,"id":"a16c7b42","metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:15:14.483371Z","start_time":"2022-07-01T03:15:14.319369Z"},"id":"a16c7b42"},"outputs":[],"source":["train_df[\"country\"] = train_df[\"country\"].fillna(\"NAN\")\n","country_index_df = train_df[\"country\"].value_counts().reset_index()\n","country_index_df.columns = [\"country\", \"country_count\"]\n","country_index_df[\"country_index\"] = np.arange(len(country_index_df))\n","country_index_map = {country: i for i, country in enumerate(country_index_df[\"country\"].values, start=1)} # zero as nan\n","train_df[\"country_ix\"] = train_df[\"country\"].map(country_index_map)"]},{"cell_type":"code","execution_count":null,"id":"3485550a","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:04:33.733563Z","start_time":"2022-06-30T21:04:19.366775Z"},"id":"3485550a"},"outputs":[],"source":["from fasttext import load_model\n","ft_model = load_model(nadare_util_dir + \"lid.176.bin\")\n","def predict_language(text):\n","    label, prob = ft_model.predict(text, 1)\n","    return list(zip([l.replace(\"__label__\", \"\") for l in label], prob))[0][0]\n","\n","train_df[\"name_language\"] = np.vectorize(predict_language)(train_df[\"name\"].fillna(\"\"))\n","train_df[\"address_language\"] = np.vectorize(predict_language)(train_df[\"address\"].fillna(\"\"))\n","\n","def language_pattern(country, language):\n","    if country == \"JP\":\n","        return 0\n","    elif country == \"TH\":\n","        return 1\n","    elif country == \"CN\":\n","        return 2\n","    elif language == \"ja\":\n","        return 0\n","    elif language == \"th\":\n","        return 1\n","    elif language == \"zh\":\n","        return 2\n","    else:\n","        return 3\n","train_df[\"name_handle_pattern\"] = np.vectorize(language_pattern)(train_df[\"country\"], train_df[\"name_language\"])     \n","train_df[\"address_handle_pattern\"] = np.vectorize(language_pattern)(train_df[\"country\"], train_df[\"address_language\"])\n","lang_vc = train_df[\"name_language\"].value_counts()\n","del ft_model"]},{"cell_type":"code","execution_count":null,"id":"d325f467","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:04:35.680563Z","start_time":"2022-06-30T21:04:33.734566Z"},"colab":{"referenced_widgets":["5236e83e91b3481e81ee4bd7813f8294"]},"id":"d325f467","outputId":"d1be3988-151d-4200-c79f-dc69e3dbe765"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5236e83e91b3481e81ee4bd7813f8294","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from num2words import num2words\n","from collections import defaultdict\n","\n","num2words_languages = [\"ar\", \"cz\", \"en\", \"fr\", \"de\", \"fi\", \"es\", \"id\", \"kn\", \"ko\", \"kz\", \"lt\", \"lv\", \"pl\", \"ro\", \"ru\", \"sl\",\n","                       \"sr\", \"sv\", \"no\", \"dk\", \"pt\", \"he\", \"it\", \"vi\", \"th\", \"tr\", \"nl\", \"uk\", \"te\", \"hu\"]\n","\n","word_number_df = []\n","for lang in tqdm(num2words_languages):\n","    for i in range(1001):\n","        try:\n","            word = num2words(i, lang=lang)\n","        except:\n","            word = np.NaN\n","        try:\n","            ordinal = num2words(i, lang=lang, to=\"ordinal\")\n","        except:\n","            ordinal = np.NaN\n","        try:\n","            ordinal_num = num2words(i, lang=lang, to=\"ordinal_num\")\n","        except:\n","            ordinal_num = np.NaN\n","        if (pd.isna(ordinal)) or (pd.isna(ordinal_num)) or (ordinal == ordinal_num):\n","            ordinal_num = i\n","        if lang == \"ko\" and not pd.isna(ordinal):\n","            ordinal = ordinal[:-3]\n","            ordinal_num = ordinal_num[:-3] \n","        \n","        word_number_df.append({\"lang\": lang,\n","                               \"number\": i,\n","                               \"word\": word,\n","                               \"ordinal\": ordinal,\n","                               \"ordinal_num\": ordinal_num})\n","\n","word_number_df = pd.DataFrame(word_number_df)\n","word_number_df[\"lang_count\"] = word_number_df[\"lang\"].apply(lambda x: lang_vc.get(x, 0))\n","\n","word2num_dict = {}\n","possibly_have_next_dict = {}\n","\n","for i, row in word_number_df.sort_values(by=\"lang_count\").iterrows():\n","    if not pd.isna(row[\"word\"]):\n","        word = row[\"word\"]\n","        if not word in possibly_have_next_dict.keys():\n","            possibly_have_next_dict[word] = False\n","\n","        word2num_dict[word] = str(row[\"number\"])\n","        word2num_dict[word.replace(\"-\", \" \")] = str(row[\"number\"])\n","        \n","        wsp = word.split()\n","        for i in range(1, len(wsp)-1):\n","            possibly_have_next_dict[\" \".join(wsp[:i])] = True\n","        wsp = word.replace(\"-\", \" \").split()[:-1]\n","        for i in range(1, len(wsp)-1):\n","            possibly_have_next_dict[\" \".join(wsp[:i])] = True\n","        \n","    if (not pd.isna(row[\"ordinal\"])) and (not pd.isna(row[\"ordinal_num\"])):\n","        word = row[\"ordinal\"]\n","        if not word in possibly_have_next_dict.keys():\n","            possibly_have_next_dict[word] = False\n","        word2num_dict[word] = str(row[\"ordinal_num\"])\n","        word2num_dict[word.replace(\"-\", \" \")] = str(row[\"ordinal_num\"])\n","        \n","        wsp = word.split()\n","        for i in range(1, len(wsp)-1):\n","            possibly_have_next_dict[\" \".join(wsp[:i])] = True\n","        wsp = word.replace(\"-\", \" \").split()[:-1]\n","        for i in range(1, len(wsp)-1):\n","            possibly_have_next_dict[\" \".join(wsp[:i])] = True\n","        "]},{"cell_type":"code","execution_count":null,"id":"6c193154","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:04:37.877180Z","start_time":"2022-06-30T21:04:35.681565Z"},"id":"6c193154","outputId":"815d935d-a68b-4c6f-9f58-8fa17c0d099a"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:22: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:23: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:24: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:25: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  conversion = kakasi.getConverter()\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:28: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  kakasi.setMode('H', 'K')  # Convert Hiragana into alphabet\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:29: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  kakasi.setMode('K', 'K')  # Convert Katakana into alphabet\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:30: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  force_katakana.setMode('J', 'K')  # Convert Hiragana into alphabet\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:31: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  force_katakana_conversion = force_katakana.getConverter()\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:33: DeprecationWarning: Parameter dict_type of Dictionary() is deprecated, use dict instead\n","  sudachi_tokenizer = Dictionary(dict_type=\"full\").create()\n"]}],"source":["import string\n","import pycnnum\n","import pythainlp\n","from pythainlp.util import thai_digit_to_arabic_digit\n","from sudachipy import Dictionary, SplitMode\n","import mojimoji\n","import pykakasi\n","from unidecode import unidecode\n","import emoji\n","\n","emoji_set = set()\n","for k, v in emoji.UNICODE_EMOJI.items():\n","    emoji_set.update(v.keys())\n","thai_word2num_dict = {num2words(i, lang=\"th\"):str(i) for i in range(10001)}\n","CHINESE_DIGITS_PATTERN = re.compile(r'[〇〇零一二三四五六七八九零壹贰叁肆伍陆柒捌玖十百千万拾佰仟萬]+')\n","remove_punctuations = '!\"#$%\\'()*+,./:;<=>?@[\\\\]^_`{|}~'\n","punctuation_translater = str.maketrans(remove_punctuations, \" \"*len(remove_punctuations))\n","japanese_char2num_translater = str.maketrans(\"〇零一壱二弐三参四肆五伍六陸七漆八捌九玖\", \"00112233445566778899\")\n","\n","\n","kakasi = pykakasi.kakasi()\n","kakasi.setMode('H', 'a')  # Convert Hiragana into alphabet\n","kakasi.setMode('K', 'a')  # Convert Katakana into alphabet\n","kakasi.setMode('J', 'a')  # Convert Kanji into alphabet\n","conversion = kakasi.getConverter()\n","\n","force_katakana = pykakasi.kakasi()\n","kakasi.setMode('H', 'K')  # Convert Hiragana into alphabet\n","kakasi.setMode('K', 'K')  # Convert Katakana into alphabet\n","force_katakana.setMode('J', 'K')  # Convert Hiragana into alphabet\n","force_katakana_conversion = force_katakana.getConverter()\n","\n","sudachi_tokenizer = Dictionary(dict_type=\"full\").create()\n","sudachi_tokenize_mode = SplitMode.A\n","\n","zh_segmenter = tf_text.HubModuleTokenizer(nadare_util_dir + \"zh_segmentation\")\n","\n","def remove_emoji(x):\n","    return ''.join([c for c in x if c not in emoji_set])\n","\n","def remove_marks(x):\n","    x = re.sub(r'[^a-zA-Z0-9& ]', ' ', x)\n","    x = re.sub(r' +', ' ', x)\n","    return x\n","\n","def zenhan_normalize(x):\n","    return mojimoji.han_to_zen(mojimoji.zen_to_han(x, kana=False), digit=False, ascii=False)\n","\n","def japanese_wakachi_reading(morphemes):\n","    readings = []\n","    for m in morphemes:\n","        if (m.normalized_form() == \" \") or (m.reading_form() == \"キゴウ\"):\n","            continue\n","        if m.normalized_form().endswith(\"丁目\") and m.normalized_form() != \"丁目\":\n","            norm = m.normalized_form().translate(japanese_char2num_translater)\n","            norm = re.sub(r\"(\\d+)\", r\" \\1 \", \"6丁目\")\n","            rs = japanese_wakachi_reading(sudachi_tokenizer.tokenize(norm, sudachi_tokenize_mode))\n","            readings.extend(rs)\n","        elif m.normalized_form().isdigit():\n","            readings.append(m.normalized_form())\n","        else:\n","            readings.append(force_katakana_conversion.do(m.reading_form()))\n","    return readings\n","\n","def handle_japanese(x):\n","    x = zenhan_normalize(x)\n","    morphemes = sudachi_tokenizer.tokenize(x, sudachi_tokenize_mode)\n","    reading_forms = japanese_wakachi_reading(morphemes)\n","    romanize_forms = conversion.do(\" \".join(reading_forms))\n","    return \" \".join(reading_forms), romanize_forms\n","\n","def decode_list(x):\n","    if type(x) is list:\n","        return list(map(decode_list, x))\n","    return x.decode(\"UTF-8\")\n","\n","def decode_utf8_tensor(x):\n","    return list(map(decode_list, x.to_list()))\n","\n","def handle_chinese(x):\n","    x = x.replace(\"'\", \"\").translate(punctuation_translater).lower()\n","    if re.fullmatch(\"\\s*\", x):\n","        return \"\", \"\"\n","    with tf.device(\"CPU:0\"):\n","        words_list = decode_utf8_tensor(zh_segmenter.tokenize(x.split()))\n","        \n","    number_words = []\n","    romanized_words = []\n","    for words in words_list:\n","        num_word = []\n","        roman_word = []\n","        for word in words:\n","            if re.fullmatch(CHINESE_DIGITS_PATTERN, word):\n","                word = str(pycnnum.cn2num(word))\n","            if (word[0]) == \"第\" and re.fullmatch(CHINESE_DIGITS_PATTERN, word[1:]):\n","                num = str(pycnnum.cn2num(word[1:]))\n","                word = \"第\"\n","                num_word.append(word + num)\n","                roman_word.append(unidecode(word[0]).lower().replace(\" \", \"\") + \" \" + num)\n","            else:\n","                num_word.append(word)\n","                roman_word.append(unidecode(word).lower().replace(\" \", \"\"))\n","        number_words.append(\"\".join(num_word))\n","        romanized_words.append(\" \".join(roman_word))\n","    return \" \".join(number_words), \" \".join(romanized_words)\n","\n","def handle_thai(x):\n","    x = thai_digit_to_arabic_digit(x)\n","    tokenized_word = pythainlp.tokenize.word_tokenize(x, engine=\"newmm\")\n","    number_words = []\n","    romanized_words = []\n","    for word in tokenized_word:\n","        word = thai_word2num_dict.get(word, word)\n","        number_words.append(word)\n","        try:\n","            romanized_words.append(pythainlp.romanize(word, engine='royin'))\n","        except:\n","            pass\n","    return \"\".join(number_words), \" \".join(romanized_words)\n","\n","\n","def language_normalize(word):\n","    result = []\n","    word = word.replace(\"'\", \"\").translate(punctuation_translater).lower()\n","    tmp = \"\"\n","    for w in word.split():\n","        if len(tmp):\n","            tmp = tmp + \" \" + w\n","            if tmp in possibly_have_next_dict.keys():\n","                if possibly_have_next_dict[tmp]:\n","                    continue\n","                else:\n","                    result.append(word2num_dict[tmp])\n","                    tmp = \"\"\n","        else:\n","            if w in possibly_have_next_dict.keys():\n","                if possibly_have_next_dict[w]:\n","                    tmp = w\n","                    continue\n","                else:\n","                    result.append(word2num_dict[w])\n","            else:\n","                result.append(w)\n","    if len(tmp):\n","        result.append(word2num_dict.get(tmp, tmp))\n","            \n","    return re.sub(r' +', ' ', \" \".join(result))\n","\n","def language_handle(handle_pattern, text):\n","    if handle_pattern == 0:\n","        normalized, romanized = handle_japanese(text)\n","    elif handle_pattern == 1:\n","        normalized, romanized = handle_thai(text)\n","    elif handle_pattern == 2:\n","        normalized, romanized = handle_chinese(text)\n","    return normalized, romanized\n"]},{"cell_type":"code","execution_count":null,"id":"a2a19608","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:05:52.411521Z","start_time":"2022-06-30T21:04:37.878183Z"},"colab":{"referenced_widgets":["44fee99c94114dbebe7a9e92d3832521"]},"id":"a2a19608","outputId":"62d2e86e-e5c3-47e7-c166-180640367c46"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44fee99c94114dbebe7a9e92d3832521","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/150443 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:62: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  readings.append(force_katakana_conversion.do(m.reading_form()))\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:69: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  romanize_forms = conversion.do(\" \".join(reading_forms))\n"]}],"source":["name_handle_index = np.where((train_df[\"name_handle_pattern\"] <= 2) & (~train_df[\"name\"].isna()))[0]\n","\n","name_normalized = []\n","name_romanized = []\n","for pattern, name in tqdm(zip(train_df[\"name_handle_pattern\"].values[name_handle_index],\n","                              train_df[\"name\"].fillna(\"\").astype(str).values[name_handle_index]), total=len(name_handle_index)):\n","    normalized, romanized = language_handle(pattern, name)\n","    name_normalized.append(normalized)\n","    name_romanized.append(romanized)"]},{"cell_type":"code","execution_count":null,"id":"8190925c","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:06:31.220657Z","start_time":"2022-06-30T21:05:52.412521Z"},"colab":{"referenced_widgets":["5a860d4952834e2f8c8c3b328bb1d787"]},"id":"8190925c","outputId":"1e97612d-1e45-4904-a558-76dcf8b154c9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a860d4952834e2f8c8c3b328bb1d787","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/90478 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:62: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  readings.append(force_katakana_conversion.do(m.reading_form()))\n","C:\\Users\\nadare\\AppData\\Local\\Temp/ipykernel_32624/2681526445.py:69: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n","  romanize_forms = conversion.do(\" \".join(reading_forms))\n"]}],"source":["address_handle_index = np.where((train_df[\"address_handle_pattern\"] <= 2) & (~train_df[\"address\"].isna()))[0]\n","\n","address_normalized = []\n","address_romanized = []\n","for pattern, address in tqdm(zip(train_df[\"address_handle_pattern\"].values[address_handle_index],\n","                              train_df[\"address\"].fillna(\"\").astype(str).values[address_handle_index]), total=len(address_handle_index)):\n","    normalized, romanized = language_handle(pattern, address)\n","    address_normalized.append(normalized)\n","    address_romanized.append(romanized)"]},{"cell_type":"code","execution_count":null,"id":"359a7e32","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:06:31.631661Z","start_time":"2022-06-30T21:06:31.221660Z"},"id":"359a7e32","outputId":"eb6013c6-f0e5-4a38-fb64-eb310c0efa24"},"outputs":[{"data":{"text/plain":["10576"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del zh_segmenter, kakasi, conversion, sudachi_tokenizer\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"id":"caecc31a","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:06:45.695481Z","start_time":"2022-06-30T21:06:31.632661Z"},"id":"caecc31a"},"outputs":[],"source":["train_df[\"name_normalized\"] = np.vectorize(language_normalize)(train_df[\"name\"].fillna(\"\").astype(str))\n","train_df[\"address_normalized\"] = np.vectorize(language_normalize)(train_df[\"address\"].fillna(\"\").astype(str))\n","\n","\n","from_cols = [\"name_normalized\", \"address_normalized\"]\n","to_cols = ['sp_name', \"sp_address_raw\"]\n","for from_, to_ in zip(from_cols, to_cols):\n","    train_df[to_] = train_df[from_].fillna('').apply(unidecode)\n","    train_df[to_] = train_df[to_].str.lower()\n","    train_df[to_] = train_df[to_].apply(remove_emoji)\n","    train_df[to_] = train_df[to_].apply(remove_marks)\n","    \n","train_df.loc[name_handle_index, \"name_normalized\"] = name_normalized\n","train_df.loc[name_handle_index, \"sp_name\"] = name_romanized\n","\n","train_df.loc[address_handle_index, \"address_normalized\"] = address_normalized\n","train_df.loc[address_handle_index, \"sp_address_raw\"] = address_romanized"]},{"cell_type":"code","execution_count":null,"id":"64cd3d92","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:07:11.849518Z","start_time":"2022-06-30T21:06:45.696481Z"},"id":"64cd3d92"},"outputs":[],"source":["import sentencepiece as spm\n","\n","train_df[[\"sp_name\"]][~np.vectorize(lambda x: bool(re.fullmatch(\"\\s*\", x)))(train_df[\"sp_name\"].fillna(\"\"))].to_csv(\"./texts/sp_name.txt\", index=None, encoding=\"utf-8\", header=None)\n","train_df[[\"sp_address_raw\"]][~np.vectorize(lambda x: bool(re.fullmatch(\"\\s*\", x)))(train_df[\"sp_address_raw\"].fillna(\"\"))].to_csv(\"./texts/sp_address.txt\", index=None, encoding=\"utf-8\", header=None)\n","\n","if True:\n","    spm.SentencePieceTrainer.train(\n","        input=\"./texts/sp_name.txt\",\n","        model_type=\"unigram\",\n","        split_by_whitespace=True,\n","        model_prefix=nadare_feature_dir + \"sp_name\",\n","        character_coverage=1.,\n","        vocab_size=32000,\n","    )\n","\n","if True:\n","    spm.SentencePieceTrainer.train(\n","        input=\"./texts/sp_address.txt\",\n","        model_type=\"unigram\",\n","        split_by_whitespace=True,\n","        model_prefix=nadare_feature_dir + \"sp_address\",\n","        character_coverage=1.,\n","        vocab_size=32000,\n","    )\n","\n"]},{"cell_type":"code","execution_count":null,"id":"baeb7b70","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:45.633322Z","start_time":"2022-06-30T21:07:11.850521Z"},"id":"baeb7b70"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","\n","address_index = np.where(~np.vectorize(lambda x: bool(re.fullmatch(\"\\s*\", x)))(train_df[\"sp_address_raw\"].fillna(\"\")))[0]\n","knn = KNeighborsRegressor(n_neighbors=3, \n","                          metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(train_df[['latitude','longitude']].values)[address_index], address_index)\n","address_neiighbor = address_index[knn.kneighbors(np.deg2rad(train_df[['latitude','longitude']].values))[1]]\n","train_df[\"sp_address\"] = np.vectorize(lambda x: \" \".join(x))(pd.Series(train_df[\"sp_address_raw\"].values[address_neiighbor].tolist()))"]},{"cell_type":"code","execution_count":null,"id":"af3afaac","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:50.350322Z","start_time":"2022-06-30T21:08:45.634324Z"},"id":"af3afaac"},"outputs":[],"source":["train_df[\"sp_name\"] = train_df[\"sp_name\"].apply(remove_marks)\n","train_df[\"sp_address\"] = train_df[\"sp_address\"].apply(remove_marks)"]},{"cell_type":"markdown","id":"0c70fb0a","metadata":{"ExecuteTime":{"end_time":"2022-06-23T20:29:50.054030Z","start_time":"2022-06-23T20:29:43.666964Z"},"id":"0c70fb0a"},"source":["from tensorflow_text import BertTokenizer, SentencepieceTokenizer\n","preprocessor = hub.KerasLayer(\n","    \"../input/universalsentenceencodercmlmbr/multilingual-base-br\")\n","encoder = hub.KerasLayer(\n","    \"../input/universalsentenceencodercmlmbr/multilingual-preprocess\")\n","\n","def trimer(X):\n","    trim_size = tf.reduce_max(tf.reduce_sum(X[\"input_mask\"], axis=1))\n","    Y = {\"input_mask\": tf.slice(X[\"input_mask\"], [0, 0], [-1, trim_size]),\n","         \"input_type_ids\": tf.slice(X[\"input_type_ids\"], [0, 0], [-1, trim_size]),\n","         \"input_word_ids\": tf.slice(X[\"input_word_ids\"], [0, 0], [-1, trim_size]),\n","         }\n","    return Y\n","\n","batch_size = 128"]},{"cell_type":"code","execution_count":null,"id":"f1826111","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:50.366322Z","start_time":"2022-06-30T21:08:50.351323Z"},"id":"f1826111","outputId":"008a8faa-8df0-4820-e4f9-e010c86d9625"},"outputs":[{"data":{"text/plain":["'\\nbatch_size = 128\\n\\ntoken_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_df[\"name\"].fillna(\"\").values))            .batch(batch_size)            .map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)            .map(trimer, num_parallel_calls=tf.data.AUTOTUNE)            .prefetch(tf.data.AUTOTUNE)\\n\\nembeddings = []\\nfor text in tqdm(token_ds, total=len(train_df)//batch_size):\\n    embeddings.append(encoder(text)[\\'pooled_output\\'])\\n\\nname_embeddings = tf.concat(embeddings, axis=0)\\n\\nfrom sklearn.decomposition import PCA\\nname_embeddings = tf.convert_to_tensor(PCA(n_components=128).fit_transform(name_embeddings.numpy()))\\n\\nmodel = open(\\'./sentencepiece/sp_name.model\\', \"rb\").read()\\nsp_tokenizer = SentencepieceTokenizer(model)\\n\\ntoken_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_df[\"sp_name\"]))            .batch(batch_size)            .map(sp_tokenizer.tokenize, num_parallel_calls=tf.data.AUTOTUNE)            .prefetch(tf.data.AUTOTUNE)\\nname_init_embedding = tf.zeros([32000, 128])\\nname_init_count = tf.zeros([32000])\\n\\nfor token in tqdm(token_ds):\\n    name_init_embedding = tf.tensor_scatter_nd_add(name_init_embedding,\\n                                                   tf.expand_dims(token.values, axis=1),\\n                                                   tf.gather(name_embeddings, token.value_rowids()))\\n    name_init_count = tf.tensor_scatter_nd_add(name_init_count,\\n                                               tf.expand_dims(token.values, axis=1),\\n                                               tf.ones(tf.shape(token.value_rowids())))    \\nname_init_embedding = tf.nn.l2_normalize(name_init_embedding / (tf.expand_dims(name_init_count, axis=1) + tf.keras.backend.epsilon()), axis=1)\\npd.to_pickle(name_init_embedding.numpy(), \"../processed/name_init_embedding.pickle\")\\n'"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# あまり効かない\n","\"\"\"\n","batch_size = 128\n","\n","token_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_df[\"name\"].fillna(\"\").values))\\\n","            .batch(batch_size)\\\n","            .map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)\\\n","            .map(trimer, num_parallel_calls=tf.data.AUTOTUNE)\\\n","            .prefetch(tf.data.AUTOTUNE)\n","\n","embeddings = []\n","for text in tqdm(token_ds, total=len(train_df)//batch_size):\n","    embeddings.append(encoder(text)['pooled_output'])\n","\n","name_embeddings = tf.concat(embeddings, axis=0)\n","\n","from sklearn.decomposition import PCA\n","name_embeddings = tf.convert_to_tensor(PCA(n_components=128).fit_transform(name_embeddings.numpy()))\n","\n","model = open('./sentencepiece/sp_name.model', \"rb\").read()\n","sp_tokenizer = SentencepieceTokenizer(model)\n","\n","token_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_df[\"sp_name\"]))\\\n","            .batch(batch_size)\\\n","            .map(sp_tokenizer.tokenize, num_parallel_calls=tf.data.AUTOTUNE)\\\n","            .prefetch(tf.data.AUTOTUNE)\n","name_init_embedding = tf.zeros([32000, 128])\n","name_init_count = tf.zeros([32000])\n","\n","for token in tqdm(token_ds):\n","    name_init_embedding = tf.tensor_scatter_nd_add(name_init_embedding,\n","                                                   tf.expand_dims(token.values, axis=1),\n","                                                   tf.gather(name_embeddings, token.value_rowids()))\n","    name_init_count = tf.tensor_scatter_nd_add(name_init_count,\n","                                               tf.expand_dims(token.values, axis=1),\n","                                               tf.ones(tf.shape(token.value_rowids())))    \n","name_init_embedding = tf.nn.l2_normalize(name_init_embedding / (tf.expand_dims(name_init_count, axis=1) + tf.keras.backend.epsilon()), axis=1)\n","pd.to_pickle(name_init_embedding.numpy(), \"../processed/name_init_embedding.pickle\")\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"a0b6556c","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:50.381955Z","start_time":"2022-06-30T21:08:50.367325Z"},"id":"a0b6556c","outputId":"be623548-a588-40f1-d650-16ae2c81d97a"},"outputs":[{"data":{"text/plain":["'\\nfrom tensorflow_text import BertTokenizer, SentencepieceTokenizer\\nbatch_size = 128\\n\\ntoken_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_df[\"address\"].fillna(\"\").values))            .batch(batch_size)            .map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)            .map(trimer, num_parallel_calls=tf.data.AUTOTUNE)            .prefetch(tf.data.AUTOTUNE)\\n\\nembeddings = []\\nfor text in tqdm(token_ds, total=len(train_df)//batch_size):\\n    embeddings.append(encoder(text)[\\'pooled_output\\'])\\n\\naddress_embeddings = tf.concat(embeddings, axis=0)\\n\\nfrom sklearn.decomposition import PCA\\naddress_embeddings = tf.convert_to_tensor(PCA(n_components=128).fit_transform(address_embeddings.numpy()))\\n\\nmodel = open(\\'./sentencepiece/sp_address.model\\', \"rb\").read()\\nsp_tokenizer = SentencepieceTokenizer(model)\\n\\ntoken_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_df[\"sp_address\"].fillna(\"\").values))            .batch(batch_size)            .map(sp_tokenizer.tokenize, num_parallel_calls=tf.data.AUTOTUNE)            .prefetch(tf.data.AUTOTUNE)\\naddress_init_embedding = tf.zeros([32000, 128])\\naddress_init_count = tf.zeros([32000])\\n\\nfor token in tqdm(token_ds):\\n    address_init_embedding = tf.tensor_scatter_nd_add(address_init_embedding,\\n                                                   tf.expand_dims(token.values, axis=1),\\n                                                   tf.gather(address_embeddings, token.value_rowids()))\\n    address_init_count = tf.tensor_scatter_nd_add(address_init_count,\\n                                               tf.expand_dims(token.values, axis=1),\\n                                               tf.ones(tf.shape(token.value_rowids())))    \\naddress_init_embedding = tf.nn.l2_normalize(address_init_embedding / (tf.expand_dims(address_init_count, axis=1) + tf.keras.backend.epsilon()), axis=1)\\npd.to_pickle(address_init_embedding.numpy(), \"../processed/address_init_embedding.pickle\")\\n'"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# あまり効かない\n","\"\"\"\n","from tensorflow_text import BertTokenizer, SentencepieceTokenizer\n","batch_size = 128\n","\n","token_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_df[\"address\"].fillna(\"\").values))\\\n","            .batch(batch_size)\\\n","            .map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)\\\n","            .map(trimer, num_parallel_calls=tf.data.AUTOTUNE)\\\n","            .prefetch(tf.data.AUTOTUNE)\n","\n","embeddings = []\n","for text in tqdm(token_ds, total=len(train_df)//batch_size):\n","    embeddings.append(encoder(text)['pooled_output'])\n","\n","address_embeddings = tf.concat(embeddings, axis=0)\n","\n","from sklearn.decomposition import PCA\n","address_embeddings = tf.convert_to_tensor(PCA(n_components=128).fit_transform(address_embeddings.numpy()))\n","\n","model = open('./sentencepiece/sp_address.model', \"rb\").read()\n","sp_tokenizer = SentencepieceTokenizer(model)\n","\n","token_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_df[\"sp_address\"].fillna(\"\").values))\\\n","            .batch(batch_size)\\\n","            .map(sp_tokenizer.tokenize, num_parallel_calls=tf.data.AUTOTUNE)\\\n","            .prefetch(tf.data.AUTOTUNE)\n","address_init_embedding = tf.zeros([32000, 128])\n","address_init_count = tf.zeros([32000])\n","\n","for token in tqdm(token_ds):\n","    address_init_embedding = tf.tensor_scatter_nd_add(address_init_embedding,\n","                                                   tf.expand_dims(token.values, axis=1),\n","                                                   tf.gather(address_embeddings, token.value_rowids()))\n","    address_init_count = tf.tensor_scatter_nd_add(address_init_count,\n","                                               tf.expand_dims(token.values, axis=1),\n","                                               tf.ones(tf.shape(token.value_rowids())))    \n","address_init_embedding = tf.nn.l2_normalize(address_init_embedding / (tf.expand_dims(address_init_count, axis=1) + tf.keras.backend.epsilon()), axis=1)\n","pd.to_pickle(address_init_embedding.numpy(), \"../processed/address_init_embedding.pickle\")\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"e94a44dd","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:50.397968Z","start_time":"2022-06-30T21:08:50.383953Z"},"id":"e94a44dd","outputId":"6b268917-863c-40b0-ae2c-ffdb2e4d2148"},"outputs":[{"data":{"text/plain":["'\\nbatch_size = 128\\n\\ntoken_ds = tf.data.Dataset.from_tensor_slices(tf.constant(pd.Series(category_ix_dict).sort_values().index.values))            .batch(batch_size)            .map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)            .map(trimer, num_parallel_calls=tf.data.AUTOTUNE)            .prefetch(tf.data.AUTOTUNE)\\n\\nembeddings = []\\nfor text in tqdm(token_ds, total=len(category_ix_dict)//batch_size):\\n    embeddings.append(encoder(text)[\\'pooled_output\\'])\\n\\ncategory_init_embedding = tf.concat(embeddings, axis=0)\\ncategory_init_embedding = tf.nn.l2_normalize(tf.convert_to_tensor(PCA(n_components=128).fit_transform(category_init_embedding.numpy())), axis=1)\\npd.to_pickle(category_init_embedding.numpy(), \"../processed/category_init_embedding.pickle\")'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","batch_size = 128\n","\n","token_ds = tf.data.Dataset.from_tensor_slices(tf.constant(pd.Series(category_ix_dict).sort_values().index.values))\\\n","            .batch(batch_size)\\\n","            .map(preprocessor, num_parallel_calls=tf.data.AUTOTUNE)\\\n","            .map(trimer, num_parallel_calls=tf.data.AUTOTUNE)\\\n","            .prefetch(tf.data.AUTOTUNE)\n","\n","embeddings = []\n","for text in tqdm(token_ds, total=len(category_ix_dict)//batch_size):\n","    embeddings.append(encoder(text)['pooled_output'])\n","\n","category_init_embedding = tf.concat(embeddings, axis=0)\n","category_init_embedding = tf.nn.l2_normalize(tf.convert_to_tensor(PCA(n_components=128).fit_transform(category_init_embedding.numpy())), axis=1)\n","pd.to_pickle(category_init_embedding.numpy(), \"../processed/category_init_embedding.pickle\")\"\"\""]},{"cell_type":"markdown","id":"208cd4d0","metadata":{"id":"208cd4d0"},"source":["knn = KNeighborsRegressor(n_neighbors=128 + train_df[\"pid\"].value_counts().max(), \n","                              metric='haversine', \n","                          n_jobs=-1)\n","knn.fit(np.deg2rad(train_df[['latitude','longitude']]), train_df[\"ix\"])\n","neighbors_dist, neighbors_ix = knn.kneighbors(np.deg2rad(train_df[['latitude','longitude']]), return_distance=True)\n","neighbors_dist = np.where(train_df[\"pid\"].values.reshape(-1, 1) != train_df[\"pid\"].values[neighbors_ix], neighbors_dist, 1e9).astype(np.float32)\n","order = np.argsort(neighbors_dist, axis=1)\n","neighbors_dist = np.take_along_axis(neighbors_dist, order, axis=1)[:, :128]\n","neighbors_ix = np.take_along_axis(neighbors_ix, order, axis=1)[:, :128]\n","pd.to_pickle((neighbors_dist, neighbors_ix), \"../processed/neighbor_pairs.pickle\")"]},{"cell_type":"code","execution_count":null,"id":"17ac7f1a","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:51.889018Z","start_time":"2022-06-30T21:08:50.398970Z"},"id":"17ac7f1a"},"outputs":[],"source":["name_init_embedding = None#tf.convert_to_tensor(pd.read_pickle(\"../processed/name_init_embedding.pickle\"))\n","address_init_embedding = None#tf.convert_to_tensor(pd.read_pickle(\"../processed/address_init_embedding.pickle\"))\n","#category_init_embedding = tf.convert_to_tensor(pd.read_pickle(\"../processed/category_init_embedding.pickle\"))\n","neighbor_dist, neighbor_ix = pd.read_pickle(\"../processed/neighbor_pairs.pickle\")\n","neighbor_dist = -np.log(neighbor_dist + tf.keras.backend.epsilon())\n","\n","neighbor_ix = tf.convert_to_tensor(neighbor_ix.astype(np.int32))\n","neighbor_dist = tf.convert_to_tensor(neighbor_dist.astype(np.float32))\n"]},{"cell_type":"code","execution_count":null,"id":"48ea38d7","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:51.905016Z","start_time":"2022-06-30T21:08:51.890017Z"},"id":"48ea38d7"},"outputs":[],"source":["ix_params_dict = {\"country\": {\"dimention\": 64, \"num_category\": len(country_index_df)+1},\n","                  \"state\": {\"dimention\": 64, \"num_category\": len(state_index_df)},\n","                  \"city\": {\"dimention\": 64, \"num_category\": len(city_index_df)},\n","                  \"geo_name\": {\"dimention\": 64, \"num_category\": len(geo_name_index_df)}\n","                  }\n","train_ix_values = {\"country\": tf.convert_to_tensor(train_df[\"country_ix\"].values.astype(np.int32)),\n","                   \"state\": tf.convert_to_tensor(train_df[\"pseudo_state_ix\"].values.astype(np.int32)),\n","                   \"city\": tf.convert_to_tensor(train_df[\"pseudo_city_ix\"].values.astype(np.int32)),\n","                   \"geo_name\": tf.convert_to_tensor(train_df[\"pseudo_geo_name_ix\"].values.astype(np.int32)),\n","                   }\n","\n","class IxEmbeddingLayer(tf.keras.layers.Layer):\n","    def __init__(self, ix_params_dict, num_layer=1, out_dim=128):\n","        super(IxEmbeddingLayer, self).__init__()\n","        self.ix_params_dict = ix_params_dict\n","        self.num_layer = num_layer\n","        self.out_dim = out_dim\n","        self.embedding_layers = {k: tf.keras.layers.Embedding(v[\"num_category\"], v[\"dimention\"]) for k, v in ix_params_dict.items()}\n","        self.denses = [tf.keras.layers.Dense(out_dim, activation=\"gelu\")]\n","        self.out = tf.keras.layers.Dense(out_dim)\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","    \n","    def call(self, ix_values):\n","        results = []\n","        for k, v in ix_values.items():\n","            results.append(self.embedding_layers[k](v))\n","        X = tf.concat(results, axis=-1)\n","        \n","        for i in range(self.num_layer):\n","            X = self.drop_out(self.denses[i](X))\n","        return self.out(X)\n","        "]},{"cell_type":"code","execution_count":null,"id":"e17f9a10","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:51.920643Z","start_time":"2022-06-30T21:08:51.906017Z"},"id":"e17f9a10"},"outputs":[],"source":["def approx_ranks(logits):\n","    r\"\"\"Computes approximate ranks given a list of logits.\n","    Given a list of logits, the rank of an item in the list is one plus the total\n","    number of items with a larger logit. In other words,\n","    rank_i = 1 + \\sum_{j \\neq i} I_{s_j > s_i},\n","    where \"I\" is the indicator function. The indicator function can be\n","    approximated by a generalized sigmoid:\n","    I_{s_j < s_i} \\approx 1/(1 + exp(-(s_j - s_i)/temperature)).\n","    This function approximates the rank of an item using this sigmoid\n","    approximation to the indicator function. This technique is at the core\n","    of \"A general approximation framework for direct optimization of\n","    information retrieval measures\" by Qin et al.\n","    Args:\n","    logits: A `Tensor` with shape [batch_size, list_size]. Each value is the\n","      ranking score of the corresponding item.\n","    Returns:\n","    A `Tensor` of ranks with the same shape as logits.\n","    \"\"\"\n","    list_size = tf.shape(input=logits)[1]\n","    x = tf.tile(tf.expand_dims(logits, 2), [1, 1, list_size])\n","    y = tf.tile(tf.expand_dims(logits, 1), [1, list_size, 1])\n","    pairs = tf.sigmoid(y - x)\n","    return tf.reduce_sum(input_tensor=pairs, axis=-1) + .5\n","    \n","    \n","class RankMSE(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(RankMSE, self).__init__()\n","    \n","    def call(self, y_true, y_pred):        \n","        weight = 1 / y_true\n","        rmse = tf.reduce_sum(tf.math.sqrt(tf.math.square(y_true - y_pred))*weight, axis=-1) / tf.reduce_sum(weight, axis=1)\n","        \n","        return tf.reduce_mean(rmse)"]},{"cell_type":"code","execution_count":null,"id":"33ef7cb0","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:51.936242Z","start_time":"2022-06-30T21:08:51.921644Z"},"id":"33ef7cb0"},"outputs":[],"source":["class MixerModel(tf.keras.Model):\n","    \n","    def __init__(self, name_model, address_model, ix_emb_layer, cat_emb_layer, num_layer=2, num_dim=128):\n","        super(MixerModel, self).__init__()\n","        self.num_layer = num_layer\n","        self.num_dim = num_dim\n","        self.name_model = name_model\n","        self.address_model = address_model\n","        self.ix_emb_layer = ix_emb_layer\n","        self.cat_emb_layer = cat_emb_layer\n","        \n","        self.tokenize_norm = [tf.keras.layers.LayerNormalization() for i in range(4)]\n","    \n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        \n","        self.filter_denses = [tf.keras.layers.DepthwiseConv2D(kernel_size=(1, 4),\n","                                                              strides=1,\n","                                                              padding=\"valid\",\n","                                                              depth_multiplier=4,\n","                                                              activation=\"gelu\") for _ in range(self.num_layer)]\n","        self.out_denses = [tf.keras.layers.Dense(num_dim) for _ in range(self.num_layer)]\n","        \n","        #self.pointwise_filter_denses = [tf.keras.layers.Dense(num_dim, activation=\"gelu\") for _ in range(self.num_layer)]\n","        #self.pointwise_out_denses = [tf.keras.layers.Dense(num_dim) for _ in range(self.num_layer)]\n","        self.norm_layers = [tf.keras.layers.LayerNormalization() for _ in range(self.num_layer)]\n","        \n","    def check_tensor(self, x, ix):\n","        if isinstance(x, tf.RaggedTensor):\n","            x = x.to_tensor()\n","        return x\n","        \n","    def call(self, info, training=False):\n","        ix, name, address, position, ix_values, categories, pids, size = info\n","        X_name = self.tokenize_norm[0](tf.reduce_sum(self.name_model(name), axis=-2))\n","        X_address = self.tokenize_norm[1](tf.reduce_sum(self.address_model(address), axis=-2))\n","        X_ix = self.tokenize_norm[2](self.drop_out(self.ix_emb_layer(ix_values)))\n","        X_categories = self.tokenize_norm[3](tf.reduce_sum(self.drop_out(self.cat_emb_layer(categories)), axis=-2))\n","        \n","        X = tf.stack([X_name, X_address, X_ix, X_categories], axis=-2)\n","        original_shape = tf.shape(X)\n","        X = tf.expand_dims(X, axis=1)\n","        expand_shape = tf.shape(X)\n","        for i in range(self.num_layer):\n","            X_ = self.norm_layers[i](X)\n","            X_ = self.out_denses[i](tf.reshape(self.filter_denses[i](X_), expand_shape))\n","            X = X + X_\n","        X = tf.reshape(X, original_shape)\n","        return tf.reduce_mean(X, axis=-2)\n","\n","def simcse_task(mixer_model, loss_func, neighbor_info):\n","    X_a = tf.nn.l2_normalize(mixer_model(neighbor_info, training=True), axis=-1)\n","    X_b = tf.nn.l2_normalize(mixer_model(neighbor_info, training=True), axis=-1)\n","    \n","    X_p = tf.reshape(neighbor_info[-2], [-1, neighbor_info[-1]])\n","    \n","    N = tf.shape(X_p)[0]\n","    R = tf.shape(X_p)[1]\n","    \n","    shape = tf.shape(X_a)\n","    X_a = tf.reshape(X_a, [-1, neighbor_info[-1], shape[-1]])\n","    X_b = tf.reshape(X_b, [-1, neighbor_info[-1], shape[-1]])\n","    shape = tf.shape(X_a)\n","    \n","    cossim_c = tf.reshape(tf.einsum(\"nad,nbd->nab\", X_a, X_b), [shape[0]*shape[1], shape[1]])\n","    cossim_r = tf.reshape(tf.einsum(\"nad,nbd->nab\", tf.transpose(X_a, [1, 0, 2]), tf.transpose(X_b, [1, 0, 2])), [shape[0]*shape[1], shape[0]])\n"," \n","    mask_c = tf.reshape(tf.expand_dims(X_p, axis=1) != tf.expand_dims(X_p, axis=2), [shape[0]*shape[1], shape[1]])\n","    mask_r = tf.reshape(tf.expand_dims(X_p, axis=0) != tf.expand_dims(X_p, axis=1), [shape[0]*shape[1], shape[0]])    \n","\n","    label_c = tf.reshape(tf.repeat(tf.expand_dims(tf.eye(tf.shape(X_a)[1]), axis=0), tf.shape(X_a)[0], axis=0), tf.shape(cossim_c))\n","    label_r = tf.reshape(tf.repeat(tf.expand_dims(tf.eye(tf.shape(X_a)[0]), axis=1), tf.shape(X_a)[1], axis=0), tf.shape(cossim_r))\n","    \n","    mask_c = tf.math.logical_or(label_c == 1., mask_c)\n","    mask_r = tf.math.logical_or(label_r == 1., mask_r)\n","    \n","    cossim_c = tf.where(mask_c, cossim_c, -1.)\n","    cossim_r = tf.where(mask_r, cossim_r, -1.)\n","    \n","    scale_c = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label_c)[1], \"float32\") - 1.)\n","    scale_r = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label_r)[1], \"float32\") - 1.)\n","    loss = loss_func(label_c, cossim_c * scale_c) + loss_func(label_r, cossim_r * scale_r)\n","    return loss\n"]},{"cell_type":"code","execution_count":null,"id":"15204c8a","metadata":{"id":"15204c8a"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"a1ff96f1","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:51.952056Z","start_time":"2022-06-30T21:08:51.937244Z"},"id":"a1ff96f1"},"outputs":[],"source":["class DimFreeLayerNormalization(tf.keras.layers.Layer):\n","    def __init__(self, name):\n","        super(DimFreeLayerNormalization, self).__init__()\n","        self.g = tf.Variable(1., dtype=\"float32\", trainable=True, name=\"/\" + name + \"_a\")\n","        self.a = tf.Variable(0., dtype=\"float32\", trainable=True, name=\"/\" + name + \"_b\")\n","        \n","    def call(self, X):\n","        X_mean = tf.reduce_mean(X, axis=-1, keepdims=True)\n","        X_std = tf.math.reduce_std(X, axis=-1, keepdims=True)\n","        X = self.g * (X-X_mean) / (X_std + tf.keras.backend.epsilon()) + self.a\n","        return X       "]},{"cell_type":"code","execution_count":null,"id":"75cf5839","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:51.968136Z","start_time":"2022-06-30T21:08:51.953056Z"},"id":"75cf5839"},"outputs":[],"source":["class CategoryEmbeddingLayer(tf.keras.layers.Layer):\n","    \n","    def __init__(self, num_categories, dim_categories, init_embedding=None):\n","        super(CategoryEmbeddingLayer, self).__init__()\n","        self.num_categories = num_categories\n","        self.dim_categories = dim_categories\n","        self.category_dense = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=(num_categories, dim_categories)), trainable=True, name=self.name + \"/category_embedding\")\n","        if not init_embedding is None:\n","            self.category_dense.assign(init_embedding)\n","            \n","    def call(self, C):\n","        X = tf.gather(self.category_dense, C)\n","        return X"]},{"cell_type":"code","execution_count":null,"id":"b3b50316","metadata":{"id":"b3b50316"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"8b99f6a6","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:51.999591Z","start_time":"2022-06-30T21:08:51.969057Z"},"id":"8b99f6a6"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow_text import SentencepieceTokenizer\n","\n","class SupervisedContrastiveLoss(tf.keras.layers.Layer):\n","    def __init__(self, from_logits=False):\n","        super(SupervisedContrastiveLoss, self).__init__()\n","        if not from_logits:\n","            NotImplementedError\n","        \n","    @tf.function\n","    def call(self, true, pred, sample_weight=None):\n","        if sample_weight is None:\n","            sample_weight = tf.ones(tf.shape(pred), dtype=tf.float32)\n","        epred = tf.exp(pred) * sample_weight\n","        scale = tf.reduce_sum(epred, axis=1, keepdims=True)\n","        loss = tf.reduce_sum((tf.math.log(scale - epred * true) - true*pred) * true * sample_weight) / tf.reduce_sum(true*sample_weight)\n","        return loss\n","    \n","class SentencePieceEmbeddingLayer(tf.keras.layers.Layer):\n","    \n","    def __init__(self, vocab_size, out_dim, sp_model_path, init_embedding=None):\n","        super(SentencePieceEmbeddingLayer, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.out_dim = out_dim\n","        model = open(sp_model_path, \"rb\").read()\n","        self.tokenizer = SentencepieceTokenizer(model)\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, out_dim)\n","        if not init_embedding is None:\n","            self.embedding(0)\n","            self.embedding.trainable_variables[0].assign(init_embedding)\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","    \n","    def call(self, X):\n","        token = self.tokenizer.tokenize(X)\n","        X = self.drop_out(self.embedding(token))\n","        return X\n","    \n","class DpConvWrapper(tf.keras.layers.Layer):\n","    \n","    def __init__(self, sp_layer, num_layer=2):\n","        super(DpConvWrapper, self).__init__()\n","        self.sp_layer = sp_layer\n","        self.num_layer = num_layer\n","        self.out_dim = self.sp_layer.out_dim\n","        self.norms = [tf.keras.layers.LayerNormalization() for i in range(num_layer)]\n","        self.dep_convs = [tf.keras.layers.DepthwiseConv1D(kernel_size=3, strides=1, padding=\"same\", activation=\"gelu\") for _ in range(num_layer)]\n","        self.denses = [tf.keras.layers.Dense(sp_layer.out_dim) for _ in range(num_layer)]\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        \n","        \n","    def call(self, X):\n","        token = name_dpc.sp_layer.tokenizer.tokenize(X) + 1\n","        start = tf.cast(tf.concat([tf.zeros([1], dtype=\"int64\"), tf.cumsum(token.row_lengths())], axis=0), \"int32\")\n","        row_ids = tf.cast(token.value_rowids(), \"int32\")\n","        col_ids = tf.range(tf.shape(token.value_rowids())[0], dtype=\"int32\") - tf.gather(start, token.value_rowids())\n","        X = self.drop_out(name_dpc.sp_layer.embedding(token.to_tensor(default_value=2)))\n","        rank = tf.rank(X)\n","        shape = tf.shape(X)\n","        \n","        for i in range(name_dpc.num_layer):\n","            X_ = name_dpc.norms[i](X)\n","            X_ = name_dpc.dep_convs[i](X_)\n","            X_ = name_dpc.denses[i](X_)\n","            X += X_\n","            \n","        X = tf.RaggedTensor.from_value_rowids(tf.gather_nd(X, tf.stack([row_ids, col_ids], axis=1)),\n","                                              token.value_rowids(),\n","                                              nrows=tf.cast(shape[0], \"int64\"))\n","        return X        \n","\n","    \n","class SkipgramModel(tf.keras.Model):\n","    \n","    def __init__(self, name_spe):\n","        super(SkipgramModel, self).__init__()\n","        self.name_spe = name_spe\n","        \n","    #@tf.function(experimental_relax_shapes=True)\n","    def call(self, name, n_name, n_pid):\n","        X = skipgram_model.name_spe(name)\n","        X_sum_neighbor = tf.reduce_sum(skipgram_model.name_spe(tf.slice(n_name, [0, 1], [-1, -1])), axis=2).to_tensor()\n","        name_skip_loss = skipgram_model.skipgram_task(X, X_sum_neighbor, n_pid)\n","\n","        return name_skip_loss\n","    \n","    #@tf.function(experimental_relax_shapes=True)\n","    def skipgram_task(self, X, X_sum_neighbor, n_pid):\n","\n","        my_pid, other_pid = tf.split(n_pid, [1, tf.shape(n_pid)[1]-1], axis=1)\n","        \n","        X_sum = tf.reduce_sum(X, axis=1)\n","        X_sum_other = tf.gather(X_sum, X.value_rowids()) - X.values\n","\n","        \n","        X_sum_norm = tf.nn.l2_normalize(X_sum, axis=1)\n","        X_sum_other_norm = tf.nn.l2_normalize(X_sum_other, axis=1)\n","        X_sum_neighbor_norm = tf.slice(tf.gather(tf.nn.l2_normalize(X_sum_neighbor, axis=-1), X.value_rowids()), [0, 1, 0], [-1, -1, -1])\n","        X_value_norm = tf.nn.l2_normalize(X.values, axis=1)\n","        \n","\n","        correct_cossim = tf.expand_dims(tf.einsum(\"Vd,Vd->V\", X_value_norm, X_sum_other_norm), axis=1)\n","        wrong_cossim = tf.einsum(\"Vd,Nd->VN\", X_value_norm, X_sum_norm)\n","        neighbor_cossim = tf.einsum(\"Vd,VNd->VN\", X_value_norm, X_sum_neighbor_norm)\n","        cossim = tf.concat([correct_cossim, wrong_cossim, neighbor_cossim], axis=1)\n","\n","        rowwise_mask = tf.expand_dims(tf.gather(X.row_lengths(), X.value_rowids()) > 1, axis=1)\n","        sameid_mask = tf.concat([tf.expand_dims(tf.cast(X.value_rowids(), \"int32\"), axis=1) != tf.expand_dims(tf.range(tf.shape(X.row_lengths())[0] + 1)-1, axis=0),\n","                                 tf.ones(tf.gather(tf.shape(X_sum_neighbor_norm), [0, 1], axis=0), dtype=\"bool\")], axis=1)\n","        same_pid_mask = tf.concat([tf.gather(my_pid, X.value_rowids(), axis=0) != tf.linalg.matrix_transpose(my_pid),\n","                                   tf.gather(my_pid, X.value_rowids()) != tf.gather(other_pid, X.value_rowids())], axis=1)\n","        \n","        mask = tf.math.logical_and(tf.math.logical_and(rowwise_mask, sameid_mask), same_pid_mask)\n","        label = tf.concat([tf.ones(tf.shape(correct_cossim)), tf.zeros(tf.shape(wrong_cossim)), tf.zeros(tf.shape(neighbor_cossim))], axis=1)\n","        \n","        # Fixed AdaCos paramet2r\n","        scale = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(label)[1], \"float32\") - 1.)\n","\n","        label = tf.where(mask, label, tf.zeros(tf.shape(mask)))\n","        pred = tf.where(mask, scale * cossim, scale * -1.)\n","        loss = tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(label, pred, from_logits=True))\n","        return loss   \n","    \n","class ClassifyTrainModel(tf.keras.Model):\n","    \n","    def __init__(self, name_model, address_model, ix_emb_layer, cat_emb_layer):\n","        super(ClassifyTrainModel, self).__init__()\n","        self.name_model = name_model\n","        self.address_model = address_model\n","        self.ix_emb_layer = ix_emb_layer\n","        self.cat_emb_layer = cat_emb_layer\n","        self.dense = tf.keras.layers.Dense(cat_emb_layer.dim_categories, use_bias=True)\n","        self.scale = tf.sqrt(2.) * tf.math.log(tf.cast(cat_emb_layer.num_categories, \"float32\") - 1.)\n","        self.loss_function = SupervisedContrastiveLoss(from_logits=True)\n","        \n","        self.layer_norms = [tf.keras.layers.LayerNormalization() for i in range(3)]\n","        self.drop_out = tf.keras.layers.Dropout(0.1)\n","        self.nan_mask = tf.concat([tf.zeros([1, 1]), tf.ones([1, cat_emb_layer.num_categories-1])], axis=1)\n","        \n","    def transform(self, name, address, ix_values):\n","        X_name = self.layer_norms[0](tf.reduce_sum(self.drop_out(self.name_model(name)), axis=1))\n","        X_address = self.layer_norms[1](tf.reduce_sum(self.drop_out(self.address_model(address)), axis=1))\n","        X_ix = self.layer_norms[2](self.ix_emb_layer(ix_values))\n","        X = self.dense(tf.concat([X_name, X_address, X_ix], axis=-1))\n","        return X\n","        \n","    def call(self, name, address, ix_values, categories):\n","        X = self.transform(name, address, ix_values)\n","        classify_loss = self.classify_task(X, categories)\n","        return classify_loss\n","    \n","    def classify_task(self, X, C):\n","        cossim = tf.einsum(\"ND,CD->NC\", tf.nn.l2_normalize(X, axis=1), tf.nn.l2_normalize(self.cat_emb_layer.category_dense, axis=1))\n","        \n","        true = tf.scatter_nd(tf.stack([C.value_rowids(), C.values], axis=1),\n","                              tf.ones(tf.shape(C.values)),\n","                              [tf.shape(X)[0], self.cat_emb_layer.num_categories])\n","        true = true * self.nan_mask\n","        calc_loc = tf.where(tf.reduce_sum(true, axis=1) > 0.)\n","        pred = self.scale * cossim\n","        loss = self.loss_function(tf.gather_nd(true, calc_loc), tf.gather_nd(pred, calc_loc))\n","        return loss"]},{"cell_type":"code","execution_count":null,"id":"50882c26","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:52.031583Z","start_time":"2022-06-30T21:08:52.002586Z"},"id":"50882c26"},"outputs":[],"source":["class DataContainer():\n","    def __init__(self, df, ix_values, category_ix, positive_ix, neighbor_ix, neighbor_dist):\n","        self.name = tf.constant(df[\"sp_name\"].values)\n","        self.address = tf.constant(df[\"sp_address\"].values)\n","        self.position = tf.expand_dims(tf.constant(np.deg2rad(df[[\"latitude\", \"longitude\"]].astype(np.float32).values), dtype=\"float32\"), axis=0)\n","        self.pid = tf.constant(df[\"pid\"].astype(np.int32).values, dtype=\"int32\")\n","        self.group = tf.constant(df[\"group\"].astype(np.int32).values, dtype=\"int32\")\n","        self.ix_values = ix_values\n","        self.category_ix = category_ix\n","        self.positive_ix = positive_ix\n","        self.neighbor_ix = neighbor_ix\n","        self.neighbor_dist = neighbor_dist\n","        \n","    def get_position(self, ix):\n","        return tf.gather(tf.gather(self.position, ix, axis=1), 0)\n","\n","    def call(self, ix, size=1):\n","        name = tf.gather(self.name, ix)\n","        address = tf.gather(self.address, ix)\n","        position = self.get_position(ix)\n","        categories = tf.gather(self.category_ix, ix)\n","        ix_values = {k: tf.gather(self.ix_values[k], ix) for k in self.ix_values.keys()}\n","        pids = tf.gather(self.pid, ix)\n","        return ix, name, address, position, ix_values, categories, pids, size\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def log_haversine(self, X, Y):\n","        delta = Y - X\n","        x_lats = tf.gather(X, 0, axis=-1)\n","        y_lats = tf.gather(Y, 0, axis=-1)\n","        dlat = tf.gather(delta, 0, axis=-1)\n","        dlon = tf.gather(delta, 1, axis=-1)\n","\n","        a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","        c = tf.math.log(2 * tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a)) + tf.keras.backend.epsilon())\n","        return c\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def negative_sample_by_score_top_k(self, ix, score, except_loc=None, k=128, distcut=4):\n","        pos_ix = tf.gather(self.positive_ix, tf.gather(self.pid, ix))\n","        score = tf.tensor_scatter_nd_update(score,\n","                                            tf.stack([tf.cast(pos_ix.value_rowids(), \"int32\"), pos_ix.values], axis=1),\n","                                            tf.ones([tf.shape(pos_ix.values)[0]])*tf.float32.min/10.)\n","        if not except_loc is None:\n","            score = tf.tensor_scatter_nd_update(score,\n","                                                except_loc,\n","                                                tf.ones([tf.shape(except_loc)[0]])*tf.float32.min/10.)            \n","        score, neighbor = tf.math.top_k(score, k=k)\n","        _, order = tf.math.top_k(tf.random.uniform([tf.shape(ix)[0], k]), k=k)\n","        score = tf.gather(score, order, batch_dims=1)\n","        neighbor = tf.gather(neighbor, order, batch_dims=1)\n","        \n","        score = tf.reverse_sequence(score, \n","                                    tf.ones([tf.shape(score)[0]], dtype=\"int32\") * tf.shape(score)[1],\n","                                    seq_axis=1)\n","        neighbor = tf.reverse_sequence(neighbor,\n","                                  tf.ones([tf.shape(neighbor)[0]], dtype=\"int32\") * tf.shape(neighbor)[1],\n","                                  seq_axis=1)\n","        return [score, neighbor]\n","    \n","    def get_positive_info(self, ix):\n","        pos_ix = tf.gather(self.positive_ix, tf.gather(self.pid, ix))\n","        Y = self.get_position(pos_ix)\n","        X = self.get_position(tf.gather(ix, pos_ix.value_rowids()))\n","        pos_dist = tf.RaggedTensor.from_value_rowids(self.log_haversine(X, Y.values), Y.value_rowids())\n","        pos_ix = pos_ix.to_tensor(default_value=-1)\n","        pos_ix = tf.where(tf.expand_dims(ix, axis=1) != pos_ix, pos_ix, -1)\n","        pos_dist = pos_dist.to_tensor(default_value=0.)\n","        return pos_dist, pos_ix\n","    \n","    @tf.function(experimental_relax_shapes=True)\n","    def random_walk_sampling(self, query_ix, step, top_k):\n","        query_ix = tf.cast(query_ix, \"int32\")\n","        searched = tf.expand_dims(query_ix, axis=1)\n","        current = tf.identity(query_ix)\n","\n","        for i in range(step):\n","            n_ix = tf.gather(self.neighbor_ix, current)\n","            n_dist = tf.gather(self.neighbor_dist, current)\n","            n_dist = tf.where(tf.reduce_any(tf.expand_dims(n_ix, axis=2) == tf.expand_dims(searched, axis=1), axis=2), tf.keras.backend.epsilon(), n_dist)\n","\n","            prob = n_dist / tf.reduce_sum(n_dist, axis=1, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            nexts_ = tf.gather(n_ix, tf.math.top_k(gumbel, k=top_k)[1], batch_dims=1)\n","            searched = tf.concat([searched, nexts_], axis=1)\n","            current = tf.gather(nexts_, 0, axis=1)\n","        return tf.reshape(searched, [-1]), step*top_k + 1"]},{"cell_type":"code","execution_count":null,"id":"2766e00a","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:08:52.142583Z","start_time":"2022-06-30T21:08:52.032584Z"},"id":"2766e00a"},"outputs":[],"source":["DIMSIZE = 256\n","WALK_STEP = 128\n","WALK_TOPK = 8\n","num_categories = max(category_ix_dict.values()) + 1\n","\n","name_spe = SentencePieceEmbeddingLayer(32000, DIMSIZE, nadare_feature_dir + \"sp_name.model\", name_init_embedding)\n","address_spe = SentencePieceEmbeddingLayer(32000, DIMSIZE, nadare_feature_dir + \"sp_address.model\", address_init_embedding)\n","\n","ix_emb_layer = IxEmbeddingLayer(ix_params_dict, num_layer=1, out_dim=DIMSIZE)\n","cat_emb_layer = CategoryEmbeddingLayer(num_categories, DIMSIZE)\n","#cat_emb_layer.category_dense.assign(category_init_embedding)\n","\n","skipgram_model = SkipgramModel(name_spe)\n","#locrank_model = LocationModel(address_dpc, ix_emb_layer, num_rank=128)\n","classify_model = ClassifyTrainModel(name_spe, address_spe, ix_emb_layer, cat_emb_layer)\n","mixer_model = MixerModel(name_spe, address_spe, ix_emb_layer, cat_emb_layer, num_layer=3, num_dim=DIMSIZE)\n","\n","mixer_loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":null,"id":"5042a1bc","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:09:06.185801Z","start_time":"2022-06-30T21:08:52.143584Z"},"id":"5042a1bc"},"outputs":[],"source":["container_cols = [\"sp_name\", \"sp_address\", \"latitude\", \"longitude\", \"pid\", \"group\"] \n","train_container =  DataContainer(train_df[container_cols],\n","                                   train_ix_values,\n","                                    train_categories_ix,\n","                                    tf.cast(tf.ragged.constant(train_df.groupby(\"pid\")[\"ix\"].unique().tolist()), \"int32\"),\n","                                neighbor_ix,\n","                                neighbor_dist)"]},{"cell_type":"code","execution_count":null,"id":"97282646","metadata":{"ExecuteTime":{"end_time":"2022-06-30T21:09:07.506803Z","start_time":"2022-06-30T21:09:06.186803Z"},"id":"97282646"},"outputs":[],"source":["train_ixs = tf.range(len(train_container.name), dtype=\"int64\")\n","train_ds = tf.data.Dataset.from_tensor_slices(train_ixs)\\\n","                .shuffle(len(train_ixs), reshuffle_each_iteration=True)\\\n","                .batch(64, drop_remainder=True)\\\n","                .map(lambda x: (train_container.call(x, 1), train_container.call(*train_container.random_walk_sampling(x, WALK_STEP, WALK_TOPK))), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\\\n","                .prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"id":"3a2d26fa","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:47:19.163743Z","start_time":"2022-06-30T21:09:07.507802Z"},"colab":{"referenced_widgets":["e668aad3482b4950b958147105b050c6"]},"id":"3a2d26fa","outputId":"fcfc6629-981a-402f-dbf7-76cb1f9800de"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e668aad3482b4950b958147105b050c6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["skipgram_loss: 6.2974 classify_loss: 2.6489 locrank_loss: 0.0000, mixer_loss: 5.4179\n"]}],"source":["history = {\"skipgram_loss\": [], \"classify_loss\": [], \"locrank_loss\": [], \"mixer_loss\": []}\n","skipgram_loss = tf.keras.metrics.Mean(name='loss')\n","classify_loss = tf.keras.metrics.Mean(name='loss')\n","locrank_loss = tf.keras.metrics.Mean(name='loss')\n","mixer_loss = tf.keras.metrics.Mean(name='loss')\n","\n","NUM_EPOCH = 1\n","skipgram_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","classify_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","mixer_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","\n","\n","@tf.function(experimental_relax_shapes=True)\n","def skipgram_forward_step(name, n_name, n_pid):\n","    with tf.GradientTape() as tape:\n","        loss = skipgram_model(name, n_name, n_pid, training=True)\n","    gradients = tape.gradient(loss, skipgram_model.trainable_variables)\n","    skipgram_optimizer.apply_gradients(zip(gradients, skipgram_model.trainable_variables))\n","    return loss\n","\n","@tf.function(experimental_relax_shapes=True)\n","def classify_forward_step(name, address, ix_values, categories):\n","    with tf.GradientTape() as tape:\n","        loss = classify_model(name, address, ix_values, categories, training=True)\n","    gradients = tape.gradient(loss, classify_model.trainable_variables)\n","    if tf.reduce_all([tf.reduce_all(tf.math.is_finite(g)) for g in gradients]):\n","        classify_optimizer.apply_gradients(zip(gradients, classify_model.trainable_variables))\n","    return loss\n","\n","#@tf.function(experimental_relax_shapes=True)\n","def locrank_forward_step(query_info, neighbor_info):\n","    with tf.GradientTape() as tape:\n","        loss = locrank_model(query_info, neighbor_info, training=True)\n","    gradients = tape.gradient(loss, locrank_model.trainable_variables)\n","    if tf.reduce_all([tf.reduce_all(tf.math.is_finite(g)) for g in gradients]):\n","        locrank_optimizer.apply_gradients(zip(gradients, locrank_model.trainable_variables))\n","    return loss\n","\n","@tf.function(experimental_relax_shapes=True)\n","def mixer_forward_step(neighbor_info):\n","    with tf.GradientTape() as tape:\n","        loss = simcse_task(mixer_model, mixer_loss_func, neighbor_info)\n","    order = tape\n","    gradients = tape.gradient(loss, mixer_model.trainable_variables)\n","    if tf.reduce_all([tf.reduce_all(tf.math.is_finite(g)) for g in gradients]):\n","        mixer_optimizer.apply_gradients(zip(gradients, mixer_model.trainable_variables))\n","    return loss\n","\n","with tqdm(total=NUM_EPOCH) as pbar:\n","    for epoch in range(NUM_EPOCH):\n","        step = 0\n","        for query_info, neighbor_info in train_ds:\n","            ix, name, address, position, ix_values, categories, pids, size = query_info\n","            n_name =  neighbor_info[1]\n","            n_pid = neighbor_info[6]\n","            n_size = neighbor_info[7]\n","            n_name = tf.reshape(n_name, [-1, n_size])\n","            n_pid =  tf.reshape(n_pid, [-1, n_size])\n","            sl = skipgram_forward_step(name, n_name, n_pid)\n","            cl = classify_forward_step(name, address, ix_values, categories)\n","            #ll = locrank_forward_step(query_info, neighbor_info)\n","            ml = mixer_forward_step(neighbor_info)\n","            skipgram_loss(sl)\n","            if not pd.isna(cl.numpy()):\n","                classify_loss(cl)\n","            #if not pd.isna(ll.numpy()):\n","            #    locrank_loss(ll)\n","            if not pd.isna(ml.numpy()):\n","                mixer_loss(ml)\n","            learning_text = \"[{}/{}] \".format(str(step).zfill(5), len(train_ds))\n","            progress_text = \"skipgram_loss: {:.4f} classify_loss: {:.4f} locrank_loss: {:.4f}, mixer_loss: {:.4f}\".format(skipgram_loss.result().numpy(),\n","                                                                                                    classify_loss.result().numpy(),\n","                                                                                                    locrank_loss.result().numpy(),\n","                                                                                                    mixer_loss.result().numpy(),\n","                                                                                                    )\n","            pbar.set_postfix_str(learning_text + progress_text)\n","            step += 1\n","        history[\"skipgram_loss\"].append(skipgram_loss.result().numpy())\n","        history[\"classify_loss\"].append(classify_loss.result().numpy())\n","        history[\"locrank_loss\"].append(locrank_loss.result().numpy())\n","        history[\"mixer_loss\"].append(mixer_loss.result().numpy())\n","        print(progress_text)\n","\n","        skipgram_loss.reset_states()\n","        classify_loss.reset_states()\n","        locrank_loss.reset_states()\n","        mixer_loss.reset_states()\n","        pbar.update(1)\n","        classify_model.save_weights(f\"../feature/FEAT24/classify_model_{epoch}\")\n","        mixer_model.save_weights(f\"../feature/FEAT24/mixer_model_{epoch}\")"]},{"cell_type":"code","execution_count":null,"id":"d366d57f","metadata":{"ExecuteTime":{"end_time":"2022-06-29T23:18:49.738525Z","start_time":"2022-06-29T23:18:49.723518Z"},"id":"d366d57f","outputId":"0ecef924-e21f-4639-be6b-d5e7eb5e0e95"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>skipgram_loss</th>\n","      <th>classify_loss</th>\n","      <th>locrank_loss</th>\n","      <th>mixer_loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.947135</td>\n","      <td>3.118746</td>\n","      <td>0.0</td>\n","      <td>5.808200</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.582191</td>\n","      <td>2.265229</td>\n","      <td>0.0</td>\n","      <td>5.777259</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.459677</td>\n","      <td>2.113289</td>\n","      <td>0.0</td>\n","      <td>5.770914</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   skipgram_loss  classify_loss  locrank_loss  mixer_loss\n","0       3.947135       3.118746           0.0    5.808200\n","1       3.582191       2.265229           0.0    5.777259\n","2       3.459677       2.113289           0.0    5.770914"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(history)"]},{"cell_type":"code","execution_count":null,"id":"f0147f87","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:47:19.179744Z","start_time":"2022-06-30T23:47:19.164743Z"},"id":"f0147f87","outputId":"b4255be1-86ca-4b4e-f422-327ea70a4b75"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>skipgram_loss</th>\n","      <th>classify_loss</th>\n","      <th>locrank_loss</th>\n","      <th>mixer_loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.297405</td>\n","      <td>2.648874</td>\n","      <td>0.0</td>\n","      <td>5.417853</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   skipgram_loss  classify_loss  locrank_loss  mixer_loss\n","0       6.297405       2.648874           0.0    5.417853"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(history)"]},{"cell_type":"code","execution_count":null,"id":"b5e34ba2","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:47:19.258011Z","start_time":"2022-06-30T23:47:19.180745Z"},"id":"b5e34ba2"},"outputs":[],"source":["train_df[\"true_count\"] = train_df.groupby(\"pid\")[\"ix\"].transform(\"count\")"]},{"cell_type":"code","execution_count":null,"id":"c173d72d","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:48:23.480556Z","start_time":"2022-06-30T23:47:19.259014Z"},"colab":{"referenced_widgets":["e39d01e0b2e14601bf06a21e07355d9b","b3c387d4023e442db5b3388405e2f411"]},"id":"c173d72d","outputId":"facb1acb-a55f-4244-847e-68f0b6d81ff9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e39d01e0b2e14601bf06a21e07355d9b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1113 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3c387d4023e442db5b3388405e2f411","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8897 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision@16 0.7794088383447885\n"]}],"source":["# name\n","train_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n","                .batch(1024)\\\n","                .map(train_container.call)\n","\n","embeddings = []\n","for ix, name, address, position, ix_values, categories, _, _ in tqdm(train_ds):\n","    embeddings.append(tf.nn.l2_normalize(tf.reduce_sum(name_spe(name), axis=1), axis=1))\n","name_embeddings = tf.nn.l2_normalize(tf.concat(embeddings, axis=0), axis=1)\n","\n","# embedding only\n","emb_ds = tf.data.Dataset.from_tensor_slices(train_ixs)\\\n","                .batch(128)\n","\n","neighbors = []\n","for ixs in tqdm(emb_ds):\n","    emb = tf.gather(name_embeddings, ixs)\n","    score, indices = tf.math.top_k(tf.einsum(\"md,nd->mn\", emb, name_embeddings), k=16)\n","    neighbors.append(indices)\n","neighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\n","\n","print(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())"]},{"cell_type":"code","execution_count":null,"id":"c9e3ce48","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:49:46.897772Z","start_time":"2022-06-30T23:48:23.481556Z"},"colab":{"referenced_widgets":["be5625f0b6814b6d9e6a5f02b4bc7cf3","30b38729e8f747f59bc08629eaaecb28"]},"id":"c9e3ce48","outputId":"6d4d47e9-f8d5-423c-8ff4-eb36805c312b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be5625f0b6814b6d9e6a5f02b4bc7cf3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1113 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30b38729e8f747f59bc08629eaaecb28","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8897 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision@16 0.9011579694686475\n"]}],"source":["train_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n","                .batch(1024)\\\n","                .map(train_container.call)\n","\n","\n","\n","embeddings = []\n","for query_info in tqdm(train_ds):\n","    embeddings.append(mixer_model(query_info))\n","mix_embeddings = tf.nn.l2_normalize(tf.concat(embeddings, axis=0), axis=1)\n","                      \n","# embedding only\n","emb_ds = tf.data.Dataset.from_tensor_slices(train_ixs)\\\n","                .batch(128)\n","\n","neighbors = []\n","for ixs in tqdm(emb_ds):\n","    emb = tf.gather(mix_embeddings, ixs)\n","    score, indices = tf.math.top_k(tf.einsum(\"md,nd->mn\", emb, mix_embeddings), k=16)\n","    neighbors.append(indices)\n","neighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\n","\n","print(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())"]},{"cell_type":"code","execution_count":null,"id":"0f97c0a1","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:50:06.150280Z","start_time":"2022-06-30T23:49:46.898772Z"},"colab":{"referenced_widgets":["97955efb8984464fa433444926263055"]},"id":"0f97c0a1","outputId":"118c9ca5-a351-40d2-8424-4f4f0e5888e1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97955efb8984464fa433444926263055","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1113 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["normalized_category_dense = tf.nn.l2_normalize(cat_emb_layer.category_dense, axis=1)\n","pseudo_categories = []\n","category_ix_dict_r = {v: k for k, v in category_ix_dict.items()}\n","eval_ds = tf.data.Dataset.from_tensor_slices(train_ixs)\\\n","                .batch(1024)\\\n","                .map(train_container.call)\n","\n","for ix, name, address, position, ix_values, categories, pid, size in tqdm(eval_ds):\n","    X = tf.nn.l2_normalize(classify_model.transform(name, address, ix_values), axis=1)\n","    label = tf.argmax(tf.einsum(\"nd,md->nm\", X, normalized_category_dense), axis=1)\n","    for l in label.numpy():\n","        pseudo_categories.append(\"nan, \" + category_ix_dict_r[l])\n","        \n","def remove_test_only_category(text):\n","    res = []\n","    for x in text.split(\", \"):\n","        if x in category_ix_dict.keys():\n","            res.append(x)\n","    return \", \".join(res)\n","\n","train_df[\"categories\"] = np.vectorize(remove_test_only_category)(train_df[\"categories\"])\n","train_df[\"categories\"] = np.where(train_df[\"categories\"] == \"nan\", pseudo_categories, train_df[\"categories\"])\n","train_categories_ix = get_category_ix(train_df, category_ix_dict)"]},{"cell_type":"code","execution_count":null,"id":"bf18a27c","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:50:19.415329Z","start_time":"2022-06-30T23:50:06.151282Z"},"id":"bf18a27c"},"outputs":[],"source":["train_container =  DataContainer(train_df[container_cols],\n","                                   train_ix_values,\n","                                    train_categories_ix,\n","                                    tf.cast(tf.ragged.constant(train_df.groupby(\"pid\")[\"ix\"].unique().tolist()), \"int32\"),\n","                                neighbor_ix,\n","                                neighbor_dist)"]},{"cell_type":"code","execution_count":null,"id":"9d85b541","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:51:43.088431Z","start_time":"2022-06-30T23:50:19.416331Z"},"colab":{"referenced_widgets":["ada8b0c69c364668866e243aad1699ea","d0fdb1d44ebc4e838cb929b7105c804b"]},"id":"9d85b541","outputId":"f5737fcb-aec0-4843-b233-f7312245fd5a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ada8b0c69c364668866e243aad1699ea","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1113 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0fdb1d44ebc4e838cb929b7105c804b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8897 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision@16 0.9042013831103947\n"]}],"source":["train_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n","                .batch(1024)\\\n","                .map(train_container.call)\n","\n","\n","\n","embeddings = []\n","for query_info in tqdm(train_ds):\n","    embeddings.append(mixer_model(query_info))\n","mix_embeddings = tf.nn.l2_normalize(tf.concat(embeddings, axis=0), axis=1)\n","                      \n","# embedding only\n","emb_ds = tf.data.Dataset.from_tensor_slices(train_ixs)\\\n","                .batch(128)\n","\n","neighbors = []\n","for ixs in tqdm(emb_ds):\n","    emb = tf.gather(mix_embeddings, ixs)\n","    score, indices = tf.math.top_k(tf.einsum(\"md,nd->mn\", emb, mix_embeddings), k=16)\n","    neighbors.append(indices)\n","neighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\n","\n","print(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())"]},{"cell_type":"code","execution_count":null,"id":"89b2ea5a","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:52:42.946339Z","start_time":"2022-06-30T23:51:43.090438Z"},"colab":{"referenced_widgets":["13eec4934c8049b9975f17bf91f719a1"]},"id":"89b2ea5a","outputId":"3590b294-36f8-42ae-adee-3a266cabae70"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13eec4934c8049b9975f17bf91f719a1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8897 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision@16 0.07047811507569895\n"]}],"source":["category_embeddings = tf.nn.l2_normalize(tf.math.reduce_sum(tf.gather(cat_emb_layer.category_dense, train_categories_ix), axis=1), axis=1)\n","\n","# embedding only\n","emb_ds = tf.data.Dataset.from_tensor_slices(train_ixs)\\\n","                .batch(128)\n","\n","neighbors = []\n","for ixs in tqdm(emb_ds):\n","    emb = tf.gather(category_embeddings, ixs)\n","    score, indices = tf.math.top_k(tf.einsum(\"md,nd->mn\", emb, category_embeddings), k=16)\n","    neighbors.append(indices)\n","neighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\n","\n","print(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())\n"]},{"cell_type":"code","execution_count":null,"id":"b9b8911b","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:53:49.077285Z","start_time":"2022-06-30T23:52:42.947342Z"},"colab":{"referenced_widgets":["701af3e0950a4ced8566a92a9d73a38b","206e5fffbe634a0c9bb06a7a6a714753"]},"id":"b9b8911b","outputId":"711c538e-36e5-45a1-dcba-09492859aca5"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"701af3e0950a4ced8566a92a9d73a38b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1113 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"206e5fffbe634a0c9bb06a7a6a714753","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8897 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision@16 0.8718289779802572\n"]}],"source":["# address\n","train_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n","                .batch(1024)\\\n","                .map(train_container.call)\n","\n","embeddings = []\n","for ix, address, address, position, ix_values, categories, _, _ in tqdm(train_ds):\n","    embeddings.append(tf.nn.l2_normalize(tf.reduce_sum(address_spe(address), axis=1), axis=1))\n","address_embeddings = tf.nn.l2_normalize(tf.concat(embeddings, axis=0), axis=1)\n","\n","# embedding only\n","emb_ds = tf.data.Dataset.from_tensor_slices(train_ixs)\\\n","                .batch(128)\n","\n","neighbors = []\n","for ixs in tqdm(emb_ds):\n","    emb = tf.gather(address_embeddings, ixs)\n","    score, indices = tf.math.top_k(tf.einsum(\"md,nd->mn\", emb, address_embeddings), k=16)\n","    neighbors.append(indices)\n","neighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\n","\n","print(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())"]},{"cell_type":"code","execution_count":null,"id":"831cb84e","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:54:52.945793Z","start_time":"2022-06-30T23:53:49.078283Z"},"colab":{"referenced_widgets":["f205642c9c9d40ebbcc17de1b4487b10","dfee058643334908a3743bcf6028474b"]},"id":"831cb84e","outputId":"75473fe7-d85b-411f-b38d-64a9e75b300c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f205642c9c9d40ebbcc17de1b4487b10","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1113 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfee058643334908a3743bcf6028474b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8897 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision@16 0.1810690153365134\n"]}],"source":["# ix_values\n","train_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n","                .batch(1024)\\\n","                .map(train_container.call)\n","\n","embeddings = []\n","for ix, ix_values, ix_values, position, ix_values, categories, _, _ in tqdm(train_ds):\n","    embeddings.append(tf.nn.l2_normalize(ix_emb_layer(ix_values), axis=1))\n","ix_values_embeddings = tf.nn.l2_normalize(tf.concat(embeddings, axis=0), axis=1)\n","\n","# embedding only\n","emb_ds = tf.data.Dataset.from_tensor_slices(train_ixs)\\\n","                .batch(128)\n","\n","neighbors = []\n","for ixs in tqdm(emb_ds):\n","    emb = tf.gather(ix_values_embeddings, ixs)\n","    score, indices = tf.math.top_k(tf.einsum(\"md,nd->mn\", emb, ix_values_embeddings), k=16)\n","    neighbors.append(indices)\n","neighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\n","\n","print(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())"]},{"cell_type":"code","execution_count":null,"id":"bf1fd7da","metadata":{"ExecuteTime":{"end_time":"2022-06-30T23:54:56.278837Z","start_time":"2022-06-30T23:54:52.946794Z"},"id":"bf1fd7da"},"outputs":[],"source":["# simple version\n","\n","import tensorflow_ranking as tfr\n","\n","def haversine(X, Y):\n","    delta = Y - X\n","    x_lats = tf.gather(X, 0, axis=-1)\n","    y_lats = tf.gather(Y, 0, axis=-1)\n","    dlat = tf.gather(delta, 0, axis=-1)\n","    dlon = tf.gather(delta, 1, axis=-1)\n","\n","    a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","    c = 2 * tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a))\n","    return c\n","\n","def approx_ranks(logits):\n","    r\"\"\"Computes approximate ranks given a list of logits.\n","    Given a list of logits, the rank of an item in the list is one plus the total\n","    number of items with a larger logit. In other words,\n","    rank_i = 1 + \\sum_{j \\neq i} I_{s_j > s_i},\n","    where \"I\" is the indicator function. The indicator function can be\n","    approximated by a generalized sigmoid:\n","    I_{s_j < s_i} \\approx 1/(1 + exp(-(s_j - s_i)/temperature)).\n","    This function approximates the rank of an item using this sigmoid\n","    approximation to the indicator function. This technique is at the core\n","    of \"A general approximation framework for direct optimization of\n","    information retrieval measures\" by Qin et al.\n","    Args:\n","    logits: A `Tensor` with shape [batch_size, list_size]. Each value is the\n","      ranking score of the corresponding item.\n","    Returns:\n","    A `Tensor` of ranks with the same shape as logits.\n","    \"\"\"\n","    list_size = tf.shape(input=logits)[1]\n","    x = tf.tile(tf.expand_dims(logits, 2), [1, 1, list_size])\n","    y = tf.tile(tf.expand_dims(logits, 1), [1, list_size, 1])\n","    pairs = tf.sigmoid(y - x)\n","    return tf.reduce_sum(input_tensor=pairs, axis=-1) + .5\n","    \n","    \n","class ApproxMSE(tf.keras.layers.Layer):\n","    def __init__(self,from_logits=True, alpha=1.):\n","        super(ApproxMSE, self).__init__()\n","        self.alpha = alpha\n","        if not from_logits:\n","            NotImplementedError\n","    \n","    def call(self, y_true, y_pred, sample_weight=None):\n","        if sample_weight is None:\n","            sample_weight = tf.ones(tf.shape(y_pred), dtype=tf.float32)\n","        \n","        K = tf.cast(tf.shape(y_true)[1], \"float32\")    \n","        rank = approx_ranks(y_pred * self.alpha)\n","        label_sum = tf.reduce_sum(y_true, axis=1, keepdims=True)\n","        rmse = tf.reduce_sum(tf.maximum(0., rank - label_sum) * y_true, axis=1, keepdims=True) / (label_sum + tf.keras.backend.epsilon())\n","        \n","        return tf.reduce_sum(rmse * sample_weight) / tf.reduce_sum(sample_weight)\n","\n","class LogisticModel(tf.keras.Model):\n","    def __init__(self, dim):\n","        super(LogisticModel, self).__init__()\n","        self.scale = tf.Variable(tf.zeros([dim], dtype=\"float32\"), trainable=True)\n","        self.loss_func = ApproxMSE()#tfr.keras.losses.ApproxNDCGLoss()\n","        \n","    def call(self, X, Y):\n","        logit = tf.einsum(\"nmd,d->nm\", X, self.scale)# + self.bias\n","        loss = self.loss_func(Y, logit)\n","        return loss    \n","    \n","    \n","from sklearn.feature_extraction.text import TfidfVectorizer\n","bagofword_vectorizer = TfidfVectorizer(min_df=0, binary=True, use_idf=False, norm=\"l2\", dtype=np.float32)\n","name_bow = bagofword_vectorizer.fit_transform(train_df[\"sp_name\"]).tocoo()\n","\n","name_bow_rag_data = tf.RaggedTensor.from_value_rowids(name_bow.data, name_bow.row)\n","name_bow_rag_col = tf.RaggedTensor.from_value_rowids(name_bow.col, name_bow.row)\n","\n","name_bow_coo = tf.SparseTensor(tf.cast(tf.stack([name_bow.row, name_bow.col], axis=-1), \"int64\"), name_bow.data, name_bow.shape)"]},{"cell_type":"code","execution_count":null,"id":"7547ea2d","metadata":{"ExecuteTime":{"end_time":"2022-07-01T00:29:12.578824Z","start_time":"2022-06-30T23:54:56.279838Z"},"colab":{"referenced_widgets":["481e6f339d2a437c8bd362ae51db44f7"]},"id":"7547ea2d","outputId":"341d4dc2-022b-4bf1-9e7a-d290fca9b3ff"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"481e6f339d2a437c8bd362ae51db44f7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/35587 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["simple_logistic_model = LogisticModel(5)\n","history = {\"logistic_loss\": []}\n","logistic_loss = tf.keras.metrics.Mean(name='loss')\n","logistic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n","\n","train_ixs = tf.range(len(train_container.name))\n","train_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n","                .shuffle(len(train_ixs), reshuffle_each_iteration=True)\\\n","                .batch(32, drop_remainder=True)\\\n","                .map(train_container.call)\n","\n","@tf.function(experimental_relax_shapes=True)\n","def logistic_forward_step(X, Y):\n","    with tf.GradientTape() as tape:\n","        loss = simple_logistic_model(X, Y, training=True)\n","    gradients = tape.gradient(loss, simple_logistic_model.trainable_variables)\n","    if tf.reduce_all([tf.reduce_all(tf.math.is_finite(g)) for g in gradients]):\n","        logistic_optimizer.apply_gradients(zip(gradients, simple_logistic_model.trainable_variables))\n","    return loss\n","\n","@tf.function(experimental_relax_shapes=True)\n","def get_candidates(query_info, move_scale):\n","    ix, name, address, position, ix_values, categories, pid, size = query_info\n","    X = tf.expand_dims(train_container.get_position(ix), axis=1)\n","    dist = train_container.log_haversine(X, train_container.position)\n","    name_cossim = tf.einsum(\"nd,md->nm\", tf.gather(name_embeddings, ix), name_embeddings)\n","    address_cossim = tf.einsum(\"nd,md->nm\", tf.gather(address_embeddings, ix), address_embeddings)\n","    category_cossim = tf.einsum(\"nd,md->nm\", tf.gather(category_embeddings, ix), category_embeddings)\n","\n","    #coo = (name_bow[ix] * name_bow.T).tocoo()\n","    #sparse_name_sim = tf.scatter_nd(tf.stack([coo.row, coo.col], axis=-1), coo.data, [len(ix), len(train_df)])\n","    indices = tf.stack([tf.repeat(tf.range(len(ix)), tf.gather(name_bow_rag_data, ix).row_lengths()),\n","                        tf.gather(name_bow_rag_col, ix).values], axis=-1)\n","    values = tf.gather(name_bow_rag_data, ix).values\n","    shape = [len(ix), name_bow_coo.shape[1]]\n","    query = tf.scatter_nd(indices, values, shape)\n","    sparse_name_sim = tf.sparse.sparse_dense_matmul(query, name_bow_coo, adjoint_b=True)\n","\n","    score = tf.einsum(\"d,nmd->nm\", move_scale, tf.stack([dist, sparse_name_sim, name_cossim, address_cossim, category_cossim], axis=-1))\n","    #except_loc = tf.reshape(tf.stack([tf.repeat(tf.expand_dims(tf.range(len(X)), axis=1), 4, axis=1),\n","    #                tf.math.top_k(-dist, k=4)[1]], axis=-1), [-1, 2])\n","\n","\n","    neg_dist, neg_ixs = train_container.negative_sample_by_score_top_k(ix, score, except_loc=None, k=1024)\n","\n","    pos_dist, pos_ixs = train_container.get_positive_info(ix)\n","    pos_order = tf.slice(tf.argsort(pos_dist, axis=1), [0, 0], [-1, tf.minimum(tf.shape(pos_dist)[1], 64)])\n","    pos_dist = tf.gather(pos_dist, pos_order, batch_dims=1)\n","    pos_ixs = tf.gather(pos_ixs, pos_order, batch_dims=1)\n","    pos_ix = tf.where(tf.expand_dims(ix, axis=1) != pos_ixs, pos_ixs, -1)\n","\n","    update_ixs = tf.where(pos_ixs >= 0)\n","    target_ixs = tf.tensor_scatter_nd_update(neg_ixs, update_ixs, tf.gather_nd(pos_ixs, update_ixs))\n","    target_label = tf.tensor_scatter_nd_update(tf.zeros(tf.shape(neg_dist)), update_ixs, tf.ones([tf.shape(update_ixs)[0]]))\n","    target_dist = tf.gather(dist, target_ixs, batch_dims=1)\n","    target_name_cossim = tf.gather(name_cossim, target_ixs, batch_dims=1)\n","    target_sparse_name_sim = tf.gather(sparse_name_sim, target_ixs, batch_dims=1)\n","    target_address_cossim = tf.gather(address_cossim, target_ixs, batch_dims=1)\n","    target_category_cossim = tf.gather(category_cossim, target_ixs, batch_dims=1)\n","\n","    target_feat = tf.stack([target_dist, target_sparse_name_sim, target_name_cossim, target_address_cossim, target_category_cossim], axis=-1)\n","    return target_feat, target_label     \n","\n","move_scale = tf.convert_to_tensor([-1., 0., 0., 0., 0.], dtype=\"float32\")\n","\n","with tqdm(total=len(train_ds)) as pbar:\n","    step = 0\n","    for query_info in train_ds:\n","        target_feat, target_label = get_candidates(query_info, move_scale)\n","        loss = logistic_forward_step(target_feat, target_label)\n","        move_scale = move_scale * .99 + (simple_logistic_model.scale * (1. - .99))\n","        if tf.math.is_finite(loss):\n","            logistic_loss(loss)\n","        progress_text = \"logistic_loss: {:.4f}\".format(logistic_loss.result().numpy())\n","        pbar.set_postfix_str(progress_text)\n","        pbar.update(1)\n","        step += 1"]},{"cell_type":"code","execution_count":null,"id":"3dd8fa5c","metadata":{"ExecuteTime":{"end_time":"2022-07-01T00:29:12.594824Z","start_time":"2022-07-01T00:29:12.579824Z"},"id":"3dd8fa5c","outputId":"c06ecf81-5d21-4987-ed57-befd79d5cf07"},"outputs":[{"name":"stdout","output_type":"stream","text":["logistic_loss: 8.1173\n"]},{"data":{"text/plain":["<tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=\n","array([-4.289177 , 10.927236 , 24.742352 ,  8.860054 ,  6.9624615],\n","      dtype=float32)>"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["print(progress_text)\n","simple_logistic_model.scale"]},{"cell_type":"code","execution_count":null,"id":"94fa888b","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:04:33.987846Z","start_time":"2022-07-01T00:29:12.595825Z"},"colab":{"referenced_widgets":["2f0d25e4b4b24f6e9d9a5580b9204320"]},"id":"94fa888b","outputId":"1f87b806-16ea-4558-ca8d-154773578f52"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f0d25e4b4b24f6e9d9a5580b9204320","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/71176 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["precision@16 0.9724738386633075\n","precision@100 0.9900192900391731\n"]}],"source":["# embedding + haversine\n","train_ixs_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n","                .batch(16)\n","\n","neighbors = []\n","dist_ranks = []\n","\n","@tf.function(experimental_relax_shapes=True)\n","def get_indices(ix):\n","    X = tf.expand_dims(train_container.get_position(ix), axis=1)\n","    dist = train_container.log_haversine(X, train_container.position)\n","    indices = tf.stack([tf.repeat(tf.range(len(ix)), tf.gather(name_bow_rag_data, ix).row_lengths()),\n","                        tf.gather(name_bow_rag_col, ix).values], axis=-1)\n","    values = tf.gather(name_bow_rag_data, ix).values\n","    shape = [len(ix), name_bow_coo.shape[1]]\n","    query = tf.scatter_nd(indices, values, shape)\n","    sparse_name_sim = tf.sparse.sparse_dense_matmul(query, name_bow_coo, adjoint_b=True)\n","    \n","    name_cossim = tf.einsum(\"nd,md->nm\", tf.gather(name_embeddings, ix), name_embeddings)\n","    category_cossim = tf.einsum(\"nd,md->nm\", tf.gather(category_embeddings, ix), category_embeddings)\n","    address_cossim = tf.einsum(\"nd,md->nm\", tf.gather(address_embeddings, ix), address_embeddings)\n","    score = tf.einsum(\"d,nmd->nm\", simple_logistic_model.scale, tf.stack([dist, sparse_name_sim, name_cossim, address_cossim, category_cossim], axis=-1))\n","    \n","    _, indices = tf.math.top_k(score, k=100)\n","    return indices\n","    \n","for ix in tqdm(train_ixs_ds):\n","    indices = get_indices(ix)\n","    #dist_rank = tf.gather(tf.argsort(tf.argsort(-dist, axis=1), axis=1), indices, batch_dims=-1)\n","    #dist_ranks.append(dist_rank)\n","    neighbors.append(indices)\n","\n","neighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1).numpy()\n","print(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors[:, :16]]).sum(axis=1) / train_df[\"true_count\"]).mean())\n","print(\"precision@100\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors[:, :100]]).sum(axis=1) / train_df[\"true_count\"]).mean())\n"]},{"cell_type":"code","execution_count":null,"id":"6c2598c8","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:04:35.104892Z","start_time":"2022-07-01T01:04:33.988846Z"},"id":"6c2598c8","outputId":"2e84ffb4-3765-461a-8376-1216d7cca782"},"outputs":[{"name":"stdout","output_type":"stream","text":["precision@20 0.9756721880668088\n","precision@30 0.9805842104666077\n","precision@50 0.9852901910022868\n"]}],"source":["import gc\n","gc.collect()\n","\n","print(\"precision@20\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors[:, :20]]).sum(axis=1) / train_df[\"true_count\"]).mean())\n","print(\"precision@30\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors[:, :30]]).sum(axis=1) / train_df[\"true_count\"]).mean())\n","print(\"precision@50\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors[:, :50]]).sum(axis=1) / train_df[\"true_count\"]).mean())\n"]},{"cell_type":"code","execution_count":null,"id":"cddbdba5","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:04:35.120892Z","start_time":"2022-07-01T01:04:35.105893Z"},"id":"cddbdba5"},"outputs":[],"source":["\n","\n","\n","class TFMiniBatchHKmeans(tf.keras.Model):\n","    def __init__(self, n_cluster=64, learning_rate=0.1):\n","        super(TFMiniBatchHKmeans, self).__init__()\n","        self.n_cluster = n_cluster\n","        self.learning_rate = learning_rate\n","        self.centroids = tf.Variable(tf.zeros((n_cluster, 2), dtype=\"float32\"), trainable=False)\n","    \n","    def init_centroid(self, X):\n","        new_centroids = []\n","        choosed = tf.zeros(len(X), dtype=\"bool\")\n","\n","        ix = tf.cast(tf.random.uniform([1], maxval=len(X))//1, \"int32\")[0]\n","        choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","        new_centroids.append(tf.gather(X, ix))\n","        dist = tf.ones([tf.shape(X)[0]]) * tf.float32.max / 10\n","\n","        for i in tqdm(range(self.n_cluster-1)):\n","            centroid = tf.gather(X, ix)\n","    \n","            dist = tf.where(choosed, tf.keras.backend.epsilon() , tf.minimum(dist, tf.math.square(hkmeans.haversine(centroid, X))))\n","            prob = dist / tf.reduce_sum(dist, axis=0, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            ix = tf.cast(tf.argmax(gumbel), \"int32\")\n","            choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","            new_centroids.append(tf.gather(X, ix))\n","        self.centroids.assign(tf.stack(new_centroids, axis=0))\n","        \n","    def partial_fit(self, X):\n","        score = self.haversine(tf.expand_dims(X, axis=1), tf.expand_dims(self.centroids, axis=0))\n","        maxix = tf.argmin(score, axis=1)\n","        new_centroids = tf.math.unsorted_segment_mean(X, maxix, num_segments=self.n_cluster)\n","        num_member = tf.math.unsorted_segment_sum(tf.ones((len(X), 1)), maxix, num_segments=self.n_cluster)\n","        new_centroids = tf.where(num_member > 0, new_centroids, self.centroids)\n","        new_centroids = new_centroids * self.learning_rate + self.centroids * (1 - self.learning_rate)\n","        dist = tf.reduce_mean(tf.reduce_min(score, axis=1))\n","        self.centroids.assign(new_centroids)\n","        return dist\n","    \n","    def haversine(self, X, Y):\n","        delta = Y - X\n","        x_lats = tf.gather(X, 0, axis=-1)\n","        y_lats = tf.gather(Y, 0, axis=-1)\n","        dlat = tf.gather(delta, 0, axis=-1)\n","        dlon = tf.gather(delta, 1, axis=-1)\n","\n","        a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n","        c = tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a))\n","        return c\n","    \n","    def transform(self, X):\n","        res = tf.argmin(self.haversine(tf.expand_dims(X, axis=1), tf.expand_dims(self.centroids, axis=0)), axis=-1)\n","        return res\n","\n","class TFMiniBatchSKmeans(tf.keras.Model):\n","    def __init__(self, n_cluster=64, n_dim=100, alpha=3/2, learning_rate=0.1):\n","        super(TFMiniBatchSKmeans, self).__init__()\n","        self.n_dim = n_dim\n","        self.n_cluster = n_cluster\n","        self.alpha = alpha\n","        self.learning_rate = learning_rate\n","\n","        self.centroids = tf.Variable(tf.zeros((n_cluster, n_dim), dtype=\"float32\"), trainable=False)\n","    \n","    def init_centroid(self, X):\n","        new_centroids = []\n","        X = tf.nn.l2_normalize(X, axis=-1)\n","        choosed = tf.zeros(len(X), dtype=\"bool\")\n","\n","        ix = tf.cast(tf.random.uniform([1], maxval=len(X))//1, \"int32\")[0]\n","        choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","        new_centroids.append(tf.gather(X, ix))\n","        \n","        dist = tf.ones([tf.shape(X)[0]]) * tf.float32.max / 10\n","\n","        for i in tqdm(range(self.n_cluster-1)):\n","            centroid = tf.gather(X, ix)\n","            cos = tf.einsum(\"d,nd->n\", centroid, X)\n","            dist = tf.where(choosed, tf.keras.backend.epsilon() , tf.maximum(tf.keras.backend.epsilon(), tf.minimum(dist, 1. - cos)))         \n","            prob = dist / tf.reduce_sum(dist, axis=0, keepdims=True)\n","            logit = tf.math.log(prob) - tf.math.log(1-prob)        \n","            gumbel = logit - tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logit))))\n","            ix = tf.cast(tf.argmax(gumbel), \"int32\")\n","            choosed = tf.where(tf.range(len(X)) == ix, True, choosed)\n","            new_centroids.append(X[ix])\n","        self.centroids.assign(tf.stack(new_centroids, axis=0))\n","        \n","    @tf.function\n","    def partial_fit(self, X):\n","        score = tf.einsum(\"nd,kd->nk\", X, self.centroids)\n","        maxix = tf.argmax(score, axis=1)\n","        new_centroids = tf.math.unsorted_segment_mean(X, maxix, num_segments=self.n_cluster)\n","        num_member = tf.math.unsorted_segment_sum(tf.ones((len(X), 1)), maxix, num_segments=self.n_cluster)\n","        new_centroids = tf.where(num_member > 0, new_centroids, self.centroids)\n","        new_centroids = tf.nn.l2_normalize(new_centroids, axis=1)\n","        new_centroids = new_centroids * self.learning_rate + self.centroids * (1 - self.learning_rate)\n","        new_centroids = tf.nn.l2_normalize(new_centroids, axis=1)\n","        dist = tf.reduce_mean(self.alpha - tf.reduce_max(score, axis=1))\n","        self.centroids.assign(new_centroids)\n","        return dist\n","    \n","    def transform(self, X):\n","        res = tf.argmax(tf.einsum(\"...d,kd->...k\", X, self.centroids), axis=-1)\n","        return res"]},{"cell_type":"code","execution_count":null,"id":"b762f6a3","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:10:22.495880Z","start_time":"2022-07-01T01:04:35.121893Z"},"colab":{"referenced_widgets":["c13ee906ab88479787af0c1d232e5c26","7502881b99ec4cd5866a942962e33abc"]},"id":"b762f6a3","outputId":"0d281395-10f2-4ad3-b488-c3301fcf88ba"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c13ee906ab88479787af0c1d232e5c26","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2039 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7502881b99ec4cd5866a942962e33abc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2040, 2) dtype=float32, numpy=\n","array([[ 9.4021484e-02, -9.3767560e-01],\n","       [ 3.9109762e-04, -8.9150327e-01],\n","       [-5.5816524e-02, -9.1121477e-01],\n","       ...,\n","       [ 1.5525542e-01, -1.1209325e+00],\n","       [ 1.4277864e-01, -1.0969130e+00],\n","       [ 4.9140606e-02, -1.0596082e+00]], dtype=float32)>"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["hkmeans = TFMiniBatchHKmeans(255*8, learning_rate=1.)\n","hkmeans.init_centroid(train_container.position[0])\n","\n","scores = []\n","with tqdm(total=2000) as pbar:\n","    for _ in range(2000):\n","        ixs = np.random.choice(train_ixs, 100000, False)\n","        score = hkmeans.partial_fit(tf.gather(train_container.position[0], ixs))\n","        progress_text = \"{:4f}\".format(score.numpy())\n","        pbar.set_postfix_str(progress_text)    \n","        pbar.update(1)\n","        \n","centroid_position = hkmeans.centroids.numpy()\n","data = {}\n","data[\"distance_matrix\"] = (haversine(tf.expand_dims(centroid_position, axis=1), tf.expand_dims(centroid_position, axis=0)).numpy() * 1e9).astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","hkmeans.centroids.assign(tf.gather(hkmeans.centroids, routes))"]},{"cell_type":"code","execution_count":null,"id":"6966a3cf","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:10:37.818810Z","start_time":"2022-07-01T01:10:22.496882Z"},"id":"6966a3cf"},"outputs":[],"source":["category_embedding = tf.nn.l2_normalize(cat_emb_layer.category_dense, axis=1)\n","\n","data = {}\n","data[\"distance_matrix\"] = ((1 - tf.einsum(\"nd,md->nm\", category_embedding, category_embedding)) / 2. * (1. - tf.eye(tf.shape(category_embedding)[0])) * 1e9).numpy().astype(np.int64).tolist()\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","# center frequent value\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","\n","category_df = pd.DataFrame({\"category\": [category_ix_dict_r[id_] for id_ in routes]})\n","category_df[\"category_ix\"] = category_df[\"category\"].map(category_ix_dict)\n","category_df[\"category_tsp_ix\"] = category_df.index"]},{"cell_type":"code","execution_count":null,"id":"e1688a0e","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:13:44.784642Z","start_time":"2022-07-01T01:10:37.819811Z"},"colab":{"referenced_widgets":["18f616cd1fd64aa490620519d21b5140","06821979d4554392a6e2c3f3612feb04"]},"id":"e1688a0e","outputId":"780f4b79-c74b-4a44-d156-54eced8b398e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18f616cd1fd64aa490620519d21b5140","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2039 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06821979d4554392a6e2c3f3612feb04","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2040, 256) dtype=float32, numpy=\n","array([[ 0.05367883, -0.09877654, -0.0362552 , ..., -0.02462533,\n","         0.02518191,  0.04509797],\n","       [-0.03846163, -0.07003859, -0.0143156 , ..., -0.07688364,\n","         0.06933416,  0.0271528 ],\n","       [ 0.05181199,  0.0245278 ,  0.00708091, ..., -0.02643215,\n","         0.11937096, -0.02803057],\n","       ...,\n","       [ 0.03661745, -0.02569252,  0.02217283, ..., -0.05264423,\n","        -0.10504616,  0.02601274],\n","       [ 0.07170249, -0.08004361, -0.01019648, ..., -0.06460643,\n","        -0.02874802,  0.01596966],\n","       [ 0.0722662 , -0.0015444 , -0.02889389, ..., -0.04342653,\n","         0.01560971,  0.01888697]], dtype=float32)>"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["name_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(name_embeddings)[1], learning_rate=1.)\n","name_kmeans.init_centroid(tf.gather(name_embeddings, train_df.drop_duplicates(\"name\").index))\n","with tqdm(total=1000) as pbar:\n","    for _ in range(1000):\n","        ixs = np.random.choice(train_ixs, 100000, False)\n","        score = name_kmeans.partial_fit(tf.gather(name_embeddings, ixs))\n","        progress_text = \"{:4f}\".format(score.numpy())\n","        pbar.set_postfix_str(progress_text)    \n","        pbar.update(1)\n","\n","data = {}\n","data[\"distance_matrix\"] = ((1. - tf.einsum(\"nd,md->nm\", name_kmeans.centroids, name_kmeans.centroids)) * 1e9).numpy().astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","name_kmeans.centroids.assign(tf.gather(name_kmeans.centroids, routes))"]},{"cell_type":"code","execution_count":null,"id":"72e08dd2","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:17:49.806362Z","start_time":"2022-07-01T01:13:44.785643Z"},"colab":{"referenced_widgets":["be9a097e47ed4f2eadb4b95697db0563","ff10454685c0453983089a6e207ca3a6"]},"id":"72e08dd2","outputId":"630b06a6-e2f2-4fcc-cbe8-ada27a7cc93e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be9a097e47ed4f2eadb4b95697db0563","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2039 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff10454685c0453983089a6e207ca3a6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2040, 256) dtype=float32, numpy=\n","array([[-0.01540297, -0.10685989,  0.04598546, ..., -0.03650163,\n","         0.0284561 ,  0.1487307 ],\n","       [-0.07576877, -0.0263731 ,  0.03517479, ...,  0.08155105,\n","         0.00445443, -0.03914481],\n","       [-0.04656794,  0.06876873, -0.06128385, ..., -0.0618774 ,\n","         0.05225549,  0.00664888],\n","       ...,\n","       [-0.11305363,  0.04342153, -0.02193956, ...,  0.06300627,\n","         0.00578293, -0.04309898],\n","       [-0.01640577,  0.12669231, -0.01856684, ...,  0.03727403,\n","         0.0161635 , -0.08416845],\n","       [-0.01588172,  0.02062212, -0.01316744, ...,  0.07660025,\n","         0.0980431 , -0.04491289]], dtype=float32)>"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["address_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(address_embeddings)[1], learning_rate=1.)\n","address_kmeans.init_centroid(tf.gather(address_embeddings, train_df.drop_duplicates(\"address\").index))\n","with tqdm(total=1000) as pbar:\n","    for _ in range(1000):\n","        ixs = np.random.choice(train_ixs, 100000, False)\n","        score = address_kmeans.partial_fit(tf.gather(address_embeddings, ixs))\n","        progress_text = \"{:4f}\".format(score.numpy())\n","        pbar.set_postfix_str(progress_text)    \n","        pbar.update(1)\n","\n","data = {}\n","data[\"distance_matrix\"] = ((1. - tf.einsum(\"nd,md->nm\", address_kmeans.centroids, address_kmeans.centroids)) * 1e9).numpy().astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","address_kmeans.centroids.assign(tf.gather(address_kmeans.centroids, routes))"]},{"cell_type":"code","execution_count":null,"id":"6d6ffcc2","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:21:15.875092Z","start_time":"2022-07-01T01:17:49.807364Z"},"colab":{"referenced_widgets":["8bae18bf7a7e418888174d3e713baeff","1322f05306654361ab8360d936d0433d"]},"id":"6d6ffcc2","outputId":"735b0311-ff44-4198-8ccd-b1ff35f78ae6"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8bae18bf7a7e418888174d3e713baeff","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2039 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1322f05306654361ab8360d936d0433d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2040, 256) dtype=float32, numpy=\n","array([[-0.03490861,  0.07001233,  0.07232594, ...,  0.08803911,\n","        -0.03294925, -0.0022645 ],\n","       [-0.01074211,  0.07262552, -0.01140475, ...,  0.02053834,\n","        -0.11359691, -0.01559312],\n","       [ 0.04806343,  0.09930054, -0.00049694, ...,  0.04805629,\n","        -0.07743084, -0.03839194],\n","       ...,\n","       [-0.1065522 ,  0.00829812,  0.05307719, ..., -0.02529685,\n","        -0.01690373, -0.00807665],\n","       [-0.04488681, -0.00280879,  0.03796248, ..., -0.02585155,\n","        -0.06523383, -0.03156277],\n","       [-0.07051554,  0.00851297,  0.0908815 , ...,  0.05799394,\n","        -0.00529168,  0.00387569]], dtype=float32)>"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["category_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(category_embeddings)[1], learning_rate=1.)\n","category_kmeans.init_centroid(tf.gather(category_embeddings, train_df.drop_duplicates(\"categories\").index))\n","with tqdm(total=1000) as pbar:\n","    for _ in range(1000):\n","        ixs = np.random.choice(train_ixs, 100000, False)\n","        score = category_kmeans.partial_fit(tf.gather(category_embeddings, ixs))\n","        progress_text = \"{:4f}\".format(score.numpy())\n","        pbar.set_postfix_str(progress_text)    \n","        pbar.update(1)\n","\n","data = {}\n","data[\"distance_matrix\"] = ((1. - tf.einsum(\"nd,md->nm\", category_kmeans.centroids, category_kmeans.centroids)) * 1e9).numpy().astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","category_kmeans.centroids.assign(tf.gather(category_kmeans.centroids, routes))"]},{"cell_type":"code","execution_count":null,"id":"3c944add","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:25:53.848115Z","start_time":"2022-07-01T01:21:15.876095Z"},"colab":{"referenced_widgets":["9b03e230f8054da19b60bfa2ad4e8a6f","d159c4b5a02d470ead8b8717a11645f9"]},"id":"3c944add","outputId":"72228cfd-a7da-451a-9d04-8e58f9adb095"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b03e230f8054da19b60bfa2ad4e8a6f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2039 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d159c4b5a02d470ead8b8717a11645f9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2040, 256) dtype=float32, numpy=\n","array([[-0.02488252, -0.00290189, -0.00338166, ...,  0.01909287,\n","        -0.03749029,  0.00595636],\n","       [-0.02683397, -0.00560614, -0.00079461, ...,  0.02416875,\n","        -0.04125655,  0.0053868 ],\n","       [-0.02655462, -0.00474923, -0.00684417, ...,  0.02659443,\n","        -0.05218446, -0.00306095],\n","       ...,\n","       [ 0.0024772 ,  0.00661306, -0.0056834 , ...,  0.04607531,\n","        -0.02661628,  0.02193001],\n","       [ 0.00315666,  0.00671522, -0.00205756, ...,  0.04134945,\n","        -0.02631197,  0.01930606],\n","       [-0.00847554, -0.00494793,  0.0020312 , ...,  0.03560913,\n","        -0.02989377,  0.01253536]], dtype=float32)>"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["ix_values_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(ix_values_embeddings)[1], learning_rate=1.)\n","ix_values_kmeans.init_centroid(tf.gather(ix_values_embeddings, train_df.index))\n","with tqdm(total=1000) as pbar:\n","    for _ in range(1000):\n","        ixs = np.random.choice(train_ixs, 100000, False)\n","        score = ix_values_kmeans.partial_fit(tf.gather(ix_values_embeddings, ixs))\n","        progress_text = \"{:4f}\".format(score.numpy())\n","        pbar.set_postfix_str(progress_text)    \n","        pbar.update(1)\n","\n","data = {}\n","data[\"distance_matrix\"] = ((1. - tf.einsum(\"nd,md->nm\", ix_values_kmeans.centroids, ix_values_kmeans.centroids)) * 1e9).numpy().astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","ix_values_kmeans.centroids.assign(tf.gather(ix_values_kmeans.centroids, routes))"]},{"cell_type":"code","execution_count":null,"id":"b1b896b6","metadata":{"ExecuteTime":{"end_time":"2022-07-01T01:29:52.451015Z","start_time":"2022-07-01T01:25:53.849116Z"},"colab":{"referenced_widgets":["007ccc1198a34fc391b5f38f9cac0336","6229ca79b1e646febdd8999fa966009b"]},"id":"b1b896b6","outputId":"3d4e7d05-ee7c-414e-c8ba-14ccc1e8d77f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"007ccc1198a34fc391b5f38f9cac0336","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2039 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6229ca79b1e646febdd8999fa966009b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2040, 256) dtype=float32, numpy=\n","array([[-0.05677956,  0.03678567, -0.09070697, ..., -0.0722867 ,\n","        -0.00662374, -0.0484607 ],\n","       [ 0.00336831, -0.02808611, -0.06830411, ..., -0.04615078,\n","        -0.01346683,  0.0071326 ],\n","       [ 0.01912913, -0.09788226, -0.03938286, ..., -0.04352076,\n","         0.04196085,  0.08501685],\n","       ...,\n","       [ 0.06827363, -0.07867252,  0.02086764, ..., -0.02036259,\n","         0.01449005,  0.08939813],\n","       [-0.04306776,  0.08950221,  0.02987495, ..., -0.05019516,\n","         0.05872185,  0.08296056],\n","       [-0.06479619,  0.02037481, -0.09785016, ..., -0.01392203,\n","         0.02658109, -0.10147958]], dtype=float32)>"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["mix_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(mix_embeddings)[1], learning_rate=1.)\n","mix_kmeans.init_centroid(tf.gather(mix_embeddings, train_df.index))\n","with tqdm(total=1000) as pbar:\n","    for _ in range(1000):\n","        ixs = np.random.choice(train_ixs, 100000, False)\n","        score = mix_kmeans.partial_fit(tf.gather(mix_embeddings, ixs))\n","        progress_text = \"{:4f}\".format(score.numpy())\n","        pbar.set_postfix_str(progress_text)    \n","        pbar.update(1)\n","\n","data = {}\n","data[\"distance_matrix\"] = ((1. - tf.einsum(\"nd,md->nm\", mix_kmeans.centroids, mix_kmeans.centroids)) * 1e9).numpy().astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","mix_kmeans.centroids.assign(tf.gather(mix_kmeans.centroids, routes))"]},{"cell_type":"code","execution_count":null,"id":"221a8934","metadata":{"ExecuteTime":{"end_time":"2022-07-01T02:23:22.618583Z","start_time":"2022-07-01T02:20:15.341837Z"},"id":"221a8934","outputId":"3c041e61-d950-4375-f188-ce37c2cc434f"},"outputs":[{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2040, 256) dtype=float32, numpy=\n","array([[-0.04098925, -0.06080123, -0.02781636, ...,  0.05629943,\n","         0.00336294,  0.11467186],\n","       [-0.06531695, -0.05341464, -0.02680026, ...,  0.04961438,\n","        -0.04938837,  0.10250378],\n","       [ 0.01846442, -0.06393591,  0.09954432, ...,  0.02021522,\n","        -0.05345864,  0.02081409],\n","       ...,\n","       [-0.08571657, -0.04134927,  0.01134916, ...,  0.07846334,\n","        -0.06121435,  0.07562376],\n","       [-0.08205356, -0.02641124, -0.01154182, ...,  0.12519097,\n","        -0.09823461,  0.08581351],\n","       [-0.05179709, -0.04558831, -0.03449196, ...,  0.09866905,\n","        -0.04233998,  0.11391226]], dtype=float32)>"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["brand_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(name_embeddings)[1], learning_rate=1.)\n","\n","brand_dict = {k: v for v, k in enumerate(train_df[\"name\"].value_counts().head(255*8).index)}\n","brand_df = train_df[train_df[\"name\"].isin(brand_dict.keys())].drop_duplicates(\"name\")[[\"name\", \"ix\"]]\n","brand_df[\"brand_ix\"] = brand_df[\"name\"].map(brand_dict)\n","brand_embeddings = tf.scatter_nd(tf.expand_dims(brand_df[\"brand_ix\"].values, axis=1),\n","                                 tf.gather(name_embeddings, brand_df[\"ix\"].values),\n","                                 [len(brand_df), tf.shape(name_embeddings)[1]])\n","\n","data = {}\n","data[\"distance_matrix\"] = ((1. - tf.einsum(\"nd,md->nm\", brand_embeddings, brand_embeddings)) * 1e9).numpy().astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","brand_df[\"brand_tsp_ix\"] = brand_df[\"brand_ix\"].map({v: k for k, v in enumerate(routes)})\n","\n","brand_kmeans.centroids.assign(tf.gather(brand_embeddings, routes))\n"]},{"cell_type":"code","execution_count":null,"id":"88db22bc","metadata":{"ExecuteTime":{"end_time":"2022-07-01T02:28:35.908535Z","start_time":"2022-07-01T02:28:35.905535Z"},"id":"88db22bc","outputId":"d033d6d3-4a2b-474d-dcc5-1afff4fad2be"},"outputs":[{"data":{"text/plain":["207264"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["lrnz"]},{"cell_type":"code","execution_count":null,"id":"b0b57885","metadata":{"ExecuteTime":{"end_time":"2022-07-01T02:31:40.398425Z","start_time":"2022-07-01T02:28:59.903109Z"},"id":"b0b57885","outputId":"6a24bb60-7f07-4dd5-f2fd-e91c130d7114"},"outputs":[{"data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2040, 256) dtype=float32, numpy=\n","array([[-0.03828888, -0.03829809, -0.02672131, ...,  0.00162226,\n","         0.11269024, -0.05158409],\n","       [-0.02390731, -0.01677149, -0.09876301, ...,  0.01229284,\n","         0.06156249, -0.08371226],\n","       [-0.02154939,  0.03645154,  0.04138213, ..., -0.06648063,\n","         0.03844417, -0.04540368],\n","       ...,\n","       [-0.04124658, -0.04311758,  0.03895446, ..., -0.02510447,\n","         0.13534428, -0.0195477 ],\n","       [-0.04237897, -0.03922258,  0.04113974, ..., -0.02296809,\n","         0.13632789, -0.01559151],\n","       [-0.04126463, -0.04455078,  0.0377099 , ..., -0.02197584,\n","         0.13566008, -0.01893293]], dtype=float32)>"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["brand_pid_kmeans = TFMiniBatchSKmeans(255*8, n_dim=tf.shape(name_embeddings)[1], learning_rate=1.)\n","\n","brand_pid_df = train_df[train_df[\"name\"].isin(brand_dict.keys())][[\"name\", \"ix\", \"pid\"]]\n","brand_pid_df[\"brand_pid_ix\"] = brand_pid_df[\"name\"].map(brand_dict)\n","not_brand_pid_df = train_df[~train_df[\"name\"].isin(brand_dict.keys())][[\"ix\", \"pid\"]]\n","brand_pid_df = brand_pid_df.merge(not_brand_pid_df, on=\"pid\", how=\"left\")\n","brand_pid_df[\"ix\"] = np.where(brand_pid_df[\"ix_y\"].isna(), brand_pid_df[\"ix_x\"], brand_pid_df[\"ix_y\"]).astype(np.int32)\n","brand_pid_embeddings = tf.nn.l2_normalize(tf.math.unsorted_segment_mean(tf.gather(name_embeddings, brand_pid_df[\"ix\"].values),\n","                                                                          brand_pid_df[\"brand_pid_ix\"].values,\n","                                                                          255*8), axis=-1)\n","\n","\n","\n","data = {}\n","data[\"distance_matrix\"] = ((1. - tf.einsum(\"nd,md->nm\", brand_pid_embeddings, brand_pid_embeddings)) * 1e9).numpy().astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","brand_pid_df[\"brand_pid_tsp_ix\"] = brand_pid_df[\"brand_pid_ix\"].map({v: k for k, v in enumerate(routes)})\n","\n","brand_pid_kmeans.centroids.assign(tf.gather(brand_pid_embeddings, routes))\n"]},{"cell_type":"code","execution_count":null,"id":"3f1a5d7a","metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:03:40.563924Z","start_time":"2022-07-01T03:03:39.905233Z"},"id":"3f1a5d7a"},"outputs":[],"source":["hkmeans.save_weights(nadare_feature_dir + \"haversine_kmeans\")\n","name_kmeans.save_weights(nadare_feature_dir + \"name_kmeans\")\n","address_kmeans.save_weights(nadare_feature_dir + \"address_kmeans\")\n","category_kmeans.save_weights(nadare_feature_dir + \"category_kmeans\")\n","ix_values_kmeans.save_weights(nadare_feature_dir + \"ix_value_kmeans\")\n","mix_kmeans.save_weights(nadare_feature_dir + \"mix_kmeans\")\n","brand_kmeans.save_weights(nadare_feature_dir + \"brand_kmeans\")\n","brand_pid_kmeans.save_weights(nadare_feature_dir + \"brand_pid_kmeans\")\n","\n","category_df.to_csv(nadare_feature_dir + \"category_tsp_index.csv\", index=None, encoding=\"utf-8\")\n","brand_df.to_csv(nadare_feature_dir + \"brand_tsp.csv\", index=None, encoding=\"utf-8\")\n","brand_pid_df.to_csv(nadare_feature_dir + \"brand_pid_tsp.csv\", index=None, encoding=\"utf-8\")\n","classify_model.save_weights(nadare_feature_dir + \"classify_model\")\n","mixer_model.save_weights(nadare_feature_dir + \"mixer_model\")"]},{"cell_type":"code","execution_count":null,"id":"07f063c5","metadata":{"ExecuteTime":{"end_time":"2022-07-01T02:33:54.569678Z","start_time":"2022-07-01T02:33:54.557680Z"},"id":"07f063c5","outputId":"0f140ee9-f85c-4645-d9e3-022c7374a09f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>city</th>\n","      <th>city_count</th>\n","      <th>city_index</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Singapore</td>\n","      <td>10259</td>\n","      <td>0</td>\n","      <td>1.327829</td>\n","      <td>103.849073</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Москва</td>\n","      <td>9643</td>\n","      <td>1</td>\n","      <td>55.722583</td>\n","      <td>37.593779</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Bandung</td>\n","      <td>8745</td>\n","      <td>2</td>\n","      <td>-6.904392</td>\n","      <td>107.624720</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>New York</td>\n","      <td>8666</td>\n","      <td>3</td>\n","      <td>40.720292</td>\n","      <td>-73.881549</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>İstanbul</td>\n","      <td>6455</td>\n","      <td>4</td>\n","      <td>41.015430</td>\n","      <td>28.983876</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2019</th>\n","      <td>Wisconsin Dells</td>\n","      <td>59</td>\n","      <td>2019</td>\n","      <td>43.580736</td>\n","      <td>-89.750238</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>Hangzhou</td>\n","      <td>59</td>\n","      <td>2020</td>\n","      <td>30.280548</td>\n","      <td>120.187566</td>\n","    </tr>\n","    <tr>\n","      <th>2021</th>\n","      <td>Aarschot</td>\n","      <td>59</td>\n","      <td>2021</td>\n","      <td>50.986640</td>\n","      <td>4.821829</td>\n","    </tr>\n","    <tr>\n","      <th>2022</th>\n","      <td>Ypsilanti</td>\n","      <td>59</td>\n","      <td>2022</td>\n","      <td>42.246791</td>\n","      <td>-83.633531</td>\n","    </tr>\n","    <tr>\n","      <th>2023</th>\n","      <td>Kathu</td>\n","      <td>59</td>\n","      <td>2023</td>\n","      <td>7.896511</td>\n","      <td>98.311146</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2024 rows × 5 columns</p>\n","</div>"],"text/plain":["                 city  city_count  city_index   latitude   longitude\n","0           Singapore       10259           0   1.327829  103.849073\n","1              Москва        9643           1  55.722583   37.593779\n","2             Bandung        8745           2  -6.904392  107.624720\n","3            New York        8666           3  40.720292  -73.881549\n","4            İstanbul        6455           4  41.015430   28.983876\n","...               ...         ...         ...        ...         ...\n","2019  Wisconsin Dells          59        2019  43.580736  -89.750238\n","2020         Hangzhou          59        2020  30.280548  120.187566\n","2021         Aarschot          59        2021  50.986640    4.821829\n","2022        Ypsilanti          59        2022  42.246791  -83.633531\n","2023            Kathu          59        2023   7.896511   98.311146\n","\n","[2024 rows x 5 columns]"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["city_index_df"]},{"cell_type":"code","execution_count":null,"id":"b9ef2519","metadata":{"ExecuteTime":{"end_time":"2022-07-01T02:38:50.126114Z","start_time":"2022-07-01T02:37:16.609146Z"},"id":"b9ef2519"},"outputs":[],"source":["centroid_position = np.deg2rad(city_index_df[[\"latitude\", \"longitude\"]].values)\n","data = {}\n","data[\"distance_matrix\"] = (haversine(tf.expand_dims(centroid_position, axis=1), tf.expand_dims(centroid_position, axis=0)).numpy() * 1e9).astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","city_index_df[\"city_pos_tsp_index\"] = city_index_df[\"city_index\"].map({r: i for i, r in enumerate(routes)})"]},{"cell_type":"code","execution_count":null,"id":"b2853020","metadata":{"ExecuteTime":{"end_time":"2022-07-01T02:48:02.731397Z","start_time":"2022-07-01T02:46:32.914753Z"},"id":"b2853020"},"outputs":[],"source":["centroid_position = np.deg2rad(state_index_df[[\"latitude\", \"longitude\"]].values)\n","data = {}\n","data[\"distance_matrix\"] = (haversine(tf.expand_dims(centroid_position, axis=1), tf.expand_dims(centroid_position, axis=0)).numpy() * 1e9).astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","state_index_df[\"state_pos_tsp_index\"] = state_index_df[\"state_index\"].map({r: i for i, r in enumerate(routes)})"]},{"cell_type":"code","execution_count":null,"id":"b9657195","metadata":{"ExecuteTime":{"end_time":"2022-07-01T02:50:01.940839Z","start_time":"2022-07-01T02:48:22.549173Z"},"id":"b9657195"},"outputs":[],"source":["centroid_position = np.deg2rad(geo_name_index_df[[\"latitude\", \"longitude\"]].values)\n","data = {}\n","data[\"distance_matrix\"] = (haversine(tf.expand_dims(centroid_position, axis=1), tf.expand_dims(centroid_position, axis=0)).numpy() * 1e9).astype(np.int64).tolist()\n","\n","data[\"num_vehicles\"] = 1\n","data[\"depot\"] = 0\n","\n","\n","from ortools.constraint_solver import pywrapcp\n","from ortools.constraint_solver import routing_enums_pb2\n","\n","def distance_callback(from_index, to_index):\n","    \"\"\"Returns the distance between the two nodes.\"\"\"\n","    # Convert from routing variable Index to distance matrix NodeIndex.\n","    from_node = manager.IndexToNode(from_index)\n","    to_node = manager.IndexToNode(to_index)\n","    return data['distance_matrix'][from_node][to_node]\n","\n","def get_routes(solution, routing, manager):\n","    \"\"\"Get vehicle routes from a solution and store them in an array.\"\"\"\n","    # Get vehicle routes and store them in a two dimensional array whose\n","    # i,j entry is the jth location visited by vehicle i along its route.\n","    routes = []\n","    for route_nbr in range(routing.vehicles()):\n","        index = routing.Start(route_nbr)\n","        route = [manager.IndexToNode(index)]\n","        while not routing.IsEnd(index):\n","            index = solution.Value(routing.NextVar(index))\n","            route.append(manager.IndexToNode(index))\n","        routes.append(route)\n","    return routes[0]\n","\n","manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n","                                       data['num_vehicles'], data['depot'])\n","routing = pywrapcp.RoutingModel(manager)\n","transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n","routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n","search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n","search_parameters.first_solution_strategy = (\n","    routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n","\n","\n","solution = routing.SolveWithParameters(search_parameters)\n","routes = get_routes(solution, routing, manager)[:-1]\n","routes = [routes[(i+len(routes)//2)%len(routes)] for i in range(len(routes))]\n","geo_name_index_df[\"geo_name_pos_tsp_index\"] = geo_name_index_df[\"geo_name_index\"].map({r: i for i, r in enumerate(routes)})"]},{"cell_type":"code","execution_count":null,"id":"5bf96269","metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:04:25.393200Z","start_time":"2022-07-01T03:04:25.363201Z"},"id":"5bf96269"},"outputs":[],"source":["city_index_df.to_csv(nadare_feature_dir + \"city_tsp_index.csv\", index=None, encoding=\"utf-8\")\n","state_index_df.to_csv(nadare_feature_dir + \"state_tsp_index.csv\", index=None, encoding=\"utf-8\")\n","geo_name_index_df.to_csv(nadare_feature_dir + \"geo_name_tsp_index.csv\", index=None, encoding=\"utf-8\")\n"]},{"cell_type":"code","execution_count":null,"id":"c19175b1","metadata":{"ExecuteTime":{"end_time":"2022-07-01T03:26:34.606862Z","start_time":"2022-07-01T03:26:34.590865Z"},"id":"c19175b1"},"outputs":[],"source":["country_index_df.to_csv(nadare_feature_dir + \"country_tsp_index.csv\", index=None, encoding=\"utf-8\")"]},{"cell_type":"code","execution_count":null,"id":"7d4b4217","metadata":{"id":"7d4b4217"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"papermill":{"default_parameters":{},"duration":364.626038,"end_time":"2022-05-04T10:25:57.105513","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-05-04T10:19:52.479475","version":"2.3.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"FEAT26 256 geofix.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}